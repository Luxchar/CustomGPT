{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5011417e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 CONVERSION OPTIMISÉE POUR GROS FICHIERS\n",
      "📁 Fichier source: ../data/raw/Clothing_Shoes_and_Jewelry.jsonl\n",
      "📁 Fichier cible: ../data/processed/Clothing_Shoes_and_Jewelry.parquet\n",
      "⚙️ Taille des chunks: 50,000\n",
      "💾 RAM disponible: 14.7 GB\n",
      "🔄 Comptage des lignes...\n",
      "📊 Total de lignes: 66,033,346\n",
      "🔄 Traitement par chunks de 50,000 enregistrements...\n",
      "💾 Mémoire initiale: 36.72 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   4%|▍         | 2800000/66033346 [2:07:44<48:04:46, 365.33 lines/s, chunks=55, records=2,750,000, mem=36.95GB, errors=0] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 166\u001b[39m\n\u001b[32m    163\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m💾 RAM disponible: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpsutil.virtual_memory().available\u001b[38;5;250m \u001b[39m/\u001b[38;5;250m \u001b[39m\u001b[32m1024\u001b[39m**\u001b[32m3\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m GB\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m     processed, errors = \u001b[43mconvert_large_jsonl_to_parquet_optimized\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCHUNK_SIZE\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    172\u001b[39m     \u001b[38;5;66;03m# Statistiques finales\u001b[39;00m\n\u001b[32m    173\u001b[39m     original_size = os.path.getsize(input_file) / (\u001b[32m1024\u001b[39m**\u001b[32m3\u001b[39m)  \u001b[38;5;66;03m# GB\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 82\u001b[39m, in \u001b[36mconvert_large_jsonl_to_parquet_optimized\u001b[39m\u001b[34m(input_file, output_file, chunk_size)\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m chunk_df\n\u001b[32m     81\u001b[39m chunk_data = []\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[43mgc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Force garbage collection\u001b[39;00m\n\u001b[32m     84\u001b[39m chunk_num += \u001b[32m1\u001b[39m\n\u001b[32m     86\u001b[39m \u001b[38;5;66;03m# Afficher le progrès\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Version optimisée pour TRÈS GROS fichiers JSONL vers Parquet\n",
    "import json\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from pathlib import Path\n",
    "import os\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import psutil\n",
    "\n",
    "def get_memory_usage():\n",
    "    \"\"\"Retourne l'usage mémoire actuel\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss / 1024 / 1024 / 1024  # GB\n",
    "\n",
    "def convert_large_jsonl_to_parquet_optimized(input_file, output_file, chunk_size=100000):\n",
    "    \"\"\"\n",
    "    Convertit un très gros fichier JSONL en Parquet par chunks\n",
    "    optimisé pour la mémoire\n",
    "    \"\"\"\n",
    "    \n",
    "    # Créer le dossier de sortie\n",
    "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "    \n",
    "    # Compter le nombre total de lignes pour la barre de progression\n",
    "    print(\"🔄 Comptage des lignes...\")\n",
    "    total_lines = 0\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        for _ in f:\n",
    "            total_lines += 1\n",
    "    print(f\"📊 Total de lignes: {total_lines:,}\")\n",
    "    \n",
    "    # Variables de traitement\n",
    "    chunk_data = []\n",
    "    chunk_files = []\n",
    "    processed_records = 0\n",
    "    chunk_num = 0\n",
    "    errors = 0\n",
    "    \n",
    "    print(f\"🔄 Traitement par chunks de {chunk_size:,} enregistrements...\")\n",
    "    print(f\"💾 Mémoire initiale: {get_memory_usage():.2f} GB\")\n",
    "    \n",
    "    # Traitement avec barre de progression\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        with tqdm(total=total_lines, desc=\"Processing\", unit=\" lines\") as pbar:\n",
    "            \n",
    "            for line_num, line in enumerate(f, 1):\n",
    "                pbar.update(1)\n",
    "                \n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    data_point = json.loads(line)\n",
    "                    chunk_data.append(data_point)\n",
    "                    processed_records += 1\n",
    "                    \n",
    "                except json.JSONDecodeError:\n",
    "                    errors += 1\n",
    "                    continue\n",
    "                \n",
    "                # Sauvegarder le chunk quand il est plein\n",
    "                if len(chunk_data) >= chunk_size:\n",
    "                    chunk_file = f\"{output_file}.chunk_{chunk_num:04d}.parquet\"\n",
    "                    \n",
    "                    # Créer DataFrame et sauvegarder\n",
    "                    chunk_df = pd.DataFrame(chunk_data)\n",
    "                    chunk_df.to_parquet(\n",
    "                        chunk_file,\n",
    "                        compression='snappy',\n",
    "                        index=False,\n",
    "                        engine='pyarrow'\n",
    "                    )\n",
    "                    \n",
    "                    chunk_files.append(chunk_file)\n",
    "                    \n",
    "                    # Libérer la mémoire\n",
    "                    del chunk_df\n",
    "                    chunk_data = []\n",
    "                    gc.collect()  # Force garbage collection\n",
    "                    \n",
    "                    chunk_num += 1\n",
    "                    \n",
    "                    # Afficher le progrès\n",
    "                    memory_usage = get_memory_usage()\n",
    "                    pbar.set_postfix({\n",
    "                        'chunks': chunk_num,\n",
    "                        'records': f\"{processed_records:,}\",\n",
    "                        'mem': f\"{memory_usage:.2f}GB\",\n",
    "                        'errors': errors\n",
    "                    })\n",
    "    \n",
    "    # Traiter le dernier chunk\n",
    "    if chunk_data:\n",
    "        chunk_file = f\"{output_file}.chunk_{chunk_num:04d}.parquet\"\n",
    "        chunk_df = pd.DataFrame(chunk_data)\n",
    "        chunk_df.to_parquet(chunk_file, compression='snappy', index=False)\n",
    "        chunk_files.append(chunk_file)\n",
    "        del chunk_df\n",
    "        chunk_num += 1\n",
    "    \n",
    "    print(f\"\\n✅ Traitement terminé!\")\n",
    "    print(f\"📊 {processed_records:,} enregistrements traités\")\n",
    "    print(f\"⚠️ {errors} erreurs de parsing\")\n",
    "    print(f\"📁 {len(chunk_files)} fichiers chunks créés\")\n",
    "    \n",
    "    # Combiner tous les chunks en un seul fichier Parquet\n",
    "    print(f\"\\n🔄 Fusion des chunks en un seul fichier...\")\n",
    "    \n",
    "    # Lire et combiner par batches pour éviter la saturation mémoire\n",
    "    parquet_writer = None\n",
    "    schema = None\n",
    "    \n",
    "    for i, chunk_file in enumerate(tqdm(chunk_files, desc=\"Merging chunks\")):\n",
    "        # Lire le chunk\n",
    "        chunk_df = pd.read_parquet(chunk_file)\n",
    "        \n",
    "        # Convertir en PyArrow Table\n",
    "        table = pa.Table.from_pandas(chunk_df)\n",
    "        \n",
    "        if parquet_writer is None:\n",
    "            # Premier chunk : initialiser le writer\n",
    "            schema = table.schema\n",
    "            parquet_writer = pq.ParquetWriter(\n",
    "                output_file,\n",
    "                schema,\n",
    "                compression='snappy'\n",
    "            )\n",
    "        \n",
    "        # Écrire le chunk\n",
    "        parquet_writer.write_table(table)\n",
    "        \n",
    "        # Libérer la mémoire\n",
    "        del chunk_df, table\n",
    "        gc.collect()\n",
    "        \n",
    "        # Supprimer le fichier chunk temporaire\n",
    "        os.remove(chunk_file)\n",
    "    \n",
    "    # Fermer le writer\n",
    "    if parquet_writer:\n",
    "        parquet_writer.close()\n",
    "    \n",
    "    return processed_records, errors\n",
    "\n",
    "# ================================\n",
    "# UTILISATION OPTIMISÉE\n",
    "# ================================\n",
    "\n",
    "# Chemins des fichiers\n",
    "input_file = \"../data/raw/Clothing_Shoes_and_Jewelry.jsonl\"\n",
    "output_file = \"../data/processed/Clothing_Shoes_and_Jewelry.parquet\"\n",
    "\n",
    "# Paramètres optimisés pour 25GB\n",
    "CHUNK_SIZE = 50000  # Ajustez selon votre RAM (plus petit = moins de RAM)\n",
    "\n",
    "print(f\"🚀 CONVERSION OPTIMISÉE POUR GROS FICHIERS\")\n",
    "print(f\"📁 Fichier source: {input_file}\")\n",
    "print(f\"📁 Fichier cible: {output_file}\")\n",
    "print(f\"⚙️ Taille des chunks: {CHUNK_SIZE:,}\")\n",
    "print(f\"💾 RAM disponible: {psutil.virtual_memory().available / 1024**3:.1f} GB\")\n",
    "\n",
    "try:\n",
    "    processed, errors = convert_large_jsonl_to_parquet_optimized(\n",
    "        input_file, \n",
    "        output_file, \n",
    "        chunk_size=CHUNK_SIZE\n",
    "    )\n",
    "    \n",
    "    # Statistiques finales\n",
    "    original_size = os.path.getsize(input_file) / (1024**3)  # GB\n",
    "    parquet_size = os.path.getsize(output_file) / (1024**3)  # GB\n",
    "    compression_ratio = original_size / parquet_size\n",
    "    \n",
    "    print(f\"\\n🎉 CONVERSION TERMINÉE!\")\n",
    "    print(f\"📊 {processed:,} enregistrements convertis\")\n",
    "    print(f\"⚠️ {errors} erreurs\")\n",
    "    print(f\"📁 Taille JSONL: {original_size:.2f} GB\")\n",
    "    print(f\"📁 Taille Parquet: {parquet_size:.2f} GB\") \n",
    "    print(f\"🗜️ Compression: {compression_ratio:.1f}x\")\n",
    "    print(f\"💾 Mémoire finale: {get_memory_usage():.2f} GB\")\n",
    "    \n",
    "    # Test rapide de lecture\n",
    "    print(f\"\\n🔄 Test de lecture...\")\n",
    "    df_sample = pd.read_parquet(output_file, engine='pyarrow').head(1000)\n",
    "    print(f\"✅ Lecture réussie! Colonnes: {list(df_sample.columns)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Erreur: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cbd3c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting cudf\n",
      "  Downloading cudf-0.6.1.post1.tar.gz (1.1 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Building wheels for collected packages: cudf\n",
      "  Building wheel for cudf (pyproject.toml): started\n",
      "  Building wheel for cudf (pyproject.toml): finished with status 'error'\n",
      "Failed to build cudf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Building wheel for cudf (pyproject.toml) did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [84 lines of output]\n",
      "      C:\\Users\\Yann\\AppData\\Local\\Temp\\pip-build-env-w8w1okbh\\overlay\\Lib\\site-packages\\setuptools\\dist.py:759: SetuptoolsDeprecationWarning: License classifiers are deprecated.\n",
      "      !!\n",
      "      \n",
      "              ********************************************************************************\n",
      "              Please consider removing the following classifiers in favor of a SPDX license expression:\n",
      "      \n",
      "              License :: OSI Approved :: Apache Software License\n",
      "      \n",
      "              See https://packaging.python.org/en/latest/guides/writing-pyproject-toml/#license for details.\n",
      "              ********************************************************************************\n",
      "      \n",
      "      !!\n",
      "        self._finalize_license_expression()\n",
      "      running bdist_wheel\n",
      "      running build\n",
      "      installing to build\\bdist.win-amd64\\wheel\n",
      "      running install\n",
      "      Traceback (most recent call last):\n",
      "        File \u001b[35m\"C:\\Users\\Yann\\Desktop\\DEV\\School\\ml_m1\\NLP\\CustomGPT\\scraper_env\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\"\u001b[0m, line \u001b[35m389\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "          \u001b[31mmain\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "          \u001b[31m~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "        File \u001b[35m\"C:\\Users\\Yann\\Desktop\\DEV\\School\\ml_m1\\NLP\\CustomGPT\\scraper_env\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\"\u001b[0m, line \u001b[35m373\u001b[0m, in \u001b[35mmain\u001b[0m\n",
      "          json_out[\"return_val\"] = \u001b[31mhook\u001b[0m\u001b[1;31m(**hook_input[\"kwargs\"])\u001b[0m\n",
      "                                   \u001b[31m~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "        File \u001b[35m\"C:\\Users\\Yann\\Desktop\\DEV\\School\\ml_m1\\NLP\\CustomGPT\\scraper_env\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\"\u001b[0m, line \u001b[35m280\u001b[0m, in \u001b[35mbuild_wheel\u001b[0m\n",
      "          return \u001b[31m_build_backend().build_wheel\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "                 \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "              \u001b[1;31mwheel_directory, config_settings, metadata_directory\u001b[0m\n",
      "              \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "          \u001b[1;31m)\u001b[0m\n",
      "          \u001b[1;31m^\u001b[0m\n",
      "        File \u001b[35m\"C:\\Users\\Yann\\AppData\\Local\\Temp\\pip-build-env-w8w1okbh\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\"\u001b[0m, line \u001b[35m435\u001b[0m, in \u001b[35mbuild_wheel\u001b[0m\n",
      "          return _build(['bdist_wheel', '--dist-info-dir', str(metadata_directory)])\n",
      "        File \u001b[35m\"C:\\Users\\Yann\\AppData\\Local\\Temp\\pip-build-env-w8w1okbh\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\"\u001b[0m, line \u001b[35m423\u001b[0m, in \u001b[35m_build\u001b[0m\n",
      "          return \u001b[31mself._build_with_temp_dir\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "                 \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "              \u001b[1;31mcmd,\u001b[0m\n",
      "              \u001b[1;31m^^^^\u001b[0m\n",
      "          ...<3 lines>...\n",
      "              \u001b[1;31mself._arbitrary_args(config_settings),\u001b[0m\n",
      "              \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "          \u001b[1;31m)\u001b[0m\n",
      "          \u001b[1;31m^\u001b[0m\n",
      "        File \u001b[35m\"C:\\Users\\Yann\\AppData\\Local\\Temp\\pip-build-env-w8w1okbh\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\"\u001b[0m, line \u001b[35m404\u001b[0m, in \u001b[35m_build_with_temp_dir\u001b[0m\n",
      "          \u001b[31mself.run_setup\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "          \u001b[31m~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "        File \u001b[35m\"C:\\Users\\Yann\\AppData\\Local\\Temp\\pip-build-env-w8w1okbh\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\"\u001b[0m, line \u001b[35m512\u001b[0m, in \u001b[35mrun_setup\u001b[0m\n",
      "          \u001b[31msuper().run_setup\u001b[0m\u001b[1;31m(setup_script=setup_script)\u001b[0m\n",
      "          \u001b[31m~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "        File \u001b[35m\"C:\\Users\\Yann\\AppData\\Local\\Temp\\pip-build-env-w8w1okbh\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\"\u001b[0m, line \u001b[35m317\u001b[0m, in \u001b[35mrun_setup\u001b[0m\n",
      "          \u001b[31mexec\u001b[0m\u001b[1;31m(code, locals())\u001b[0m\n",
      "          \u001b[31m~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^\u001b[0m\n",
      "        File \u001b[35m\"<string>\"\u001b[0m, line \u001b[35m18\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "        File \u001b[35m\"C:\\Users\\Yann\\AppData\\Local\\Temp\\pip-build-env-w8w1okbh\\overlay\\Lib\\site-packages\\setuptools\\__init__.py\"\u001b[0m, line \u001b[35m115\u001b[0m, in \u001b[35msetup\u001b[0m\n",
      "          return \u001b[31mdistutils.core.setup\u001b[0m\u001b[1;31m(**attrs)\u001b[0m\n",
      "                 \u001b[31m~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^\u001b[0m\n",
      "        File \u001b[35m\"C:\\Users\\Yann\\AppData\\Local\\Temp\\pip-build-env-w8w1okbh\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\core.py\"\u001b[0m, line \u001b[35m186\u001b[0m, in \u001b[35msetup\u001b[0m\n",
      "          return run_commands(dist)\n",
      "        File \u001b[35m\"C:\\Users\\Yann\\AppData\\Local\\Temp\\pip-build-env-w8w1okbh\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\core.py\"\u001b[0m, line \u001b[35m202\u001b[0m, in \u001b[35mrun_commands\u001b[0m\n",
      "          \u001b[31mdist.run_commands\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "          \u001b[31m~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "        File \u001b[35m\"C:\\Users\\Yann\\AppData\\Local\\Temp\\pip-build-env-w8w1okbh\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\dist.py\"\u001b[0m, line \u001b[35m1002\u001b[0m, in \u001b[35mrun_commands\u001b[0m\n",
      "          \u001b[31mself.run_command\u001b[0m\u001b[1;31m(cmd)\u001b[0m\n",
      "          \u001b[31m~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^\u001b[0m\n",
      "        File \u001b[35m\"C:\\Users\\Yann\\AppData\\Local\\Temp\\pip-build-env-w8w1okbh\\overlay\\Lib\\site-packages\\setuptools\\dist.py\"\u001b[0m, line \u001b[35m1102\u001b[0m, in \u001b[35mrun_command\u001b[0m\n",
      "          \u001b[31msuper().run_command\u001b[0m\u001b[1;31m(command)\u001b[0m\n",
      "          \u001b[31m~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^\u001b[0m\n",
      "        File \u001b[35m\"C:\\Users\\Yann\\AppData\\Local\\Temp\\pip-build-env-w8w1okbh\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\dist.py\"\u001b[0m, line \u001b[35m1021\u001b[0m, in \u001b[35mrun_command\u001b[0m\n",
      "          \u001b[31mcmd_obj.run\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "          \u001b[31m~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "        File \u001b[35m\"C:\\Users\\Yann\\AppData\\Local\\Temp\\pip-build-env-w8w1okbh\\overlay\\Lib\\site-packages\\setuptools\\command\\bdist_wheel.py\"\u001b[0m, line \u001b[35m405\u001b[0m, in \u001b[35mrun\u001b[0m\n",
      "          \u001b[31mself.run_command\u001b[0m\u001b[1;31m(\"install\")\u001b[0m\n",
      "          \u001b[31m~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^\u001b[0m\n",
      "        File \u001b[35m\"C:\\Users\\Yann\\AppData\\Local\\Temp\\pip-build-env-w8w1okbh\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\cmd.py\"\u001b[0m, line \u001b[35m357\u001b[0m, in \u001b[35mrun_command\u001b[0m\n",
      "          \u001b[31mself.distribution.run_command\u001b[0m\u001b[1;31m(command)\u001b[0m\n",
      "          \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^\u001b[0m\n",
      "        File \u001b[35m\"C:\\Users\\Yann\\AppData\\Local\\Temp\\pip-build-env-w8w1okbh\\overlay\\Lib\\site-packages\\setuptools\\dist.py\"\u001b[0m, line \u001b[35m1102\u001b[0m, in \u001b[35mrun_command\u001b[0m\n",
      "          \u001b[31msuper().run_command\u001b[0m\u001b[1;31m(command)\u001b[0m\n",
      "          \u001b[31m~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^\u001b[0m\n",
      "        File \u001b[35m\"C:\\Users\\Yann\\AppData\\Local\\Temp\\pip-build-env-w8w1okbh\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\dist.py\"\u001b[0m, line \u001b[35m1021\u001b[0m, in \u001b[35mrun_command\u001b[0m\n",
      "          \u001b[31mcmd_obj.run\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "          \u001b[31m~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "        File \u001b[35m\"<string>\"\u001b[0m, line \u001b[35m15\u001b[0m, in \u001b[35mrun\u001b[0m\n",
      "      \u001b[1;35mException\u001b[0m: \u001b[35mPlease install cudf via the rapidsai conda channel. See https://rapids.ai/start.html for instructions.\u001b[0m\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for cudf\n",
      "ERROR: Failed to build installable wheels for some pyproject.toml based projects (cudf)\n"
     ]
    }
   ],
   "source": [
    "!pip install cudf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23aaead8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 CuPy non installé - Mode CPU uniquement\n",
      "🎯 TRAITEMENT DE VOTRE FICHIER 25GB\n",
      "📁 Source: ../data/raw/Clothing_Shoes_and_Jewelry.jsonl\n",
      "📁 Cible: ../data/raw/Clothing_Shoes_and_Jewelry.parquet\n",
      "🚀 CONVERSION ULTRA-OPTIMISÉE RTX 4080 + WINDOWS\n",
      "============================================================\n",
      "💻 CPU Cores: 24\n",
      "💾 RAM: 63.8 GB\n",
      "⚙️ Chunk size: 200,000\n",
      "🧵 Workers: 6\n",
      "🔄 Analyse du fichier...\n",
      "📊 66,033,346 lignes (25.90 GB)\n",
      "\n",
      "🔄 Traitement en cours...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "📊 Processing: 100%|██████████| 66033346/66033346 [1:09:29<00:00]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Traitement du dernier chunk...\n",
      "\n",
      "✅ Parsing terminé!\n",
      "📊 66,033,346 enregistrements traités\n",
      "⚠️ 0 erreurs de parsing\n",
      "📁 331 chunks créés\n",
      "\n",
      "🔄 Fusion finale des chunks...\n",
      "🚀 Fusion de 331 chunks avec PyArrow...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "📖 Loading: 100%|██████████| 331/331 [01:31<00:00,  3.62 chunks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Concaténation...\n",
      "💾 Écriture finale...\n",
      "❌ Erreur: BYTE_STREAM_SPLIT only supports FLOAT, DOUBLE, INT32, INT64 and FIXED_LEN_BYTE_ARRAY\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Yann\\AppData\\Local\\Temp\\ipykernel_25564\\113660458.py\", line 351, in main\n",
      "    processed, errors = convert_large_jsonl_final(input_file, output_file)\n",
      "                        ~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Yann\\AppData\\Local\\Temp\\ipykernel_25564\\113660458.py\", line 275, in convert_large_jsonl_final\n",
      "    merge_chunks_ultra_fast(chunk_files, output_file)\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Yann\\AppData\\Local\\Temp\\ipykernel_25564\\113660458.py\", line 304, in merge_chunks_ultra_fast\n",
      "    pq.write_table(\n",
      "    ~~~~~~~~~~~~~~^\n",
      "        combined_table,\n",
      "        ^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "        use_byte_stream_split=True  # Compression améliorée\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\Users\\Yann\\Desktop\\DEV\\School\\ml_m1\\NLP\\CustomGPT\\scraper_env\\Lib\\site-packages\\pyarrow\\parquet\\core.py\", line 1909, in write_table\n",
      "    writer.write_table(table, row_group_size=row_group_size)\n",
      "    ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Yann\\Desktop\\DEV\\School\\ml_m1\\NLP\\CustomGPT\\scraper_env\\Lib\\site-packages\\pyarrow\\parquet\\core.py\", line 1115, in write_table\n",
      "    self.writer.write_table(table, row_group_size=row_group_size)\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"pyarrow\\\\_parquet.pyx\", line 2226, in pyarrow._parquet.ParquetWriter.write_table\n",
      "  File \"pyarrow\\\\error.pxi\", line 92, in pyarrow.lib.check_status\n",
      "OSError: BYTE_STREAM_SPLIT only supports FLOAT, DOUBLE, INT32, INT64 and FIXED_LEN_BYTE_ARRAY\n"
     ]
    }
   ],
   "source": [
    "# Version ultra-optimisée pour RTX 4080 + Windows (SANS cuDF)\n",
    "import json\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import psutil\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import multiprocessing as mp\n",
    "\n",
    "# CuPy pour GPU (que vous avez déjà)\n",
    "try:\n",
    "    import cupy as cp\n",
    "    GPU_AVAILABLE = cp.cuda.is_available()\n",
    "    if GPU_AVAILABLE:\n",
    "        device = cp.cuda.Device()\n",
    "        print(f\"🚀 GPU DÉTECTÉ: {device.name}\")\n",
    "        print(f\"💾 VRAM: {device.mem_info[1] / 1024**3:.1f} GB\")\n",
    "        # Limiter l'usage GPU à 80% pour éviter les crashes\n",
    "        cp.cuda.MemoryPool().set_limit(int(device.mem_info[1] * 0.8))\n",
    "    else:\n",
    "        print(\"⚠️ GPU non disponible\")\n",
    "except ImportError:\n",
    "    GPU_AVAILABLE = False\n",
    "    print(\"📦 CuPy non installé - Mode CPU uniquement\")\n",
    "\n",
    "def get_system_info():\n",
    "    \"\"\"Affiche les infos système pour optimiser\"\"\"\n",
    "    cpu_count = mp.cpu_count()\n",
    "    ram_gb = psutil.virtual_memory().total / 1024**3\n",
    "    \n",
    "    print(f\"💻 CPU Cores: {cpu_count}\")\n",
    "    print(f\"💾 RAM: {ram_gb:.1f} GB\")\n",
    "    \n",
    "    if GPU_AVAILABLE:\n",
    "        gpu_memory = cp.cuda.Device().mem_info[1] / 1024**3\n",
    "        print(f\"🎮 VRAM: {gpu_memory:.1f} GB\")\n",
    "        return cpu_count, ram_gb, gpu_memory\n",
    "    \n",
    "    return cpu_count, ram_gb, 0\n",
    "\n",
    "def calculate_optimal_params():\n",
    "    \"\"\"Calcule les paramètres optimaux selon votre matériel\"\"\"\n",
    "    cpu_count, ram_gb, gpu_memory = get_system_info()\n",
    "    \n",
    "    # Taille des chunks adaptative\n",
    "    if GPU_AVAILABLE and gpu_memory > 10:\n",
    "        chunk_size = 300000  # RTX 4080 peut gérer de gros chunks\n",
    "        workers = min(cpu_count, 8)\n",
    "    elif ram_gb > 16:\n",
    "        chunk_size = 200000\n",
    "        workers = min(cpu_count, 6)\n",
    "    elif ram_gb > 8:\n",
    "        chunk_size = 100000\n",
    "        workers = min(cpu_count, 4)\n",
    "    else:\n",
    "        chunk_size = 50000\n",
    "        workers = 2\n",
    "    \n",
    "    print(f\"⚙️ Chunk size: {chunk_size:,}\")\n",
    "    print(f\"🧵 Workers: {workers}\")\n",
    "    \n",
    "    return chunk_size, workers\n",
    "\n",
    "def parse_json_chunk_parallel(lines_chunk, worker_id=0):\n",
    "    \"\"\"Parse JSON en parallèle avec gestion d'erreurs\"\"\"\n",
    "    parsed_data = []\n",
    "    errors = 0\n",
    "    \n",
    "    for line in lines_chunk:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            try:\n",
    "                parsed_data.append(json.loads(line))\n",
    "            except json.JSONDecodeError:\n",
    "                errors += 1\n",
    "                continue\n",
    "    \n",
    "    return parsed_data, errors\n",
    "\n",
    "def optimize_dataframe_gpu(df):\n",
    "    \"\"\"Optimise le DataFrame avec GPU si disponible\"\"\"\n",
    "    if not GPU_AVAILABLE:\n",
    "        return df\n",
    "    \n",
    "    try:\n",
    "        # Optimisations GPU pour colonnes numériques\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        \n",
    "        for col in numeric_cols:\n",
    "            if col in df.columns and not df[col].isna().all():\n",
    "                # Transférer vers GPU\n",
    "                values = df[col].fillna(0).values.astype(np.float32)\n",
    "                gpu_array = cp.asarray(values)\n",
    "                \n",
    "                # Opérations GPU rapides\n",
    "                if col == 'user_rating':\n",
    "                    # Normaliser les ratings\n",
    "                    mean_val = cp.mean(gpu_array)\n",
    "                    std_val = cp.std(gpu_array)\n",
    "                    df[f'{col}_normalized'] = cp.asnumpy((gpu_array - mean_val) / (std_val + 1e-8))\n",
    "                \n",
    "                # Nettoyer GPU\n",
    "                del gpu_array\n",
    "        \n",
    "        # Optimisations pour le texte\n",
    "        if 'review_text' in df.columns:\n",
    "            # Calculer longueurs sur GPU\n",
    "            text_lengths = df['review_text'].str.len().fillna(0).values\n",
    "            if len(text_lengths) > 0:\n",
    "                gpu_lengths = cp.asarray(text_lengths)\n",
    "                \n",
    "                # Stats rapides\n",
    "                mean_length = float(cp.mean(gpu_lengths))\n",
    "                max_length = float(cp.max(gpu_lengths))\n",
    "                \n",
    "                df['text_length'] = cp.asnumpy(gpu_lengths)\n",
    "                \n",
    "                del gpu_lengths\n",
    "        \n",
    "        # Forcer le nettoyage GPU\n",
    "        cp.get_default_memory_pool().free_all_blocks()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ GPU optimization failed: {e}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def convert_large_jsonl_final(input_file, output_file, chunk_size=None, num_workers=None):\n",
    "    \"\"\"\n",
    "    VERSION FINALE - Optimisée pour RTX 4080 + Windows\n",
    "    SANS cuDF mais avec toutes les autres optimisations\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"🚀 CONVERSION ULTRA-OPTIMISÉE RTX 4080 + WINDOWS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Paramètres adaptatifs\n",
    "    if chunk_size is None or num_workers is None:\n",
    "        chunk_size, num_workers = calculate_optimal_params()\n",
    "    \n",
    "    # Créer dossier de sortie\n",
    "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "    \n",
    "    # Compter les lignes pour la progression\n",
    "    print(\"🔄 Analyse du fichier...\")\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        total_lines = sum(1 for _ in f)\n",
    "    \n",
    "    file_size_gb = os.path.getsize(input_file) / 1024**3\n",
    "    print(f\"📊 {total_lines:,} lignes ({file_size_gb:.2f} GB)\")\n",
    "    \n",
    "    # Variables de traitement\n",
    "    chunk_files = []\n",
    "    processed_records = 0\n",
    "    total_errors = 0\n",
    "    chunk_num = 0\n",
    "    \n",
    "    # Buffer pour lecture par chunks\n",
    "    lines_buffer = []\n",
    "    \n",
    "    print(f\"\\n🔄 Traitement en cours...\")\n",
    "    \n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        with tqdm(total=total_lines, desc=\"📊 Processing\", unit=\" lines\", \n",
    "                  bar_format=\"{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]\") as pbar:\n",
    "            \n",
    "            for line_num, line in enumerate(f, 1):\n",
    "                pbar.update(1)\n",
    "                lines_buffer.append(line)\n",
    "                \n",
    "                # Traiter quand le buffer est plein\n",
    "                if len(lines_buffer) >= chunk_size:\n",
    "                    \n",
    "                    # Diviser en sous-chunks pour traitement parallèle\n",
    "                    sub_chunk_size = len(lines_buffer) // num_workers\n",
    "                    if sub_chunk_size == 0:\n",
    "                        sub_chunk_size = len(lines_buffer)\n",
    "                    \n",
    "                    sub_chunks = [\n",
    "                        lines_buffer[i:i + sub_chunk_size]\n",
    "                        for i in range(0, len(lines_buffer), sub_chunk_size)\n",
    "                    ]\n",
    "                    \n",
    "                    # Parse JSON en parallèle\n",
    "                    all_parsed_data = []\n",
    "                    chunk_errors = 0\n",
    "                    \n",
    "                    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "                        future_to_chunk = {\n",
    "                            executor.submit(parse_json_chunk_parallel, chunk, i): i \n",
    "                            for i, chunk in enumerate(sub_chunks)\n",
    "                        }\n",
    "                        \n",
    "                        for future in future_to_chunk:\n",
    "                            try:\n",
    "                                parsed_data, errors = future.result()\n",
    "                                all_parsed_data.extend(parsed_data)\n",
    "                                chunk_errors += errors\n",
    "                            except Exception as e:\n",
    "                                print(f\"⚠️ Worker error: {e}\")\n",
    "                                chunk_errors += 1\n",
    "                    \n",
    "                    # Créer DataFrame si on a des données\n",
    "                    if all_parsed_data:\n",
    "                        df = pd.DataFrame(all_parsed_data)\n",
    "                        \n",
    "                        # 🚀 OPTIMISATIONS GPU\n",
    "                        df = optimize_dataframe_gpu(df)\n",
    "                        \n",
    "                        # Sauvegarder le chunk\n",
    "                        chunk_file = f\"{output_file}.chunk_{chunk_num:04d}.parquet\"\n",
    "                        df.to_parquet(\n",
    "                            chunk_file,\n",
    "                            compression='snappy',\n",
    "                            index=False,\n",
    "                            engine='pyarrow',\n",
    "                            use_dictionary=True  # Optimisation supplémentaire\n",
    "                        )\n",
    "                        \n",
    "                        chunk_files.append(chunk_file)\n",
    "                        processed_records += len(all_parsed_data)\n",
    "                        total_errors += chunk_errors\n",
    "                        \n",
    "                        # Nettoyer la mémoire\n",
    "                        del df, all_parsed_data\n",
    "                        gc.collect()\n",
    "                        \n",
    "                        chunk_num += 1\n",
    "                    \n",
    "                    # Vider le buffer\n",
    "                    lines_buffer = []\n",
    "                    \n",
    "                    # Mise à jour de la barre de progression\n",
    "                    memory_usage = psutil.Process().memory_info().rss / 1024**3\n",
    "                    \n",
    "                    postfix = {\n",
    "                        'chunks': chunk_num,\n",
    "                        'records': f\"{processed_records:,}\",\n",
    "                        'RAM': f\"{memory_usage:.1f}GB\",\n",
    "                        'errors': total_errors\n",
    "                    }\n",
    "                    \n",
    "                    if GPU_AVAILABLE:\n",
    "                        gpu_memory = cp.get_default_memory_pool().used_bytes() / 1024**3\n",
    "                        postfix['GPU'] = f\"{gpu_memory:.1f}GB\"\n",
    "                    \n",
    "                    pbar.set_postfix(postfix)\n",
    "    \n",
    "    # Traiter le dernier buffer\n",
    "    if lines_buffer:\n",
    "        print(\"🔄 Traitement du dernier chunk...\")\n",
    "        parsed_data, errors = parse_json_chunk_parallel(lines_buffer)\n",
    "        if parsed_data:\n",
    "            df = pd.DataFrame(parsed_data)\n",
    "            df = optimize_dataframe_gpu(df)\n",
    "            \n",
    "            chunk_file = f\"{output_file}.chunk_{chunk_num:04d}.parquet\"\n",
    "            df.to_parquet(chunk_file, compression='snappy', index=False)\n",
    "            chunk_files.append(chunk_file)\n",
    "            processed_records += len(parsed_data)\n",
    "            total_errors += errors\n",
    "            del df\n",
    "    \n",
    "    print(f\"\\n✅ Parsing terminé!\")\n",
    "    print(f\"📊 {processed_records:,} enregistrements traités\")\n",
    "    print(f\"⚠️ {total_errors} erreurs de parsing\")\n",
    "    print(f\"📁 {len(chunk_files)} chunks créés\")\n",
    "    \n",
    "    # 🔄 FUSION FINALE ULTRA-RAPIDE\n",
    "    print(f\"\\n🔄 Fusion finale des chunks...\")\n",
    "    merge_chunks_ultra_fast(chunk_files, output_file)\n",
    "    \n",
    "    return processed_records, total_errors\n",
    "\n",
    "def merge_chunks_ultra_fast(chunk_files, output_file):\n",
    "    \"\"\"Fusion ultra-rapide avec PyArrow pur - VERSION CORRIGÉE\"\"\"\n",
    "    \n",
    "    print(f\"🚀 Fusion de {len(chunk_files)} chunks avec PyArrow...\")\n",
    "    \n",
    "    # Lire tous les chunks comme tables PyArrow\n",
    "    tables = []\n",
    "    \n",
    "    for chunk_file in tqdm(chunk_files, desc=\"📖 Loading\", unit=\" chunks\"):\n",
    "        try:\n",
    "            table = pq.read_table(chunk_file)\n",
    "            tables.append(table)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Erreur lecture {chunk_file}: {e}\")\n",
    "    \n",
    "    if not tables:\n",
    "        print(\"❌ Aucun chunk valide trouvé!\")\n",
    "        return\n",
    "    \n",
    "    # Concaténation PyArrow (ultra-rapide)\n",
    "    print(\"🔄 Concaténation...\")\n",
    "    combined_table = pa.concat_tables(tables)\n",
    "    \n",
    "    # Écriture avec optimisations COMPATIBLES\n",
    "    print(\"💾 Écriture finale...\")\n",
    "    try:\n",
    "        # Essayer avec toutes les optimisations\n",
    "        pq.write_table(\n",
    "            combined_table,\n",
    "            output_file,\n",
    "            compression='snappy',\n",
    "            use_dictionary=True,\n",
    "            write_statistics=True,\n",
    "            row_group_size=100000,\n",
    "            # use_byte_stream_split=True  # ❌ SUPPRIMÉ - cause l'erreur\n",
    "        )\n",
    "        print(\"✅ Écriture avec optimisations complètes\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Erreur avec optimisations: {e}\")\n",
    "        print(\"🔄 Fallback vers écriture basique...\")\n",
    "        \n",
    "        # Fallback vers écriture simple\n",
    "        try:\n",
    "            pq.write_table(\n",
    "                combined_table,\n",
    "                output_file,\n",
    "                compression='snappy',\n",
    "                use_dictionary=False,  # Désactiver si problème\n",
    "                write_statistics=False\n",
    "            )\n",
    "            print(\"✅ Écriture basique réussie\")\n",
    "        except Exception as e2:\n",
    "            print(f\"❌ Erreur critique: {e2}\")\n",
    "            # Dernier recours avec pandas\n",
    "            print(\"🔄 Dernier recours avec pandas...\")\n",
    "            df = combined_table.to_pandas()\n",
    "            df.to_parquet(output_file, compression='snappy', index=False)\n",
    "            print(\"✅ Sauvegarde pandas réussie\")\n",
    "    \n",
    "    # Nettoyage\n",
    "    print(\"🧹 Nettoyage...\")\n",
    "    for chunk_file in chunk_files:\n",
    "        try:\n",
    "            os.remove(chunk_file)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Libérer mémoire\n",
    "    del tables, combined_table\n",
    "    gc.collect()\n",
    "    \n",
    "    print(\"✅ Fusion terminée!\")\n",
    "\n",
    "# ================================\n",
    "# UTILISATION SIMPLE\n",
    "# ================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Fonction principale - lancez ça!\"\"\"\n",
    "    \n",
    "    # Vos fichiers\n",
    "    input_file = \"../data/raw/Clothing_Shoes_and_Jewelry.jsonl\"\n",
    "    output_file = \"../data/raw/Clothing_Shoes_and_Jewelry.parquet\"\n",
    "    \n",
    "    print(\"🎯 TRAITEMENT DE VOTRE FICHIER 25GB\")\n",
    "    print(f\"📁 Source: {input_file}\")\n",
    "    print(f\"📁 Cible: {output_file}\")\n",
    "    \n",
    "    # Vérifier que le fichier existe\n",
    "    if not os.path.exists(input_file):\n",
    "        print(f\"❌ Fichier non trouvé: {input_file}\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        processed, errors = convert_large_jsonl_final(input_file, output_file)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        processing_time = end_time - start_time\n",
    "        \n",
    "        # Statistiques finales\n",
    "        if os.path.exists(output_file):\n",
    "            original_size = os.path.getsize(input_file) / (1024**3)\n",
    "            parquet_size = os.path.getsize(output_file) / (1024**3)\n",
    "            compression_ratio = original_size / parquet_size\n",
    "            speed = processed / processing_time\n",
    "            \n",
    "            print(f\"\\n🎉 CONVERSION TERMINÉE!\")\n",
    "            print(f\"⏱️ Temps: {processing_time/60:.1f} minutes\")\n",
    "            print(f\"📊 {processed:,} enregistrements\")\n",
    "            print(f\"⚠️ {errors} erreurs ({errors/processed*100:.2f}%)\")\n",
    "            print(f\"📁 {original_size:.2f} GB → {parquet_size:.2f} GB\")\n",
    "            print(f\"🗜️ Compression: {compression_ratio:.1f}x\")\n",
    "            print(f\"🚀 Vitesse: {speed:,.0f} records/sec\")\n",
    "            \n",
    "            # Test de lecture\n",
    "            print(f\"\\n🔍 Test de lecture...\")\n",
    "            sample = pd.read_parquet(output_file).head(5)\n",
    "            print(f\"✅ Lecture OK! Shape: {sample.shape}\")\n",
    "            print(f\"📋 Colonnes: {list(sample.columns)}\")\n",
    "            \n",
    "        else:\n",
    "            print(\"❌ Fichier de sortie non créé\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erreur: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import time\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30ff1184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Fusion de 56 fichiers en ../data/processed/Clothing_Shoes_and_Jewelry.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🔗 Fusion des chunks: 100%|██████████| 56/56 [00:13<00:00,  4.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Fusion terminée : ../data/processed/Clothing_Shoes_and_Jewelry.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def merge_parquet_chunks(chunk_files, output_file):\n",
    "    \"\"\"\n",
    "    Fusionne une liste de fichiers Parquet en un seul.\n",
    "    \n",
    "    Paramètres :\n",
    "        chunk_files (list of str): chemins des fichiers Parquet à fusionner\n",
    "        output_file (str): chemin du fichier Parquet final\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"🔄 Fusion de {len(chunk_files)} fichiers en {output_file}\")\n",
    "    \n",
    "    # S'assurer que le dossier de sortie existe\n",
    "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "    \n",
    "    parquet_writer = None\n",
    "    schema = None\n",
    "\n",
    "    for chunk_path in tqdm(chunk_files, desc=\"🔗 Fusion des chunks\"):\n",
    "        try:\n",
    "            # Lire chunk\n",
    "            chunk_df = pd.read_parquet(chunk_path)\n",
    "            table = pa.Table.from_pandas(chunk_df)\n",
    "\n",
    "            if parquet_writer is None:\n",
    "                # Initialisation du writer avec le schéma du premier chunk\n",
    "                schema = table.schema\n",
    "                parquet_writer = pq.ParquetWriter(\n",
    "                    output_file,\n",
    "                    schema=schema,\n",
    "                    compression=\"snappy\"\n",
    "                )\n",
    "\n",
    "            # Écriture du chunk\n",
    "            parquet_writer.write_table(table)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Erreur sur {chunk_path}: {e}\")\n",
    "        finally:\n",
    "            # Libération mémoire\n",
    "            del chunk_df, table\n",
    "            gc.collect()\n",
    "    \n",
    "    # Finaliser\n",
    "    if parquet_writer:\n",
    "        parquet_writer.close()\n",
    "\n",
    "    print(f\"\\n✅ Fusion terminée : {output_file}\")\n",
    "\n",
    "# =========================\n",
    "# Exemple d'utilisation\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    # Exemple de détection automatique des fichiers chunkés\n",
    "    import glob\n",
    "\n",
    "    # Chemin de base (adapter selon votre structure) Clothing_Shoes_and_Jewelry.parquet.chunk_0330.parquet\n",
    "    output_file = \"../data/processed/Clothing_Shoes_and_Jewelry.parquet\"\n",
    "    chunk_pattern = os.path.join(\n",
    "        os.path.dirname(output_file),\n",
    "        \"Clothing_Shoes_and_Jewelry.parquet.chunk_*.parquet\"\n",
    "    )\n",
    "\n",
    "\n",
    "    chunk_files = sorted(glob.glob(chunk_pattern))\n",
    "    \n",
    "    if not chunk_files:\n",
    "        print(\"⚠️ Aucun fichier chunk trouvé.\")\n",
    "    else:\n",
    "        merge_parquet_chunks(chunk_files, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81d367e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 331 fichiers trouvés avec le motif : ../data/raw\\Clothing_Shoes_and_Jewelry.parquet.chunk_*.parquet\n",
      "🔄 Fusion de 331 fichiers en ../data/processed/Clothing_Shoes_and_Jewelry.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🔗 Fusion des chunks: 100%|██████████| 331/331 [04:08<00:00,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Fusion terminée : ../data/processed/Clothing_Shoes_and_Jewelry.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import glob\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from tqdm import tqdm\n",
    "\n",
    "def merge_parquet_chunks(chunk_files, output_file):\n",
    "    \"\"\"\n",
    "    Fusionne une liste de fichiers Parquet en un seul.\n",
    "    \n",
    "    Args:\n",
    "        chunk_files (list of str): chemins des fichiers Parquet à fusionner\n",
    "        output_file (str): chemin du fichier Parquet final\n",
    "    \"\"\"\n",
    "    print(f\"🔄 Fusion de {len(chunk_files)} fichiers en {output_file}\")\n",
    "    \n",
    "    # Créer le dossier de sortie si nécessaire\n",
    "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "    \n",
    "    parquet_writer = None\n",
    "\n",
    "    for chunk_path in tqdm(chunk_files, desc=\"🔗 Fusion des chunks\"):\n",
    "        try:\n",
    "            chunk_df = pd.read_parquet(chunk_path)\n",
    "            table = pa.Table.from_pandas(chunk_df)\n",
    "\n",
    "            if parquet_writer is None:\n",
    "                parquet_writer = pq.ParquetWriter(\n",
    "                    output_file,\n",
    "                    table.schema,\n",
    "                    compression=\"snappy\"\n",
    "                )\n",
    "\n",
    "            parquet_writer.write_table(table)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Erreur avec {chunk_path} : {e}\")\n",
    "        finally:\n",
    "            del chunk_df, table\n",
    "            gc.collect()\n",
    "    \n",
    "    if parquet_writer:\n",
    "        parquet_writer.close()\n",
    "\n",
    "    print(f\"\\n✅ Fusion terminée : {output_file}\")\n",
    "\n",
    "# =========================\n",
    "# Exemple d'utilisation\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    # Dossier contenant les fichiers chunkés\n",
    "    chunk_dir = \"../data/raw\"  # <- Change ici si nécessaire\n",
    "    output_file = \"../data/processed/Clothing_Shoes_and_Jewelry.parquet\"\n",
    "\n",
    "    # Motif pour trouver tous les chunks\n",
    "    chunk_pattern = os.path.join(chunk_dir, \"Clothing_Shoes_and_Jewelry.parquet.chunk_*.parquet\")\n",
    "\n",
    "    # Lire tous les chunks\n",
    "    chunk_files = sorted(glob.glob(chunk_pattern))\n",
    "\n",
    "    print(f\"🔍 {len(chunk_files)} fichiers trouvés avec le motif : {chunk_pattern}\")\n",
    "    \n",
    "    if not chunk_files:\n",
    "        print(\"⚠️ Aucun fichier chunk trouvé.\")\n",
    "    else:\n",
    "        merge_parquet_chunks(chunk_files, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169d5177",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Chemin vers le fichier Parquet fusionné\n",
    "parquet_file = \"../data/processed/Clothing_Shoes_and_Jewelry.parquet\"\n",
    "\n",
    "# Lire le fichier complet (attention à la taille en RAM !)\n",
    "df = pd.read_parquet(parquet_file, engine=\"pyarrow\")\n",
    "\n",
    "# Afficher les premières lignes\n",
    "print(\"✅ Aperçu du DataFrame fusionné :\")\n",
    "print(df.head())\n",
    "\n",
    "# Afficher quelques infos utiles\n",
    "print(\"\\n📊 Infos générales :\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\n📏 Dimensions :\")\n",
    "print(f\"Lignes : {df.shape[0]:,}\")\n",
    "print(f\"Colonnes : {df.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4ed5ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 3,000,000 lignes lues\n",
      "   rating                                    title  \\\n",
      "0     3.0  Arrived Damaged : liquid in hub locker!   \n",
      "1     3.0                Useless under 40 degrees.   \n",
      "2     4.0   Not waterproof, but a very comfy shoe.   \n",
      "3     4.0        Lovely, but QA issues with sewing   \n",
      "4     2.0                                  Just ok   \n",
      "\n",
      "                                                text  \\\n",
      "0  Unfortunately Amazon in their wisdom (cough, c...   \n",
      "1  Useless under 40 degrees unless you’re just ru...   \n",
      "2  I purchased these bc they are supposed to be w...   \n",
      "3  I’ll start by saying I love this robe!  I trul...   \n",
      "4  Don't be fooled by the description. I was free...   \n",
      "\n",
      "                                              images        asin parent_asin  \\\n",
      "0  [{'attachment_type': 'IMAGE', 'large_image_url...  B096S6LZV4  B09NSZ5QMF   \n",
      "1                                                 []  B09KMDBDCN  B08NGL3X17   \n",
      "2                                                 []  B096N5WK8Q  B07RGM3DYC   \n",
      "3  [{'attachment_type': 'IMAGE', 'large_image_url...  B07JR4QBZ4  B07BWS4CSM   \n",
      "4                                                 []  B09GY958RK  B09GY6SG2C   \n",
      "\n",
      "                        user_id      timestamp  helpful_vote  \\\n",
      "0  AFKZENTNBQ7A7V7UXW5JJI6UGRYQ  1677938767351             0   \n",
      "1  AFKZENTNBQ7A7V7UXW5JJI6UGRYQ  1677083819242             0   \n",
      "2  AFKZENTNBQ7A7V7UXW5JJI6UGRYQ  1675524098918            11   \n",
      "3  AFKZENTNBQ7A7V7UXW5JJI6UGRYQ  1545114577507            26   \n",
      "4  AGGZ357AO26RQZVRLGU4D4N52DZQ  1645223372746             1   \n",
      "\n",
      "   verified_purchase  \n",
      "0               True  \n",
      "1              False  \n",
      "2               True  \n",
      "3               True  \n",
      "4               True  \n",
      "\n",
      "📊 Infos générales :\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3000000 entries, 0 to 2999999\n",
      "Data columns (total 10 columns):\n",
      " #   Column             Dtype  \n",
      "---  ------             -----  \n",
      " 0   rating             float64\n",
      " 1   title              object \n",
      " 2   text               object \n",
      " 3   images             object \n",
      " 4   asin               object \n",
      " 5   parent_asin        object \n",
      " 6   user_id            object \n",
      " 7   timestamp          int64  \n",
      " 8   helpful_vote       int64  \n",
      " 9   verified_purchase  bool   \n",
      "dtypes: bool(1), float64(1), int64(2), object(6)\n",
      "memory usage: 208.9+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.dataset as ds\n",
    "import pandas as pd\n",
    "\n",
    "parquet_file = \"../data/processed/merged/Clothing_Shoes_and_Jewelry.parquet\"\n",
    "dataset = ds.dataset(parquet_file, format=\"parquet\")\n",
    "\n",
    "batch_reader = dataset.to_batches()\n",
    "\n",
    "\n",
    "rows_collected = 0\n",
    "max_rows = 3_000_000\n",
    "frames = []\n",
    "\n",
    "for batch in batch_reader:\n",
    "    batch_df = batch.to_pandas()\n",
    "    batch_len = len(batch_df)\n",
    "    \n",
    "    if rows_collected + batch_len >= max_rows:\n",
    "        # Prendre uniquement le reste\n",
    "        needed = max_rows - rows_collected\n",
    "        frames.append(batch_df.iloc[:needed])\n",
    "        break\n",
    "    else:\n",
    "        frames.append(batch_df)\n",
    "        rows_collected += batch_len\n",
    "\n",
    "df = pd.concat(frames, ignore_index=True)\n",
    "print(f\"✅ {len(df):,} lignes lues\")\n",
    "print(df.head())\n",
    "print(\"\\n📊 Infos générales :\")\n",
    "print(df.info())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3b63c2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         2023-03-04 14:06:07.351\n",
       "1         2023-02-22 16:36:59.242\n",
       "2         2023-02-04 15:21:38.918\n",
       "3         2018-12-18 06:29:37.507\n",
       "4         2022-02-18 22:29:32.746\n",
       "                    ...          \n",
       "2999995   2022-09-01 18:22:35.935\n",
       "2999996   2020-12-31 04:24:51.163\n",
       "2999997   2020-08-11 23:57:21.001\n",
       "2999998   2020-07-31 18:10:50.754\n",
       "2999999   2018-01-15 19:04:41.829\n",
       "Name: date, Length: 3000000, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df[\"date\"] = pd.to_datetime(df[\"timestamp\"], unit=\"ms\")\n",
    "df[\"date\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdc01fc",
   "metadata": {},
   "source": [
    "#### Conversion timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6d5aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Lecture du dataset parquet en batchs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🧪 Traitement batchs: 661it [03:38,  3.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ 661 chunks traités. Fusion en un seul fichier final...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🔗 Fusion finale:   2%|▏         | 16/661 [00:05<03:23,  3.18it/s]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.dataset as ds\n",
    "import pyarrow.parquet as pq\n",
    "from tqdm import tqdm\n",
    "\n",
    "def process_parquet_in_batches(input_file, output_file, batch_size=100_000):\n",
    "    \"\"\"\n",
    "    Lit un gros fichier Parquet par batchs, ajoute une colonne date convertie depuis timestamp,\n",
    "    sauvegarde chaque batch en chunk parquet, puis fusionne tous les chunks.\n",
    "    \"\"\"\n",
    "    # Dossiers de travail\n",
    "    output_dir = os.path.dirname(output_file)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    temp_chunks_dir = os.path.join(output_dir, \"tmp_chunks\")\n",
    "    os.makedirs(temp_chunks_dir, exist_ok=True)\n",
    "\n",
    "    print(\"📥 Lecture du dataset parquet en batchs...\")\n",
    "    dataset = ds.dataset(input_file, format=\"parquet\")\n",
    "    batch_reader = dataset.to_batches(batch_size=batch_size)\n",
    "\n",
    "    chunk_paths = []\n",
    "    total_rows = 0\n",
    "\n",
    "    for i, batch in enumerate(tqdm(batch_reader, desc=\"🧪 Traitement batchs\")):\n",
    "        df = batch.to_pandas()\n",
    "\n",
    "        # Ajout de la colonne date (à partir de timestamp)\n",
    "        df[\"date\"] = pd.to_datetime(df[\"timestamp\"], unit=\"ms\")\n",
    "\n",
    "        # Sauvegarde du batch traité\n",
    "        chunk_path = os.path.join(temp_chunks_dir, f\"processed_chunk_{i:04d}.parquet\")\n",
    "        df.to_parquet(chunk_path, index=False, engine=\"pyarrow\", compression=\"snappy\")\n",
    "        chunk_paths.append(chunk_path)\n",
    "\n",
    "        total_rows += len(df)\n",
    "\n",
    "        # Nettoyage mémoire\n",
    "        del df, batch\n",
    "        gc.collect()\n",
    "\n",
    "    print(f\"\\n✅ {len(chunk_paths)} chunks traités. Fusion en un seul fichier final...\")\n",
    "\n",
    "    # Fusion des chunks\n",
    "    parquet_writer = None\n",
    "    for chunk_file in tqdm(chunk_paths, desc=\"🔗 Fusion finale\"):\n",
    "        chunk_df = pd.read_parquet(chunk_file)\n",
    "        table = pa.Table.from_pandas(chunk_df)\n",
    "\n",
    "        if parquet_writer is None:\n",
    "            parquet_writer = pq.ParquetWriter(output_file, table.schema, compression=\"snappy\")\n",
    "\n",
    "        parquet_writer.write_table(table)\n",
    "\n",
    "        # Nettoyage\n",
    "        del chunk_df, table\n",
    "        gc.collect()\n",
    "\n",
    "    if parquet_writer:\n",
    "        parquet_writer.close()\n",
    "\n",
    "    print(f\"\\n🎉 Fichier final traité enregistré à : {output_file}\")\n",
    "    print(f\"📊 Lignes totales : {total_rows:,}\")\n",
    "    print(\"🧹 Suppression des fichiers temporaires...\")\n",
    "    \n",
    "    # Suppression des chunks temporaires\n",
    "    for f in chunk_paths:\n",
    "        try:\n",
    "            os.remove(f)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Erreur suppression {f}: {e}\")\n",
    "    os.rmdir(temp_chunks_dir)\n",
    "\n",
    "# ===============================\n",
    "# Exemple d'utilisation\n",
    "# ===============================\n",
    "if __name__ == \"__main__\":\n",
    "    input_parquet = \"../data/processed/merged/Clothing_Shoes_and_Jewelry.parquet\"\n",
    "    output_parquet = \"../data/processed/final/Clothing_Shoes_and_Jewelry_timestamped.parquet\"\n",
    "\n",
    "    process_parquet_in_batches(input_parquet, output_parquet, batch_size=100_000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738b622d",
   "metadata": {},
   "source": [
    "#### partitionnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ef9b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import traceback\n",
    "\n",
    "# === PARAMÈTRES ===\n",
    "input_file = \"../data/processed/final/Clothing_Shoes_and_Jewelry_processed.parquet\"\n",
    "partition_dir = \"../data/processed/partitions\"\n",
    "partition_size = 6_000_000  # lignes par partition\n",
    "overwrite_existing = False  # Mettre à True pour réécraser les partitions existantes\n",
    "\n",
    "# === CRÉATION DU DOSSIER DE SORTIE ===\n",
    "try:\n",
    "    os.makedirs(partition_dir, exist_ok=True)\n",
    "except Exception as e:\n",
    "    print(f\"❌ Impossible de créer le dossier {partition_dir} : {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# === CHARGEMENT DU DATAFRAME ===\n",
    "try:\n",
    "    print(\"📥 Chargement du DataFrame depuis le Parquet...\")\n",
    "    df = pd.read_parquet(input_file, engine=\"pyarrow\")\n",
    "    total_rows = len(df)\n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ Fichier introuvable : {input_file}\")\n",
    "    sys.exit(1)\n",
    "except Exception:\n",
    "    print(\"❌ Erreur inattendue lors du chargement du Parquet :\")\n",
    "    traceback.print_exc()\n",
    "    sys.exit(1)\n",
    "\n",
    "# === CALCUL DES PARTITIONS ===\n",
    "num_partitions = math.ceil(total_rows / partition_size)\n",
    "print(f\"📊 Total lignes: {total_rows:,} → {num_partitions} partitions de {partition_size:,} lignes\")\n",
    "\n",
    "# === TRAITEMENT PAR PARTITION ===\n",
    "for i in range(num_partitions):\n",
    "    start_idx = i * partition_size\n",
    "    end_idx = min(start_idx + partition_size, total_rows)\n",
    "    part_path = os.path.join(partition_dir, f\"partition_{i:02d}.parquet\")\n",
    "    \n",
    "    if os.path.exists(part_path) and not overwrite_existing:\n",
    "        print(f\"⚠️ Partition déjà existante, ignorée : {part_path}\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        df_part = df.iloc[start_idx:end_idx]\n",
    "        df_part.to_parquet(part_path, index=False, compression=\"snappy\")\n",
    "        print(f\"✅ Partition {i+1}/{num_partitions} sauvegardée : {part_path} ({len(df_part):,} lignes)\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erreur lors de l’écriture de la partition {i} : {e}\")\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        del df_part\n",
    "\n",
    "print(\"\\n🎉 Partitionnement terminé avec succès.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scraper_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
