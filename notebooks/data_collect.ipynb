{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fb82a26",
   "metadata": {},
   "source": [
    "# üïµÔ∏è Advanced Marketplace Scraper - Anti-Detection System\n",
    "\n",
    "Ce notebook impl√©mente un scraper Selenium avanc√© avec syst√®me anti-d√©tection pour scraper des marketplaces anglaises :\n",
    "\n",
    "## üõ°Ô∏è Protections Anti-Ban :\n",
    "- **Fake User Agents** : Rotation automatique d'user agents r√©alistes\n",
    "- **Proxy Rotation** : Rotation d'IP pour √©viter la d√©tection\n",
    "- **Comportement Humain** : Mouvements de souris, scrolling, pauses naturelles\n",
    "- **D√©lais Al√©atoires** : Timing humain entre les actions\n",
    "- **Headers R√©alistes** : Simulation compl√®te de navigateur r√©el\n",
    "\n",
    "## üéØ Sites Cibl√©s :\n",
    "- **E-commerce** : Amazon-like sites, eBay, etc.\n",
    "- **Reviews** : Trustpilot, Google Reviews, Yelp\n",
    "- **Product Data** : Prix, descriptions, reviews avec dates\n",
    "- **APIs publiques** quand disponibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b4054a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (4.11.2)\n",
      "Requirement already satisfied: webdriver-manager in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (4.0.2)\n",
      "Requirement already satisfied: fake-useragent in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (2.2.0)\n",
      "Requirement already satisfied: requests in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (2.28.2)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (4.13.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (2.3.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.26.15)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from selenium) (0.30.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from selenium) (0.12.2)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from selenium) (2025.6.15)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium) (25.3.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium) (2.8)\n",
      "Requirement already satisfied: outcome in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium) (1.17.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from webdriver-manager) (1.1.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from webdriver-manager) (25.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from requests) (3.4.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from beautifulsoup4) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from beautifulsoup4) (4.13.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from pandas) (2.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: pycparser in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.22)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: undetected-chromedriver in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (3.5.3)\n",
      "Requirement already satisfied: selenium-stealth in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (1.0.6)\n",
      "Requirement already satisfied: pyautogui in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (0.9.54)\n",
      "Requirement already satisfied: selenium>=4.9.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from undetected-chromedriver) (4.11.2)\n",
      "Requirement already satisfied: requests in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from undetected-chromedriver) (2.28.2)\n",
      "Requirement already satisfied: websockets in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from undetected-chromedriver) (15.0.1)\n",
      "Requirement already satisfied: pymsgbox in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from pyautogui) (1.0.9)\n",
      "Requirement already satisfied: pytweening>=1.0.4 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from pyautogui) (1.2.0)\n",
      "Requirement already satisfied: pyscreeze>=0.1.21 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from pyautogui) (1.0.1)\n",
      "Requirement already satisfied: pygetwindow>=0.0.5 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from pyautogui) (0.0.9)\n",
      "Requirement already satisfied: mouseinfo in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from pyautogui) (0.1.3)\n",
      "Requirement already satisfied: pyrect in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from pygetwindow>=0.0.5->pyautogui) (0.2.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium>=4.9.0->undetected-chromedriver) (1.26.15)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from selenium>=4.9.0->undetected-chromedriver) (0.30.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from selenium>=4.9.0->undetected-chromedriver) (0.12.2)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from selenium>=4.9.0->undetected-chromedriver) (2025.6.15)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (25.3.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (2.8)\n",
      "Requirement already satisfied: outcome in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (1.17.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio-websocket~=0.9->selenium>=4.9.0->undetected-chromedriver) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium>=4.9.0->undetected-chromedriver) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (2.22)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium>=4.9.0->undetected-chromedriver) (0.16.0)\n",
      "Requirement already satisfied: pyperclip in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from mouseinfo->pyautogui) (1.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from requests->undetected-chromedriver) (3.4.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: undetected-chromedriver in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (3.5.3)\n",
      "Requirement already satisfied: selenium-stealth in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (1.0.6)\n",
      "Requirement already satisfied: pyautogui in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (0.9.54)\n",
      "Requirement already satisfied: selenium>=4.9.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from undetected-chromedriver) (4.11.2)\n",
      "Requirement already satisfied: requests in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from undetected-chromedriver) (2.28.2)\n",
      "Requirement already satisfied: websockets in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from undetected-chromedriver) (15.0.1)\n",
      "Requirement already satisfied: pymsgbox in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from pyautogui) (1.0.9)\n",
      "Requirement already satisfied: pytweening>=1.0.4 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from pyautogui) (1.2.0)\n",
      "Requirement already satisfied: pyscreeze>=0.1.21 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from pyautogui) (1.0.1)\n",
      "Requirement already satisfied: pygetwindow>=0.0.5 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from pyautogui) (0.0.9)\n",
      "Requirement already satisfied: mouseinfo in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from pyautogui) (0.1.3)\n",
      "Requirement already satisfied: pyrect in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from pygetwindow>=0.0.5->pyautogui) (0.2.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium>=4.9.0->undetected-chromedriver) (1.26.15)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from selenium>=4.9.0->undetected-chromedriver) (0.30.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from selenium>=4.9.0->undetected-chromedriver) (0.12.2)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from selenium>=4.9.0->undetected-chromedriver) (2025.6.15)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (25.3.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (2.8)\n",
      "Requirement already satisfied: outcome in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (1.17.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio-websocket~=0.9->selenium>=4.9.0->undetected-chromedriver) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium>=4.9.0->undetected-chromedriver) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (2.22)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium>=4.9.0->undetected-chromedriver) (0.16.0)\n",
      "Requirement already satisfied: pyperclip in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from mouseinfo->pyautogui) (1.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from requests->undetected-chromedriver) (3.4.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: requests-proxy-adapter in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (0.1.1)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (2.9.0.post0)\n",
      "Requirement already satisfied: lxml in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (6.0.0)\n",
      "Requirement already satisfied: pluggy==0.11.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from requests-proxy-adapter) (0.11.0)\n",
      "Collecting requests==2.22.0 (from requests-proxy-adapter)\n",
      "  Downloading requests-2.22.0-py2.py3-none-any.whl.metadata (5.5 kB)\n",
      "Requirement already satisfied: urllib3>=1.15 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from requests-proxy-adapter) (1.26.15)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from requests==2.22.0->requests-proxy-adapter) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from requests==2.22.0->requests-proxy-adapter) (2.8)\n",
      "Collecting urllib3>=1.15 (from requests-proxy-adapter)\n",
      "  Downloading urllib3-1.25.11-py2.py3-none-any.whl.metadata (41 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from requests==2.22.0->requests-proxy-adapter) (2025.6.15)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from python-dateutil) (1.17.0)\n",
      "Downloading requests-2.22.0-py2.py3-none-any.whl (57 kB)\n",
      "Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
      "Installing collected packages: urllib3, requests\n",
      "\n",
      "  Attempting uninstall: urllib3\n",
      "\n",
      "    Found existing installation: urllib3 1.26.15\n",
      "\n",
      "    Uninstalling urllib3-1.26.15:\n",
      "\n",
      "      Successfully uninstalled urllib3-1.26.15\n",
      "\n",
      "   ---------------------------------------- 0/2 [urllib3]\n",
      "  Attempting uninstall: requests\n",
      "   ---------------------------------------- 0/2 [urllib3]\n",
      "    Found existing installation: requests 2.28.2\n",
      "   ---------------------------------------- 0/2 [urllib3]\n",
      "    Uninstalling requests-2.28.2:\n",
      "   ---------------------------------------- 0/2 [urllib3]\n",
      "      Successfully uninstalled requests-2.28.2\n",
      "   ---------------------------------------- 0/2 [urllib3]\n",
      "   -------------------- ------------------- 1/2 [requests]\n",
      "   ---------------------------------------- 2/2 [requests]\n",
      "\n",
      "Successfully installed requests-2.22.0 urllib3-1.25.11\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "üì¶ Installation termin√©e - Scraper anti-d√©tection pr√™t !\n",
      "Requirement already satisfied: requests-proxy-adapter in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (0.1.1)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (2.9.0.post0)\n",
      "Requirement already satisfied: lxml in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (6.0.0)\n",
      "Requirement already satisfied: pluggy==0.11.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from requests-proxy-adapter) (0.11.0)\n",
      "Collecting requests==2.22.0 (from requests-proxy-adapter)\n",
      "  Downloading requests-2.22.0-py2.py3-none-any.whl.metadata (5.5 kB)\n",
      "Requirement already satisfied: urllib3>=1.15 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from requests-proxy-adapter) (1.26.15)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from requests==2.22.0->requests-proxy-adapter) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from requests==2.22.0->requests-proxy-adapter) (2.8)\n",
      "Collecting urllib3>=1.15 (from requests-proxy-adapter)\n",
      "  Downloading urllib3-1.25.11-py2.py3-none-any.whl.metadata (41 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from requests==2.22.0->requests-proxy-adapter) (2025.6.15)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from python-dateutil) (1.17.0)\n",
      "Downloading requests-2.22.0-py2.py3-none-any.whl (57 kB)\n",
      "Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
      "Installing collected packages: urllib3, requests\n",
      "\n",
      "  Attempting uninstall: urllib3\n",
      "\n",
      "    Found existing installation: urllib3 1.26.15\n",
      "\n",
      "    Uninstalling urllib3-1.26.15:\n",
      "\n",
      "      Successfully uninstalled urllib3-1.26.15\n",
      "\n",
      "   ---------------------------------------- 0/2 [urllib3]\n",
      "  Attempting uninstall: requests\n",
      "   ---------------------------------------- 0/2 [urllib3]\n",
      "    Found existing installation: requests 2.28.2\n",
      "   ---------------------------------------- 0/2 [urllib3]\n",
      "    Uninstalling requests-2.28.2:\n",
      "   ---------------------------------------- 0/2 [urllib3]\n",
      "      Successfully uninstalled requests-2.28.2\n",
      "   ---------------------------------------- 0/2 [urllib3]\n",
      "   -------------------- ------------------- 1/2 [requests]\n",
      "   ---------------------------------------- 2/2 [requests]\n",
      "\n",
      "Successfully installed requests-2.22.0 urllib3-1.25.11\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "üì¶ Installation termin√©e - Scraper anti-d√©tection pr√™t !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "jupyterlab-server 2.27.3 requires requests>=2.31, but you have requests 2.22.0 which is incompatible.\n",
      "selenium 4.11.2 requires urllib3[socks]<3,>=1.26, but you have urllib3 1.25.11 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "# Installation des packages pour scraping anti-d√©tection\n",
    "%pip install selenium webdriver-manager fake-useragent requests beautifulsoup4 pandas\n",
    "%pip install undetected-chromedriver selenium-stealth pyautogui\n",
    "%pip install requests-proxy-adapter python-dateutil lxml\n",
    "\n",
    "print(\"üì¶ Installation termin√©e - Scraper anti-d√©tection pr√™t !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88736027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting urllib3==1.26.18\n",
      "  Downloading urllib3-1.26.18-py2.py3-none-any.whl.metadata (48 kB)\n",
      "Downloading urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
      "Installing collected packages: urllib3\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.25.11\n",
      "    Uninstalling urllib3-1.25.11:\n",
      "      Successfully uninstalled urllib3-1.25.11\n",
      "Successfully installed urllib3-1.26.18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "requests 2.22.0 requires urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you have urllib3 1.26.18 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium==4.15.0\n",
      "  Downloading selenium-4.15.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium==4.15.0) (1.26.18)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from selenium==4.15.0) (0.30.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from selenium==4.15.0) (0.12.2)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from selenium==4.15.0) (2025.6.15)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium==4.15.0) (25.3.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium==4.15.0) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium==4.15.0) (2.8)\n",
      "Requirement already satisfied: outcome in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium==4.15.0) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium==4.15.0) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium==4.15.0) (1.17.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio-websocket~=0.9->selenium==4.15.0) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium==4.15.0) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium==4.15.0) (2.22)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium==4.15.0) (0.16.0)\n",
      "Downloading selenium-4.15.0-py3-none-any.whl (10.2 MB)\n",
      "   ---------------------------------------- 0.0/10.2 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 1.0/10.2 MB 6.6 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 2.6/10.2 MB 6.7 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 4.2/10.2 MB 6.8 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 5.5/10.2 MB 6.7 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 7.1/10.2 MB 6.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 8.4/10.2 MB 6.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.0/10.2 MB 6.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.0/10.2 MB 6.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.2/10.2 MB 5.4 MB/s eta 0:00:00\n",
      "Installing collected packages: selenium\n",
      "  Attempting uninstall: selenium\n",
      "    Found existing installation: selenium 4.11.2\n",
      "    Uninstalling selenium-4.11.2:\n",
      "      Successfully uninstalled selenium-4.11.2\n",
      "Successfully installed selenium-4.15.0\n",
      "Collecting undetected-chromedriver==3.5.4\n",
      "  Downloading undetected-chromedriver-3.5.4.tar.gz (65 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: selenium>=4.9.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from undetected-chromedriver==3.5.4) (4.15.0)\n",
      "Requirement already satisfied: requests in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from undetected-chromedriver==3.5.4) (2.22.0)\n",
      "Requirement already satisfied: websockets in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from undetected-chromedriver==3.5.4) (15.0.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium>=4.9.0->undetected-chromedriver==3.5.4) (1.26.18)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from selenium>=4.9.0->undetected-chromedriver==3.5.4) (0.30.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from selenium>=4.9.0->undetected-chromedriver==3.5.4) (0.12.2)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from selenium>=4.9.0->undetected-chromedriver==3.5.4) (2025.6.15)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver==3.5.4) (25.3.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver==3.5.4) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver==3.5.4) (2.8)\n",
      "Requirement already satisfied: outcome in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver==3.5.4) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver==3.5.4) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver==3.5.4) (1.17.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio-websocket~=0.9->selenium>=4.9.0->undetected-chromedriver==3.5.4) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium>=4.9.0->undetected-chromedriver==3.5.4) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium>=4.9.0->undetected-chromedriver==3.5.4) (2.22)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium>=4.9.0->undetected-chromedriver==3.5.4) (0.16.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from requests->undetected-chromedriver==3.5.4) (3.0.4)\n",
      "INFO: pip is looking at multiple versions of requests to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting requests (from undetected-chromedriver==3.5.4)\n",
      "  Downloading requests-2.32.4-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from requests->undetected-chromedriver==3.5.4) (3.4.2)\n",
      "Downloading requests-2.32.4-py3-none-any.whl (64 kB)\n",
      "Building wheels for collected packages: undetected-chromedriver\n",
      "  Building wheel for undetected-chromedriver (pyproject.toml): started\n",
      "  Building wheel for undetected-chromedriver (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for undetected-chromedriver: filename=undetected_chromedriver-3.5.4-py3-none-any.whl size=47221 sha256=2906bdf9288dd8e24264f2f1eb6fd072f3be1fa9fd107e7123947cc5c3c2d2fa\n",
      "  Stored in directory: c:\\users\\yann\\appdata\\local\\pip\\cache\\wheels\\ca\\7f\\a2\\7feaa6b6bfdf742311206d2b103d185711dda87177add2ee71\n",
      "Successfully built undetected-chromedriver\n",
      "Installing collected packages: requests, undetected-chromedriver\n",
      "\n",
      "  Attempting uninstall: requests\n",
      "\n",
      "    Found existing installation: requests 2.22.0\n",
      "\n",
      "    Uninstalling requests-2.22.0:\n",
      "\n",
      "      Successfully uninstalled requests-2.22.0\n",
      "\n",
      "  Attempting uninstall: undetected-chromedriver\n",
      "\n",
      "    Found existing installation: undetected-chromedriver 3.5.3\n",
      "\n",
      "    Uninstalling undetected-chromedriver-3.5.3:\n",
      "\n",
      "      Successfully uninstalled undetected-chromedriver-3.5.3\n",
      "\n",
      "   -------------------- ------------------- 1/2 [undetected-chromedriver]\n",
      "   -------------------- ------------------- 1/2 [undetected-chromedriver]\n",
      "   ---------------------------------------- 2/2 [undetected-chromedriver]\n",
      "\n",
      "Successfully installed requests-2.32.4 undetected-chromedriver-3.5.4\n",
      "Collecting undetected-chromedriver==3.5.4\n",
      "  Downloading undetected-chromedriver-3.5.4.tar.gz (65 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: selenium>=4.9.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from undetected-chromedriver==3.5.4) (4.15.0)\n",
      "Requirement already satisfied: requests in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from undetected-chromedriver==3.5.4) (2.22.0)\n",
      "Requirement already satisfied: websockets in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from undetected-chromedriver==3.5.4) (15.0.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium>=4.9.0->undetected-chromedriver==3.5.4) (1.26.18)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from selenium>=4.9.0->undetected-chromedriver==3.5.4) (0.30.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from selenium>=4.9.0->undetected-chromedriver==3.5.4) (0.12.2)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from selenium>=4.9.0->undetected-chromedriver==3.5.4) (2025.6.15)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver==3.5.4) (25.3.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver==3.5.4) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver==3.5.4) (2.8)\n",
      "Requirement already satisfied: outcome in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver==3.5.4) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver==3.5.4) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver==3.5.4) (1.17.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio-websocket~=0.9->selenium>=4.9.0->undetected-chromedriver==3.5.4) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium>=4.9.0->undetected-chromedriver==3.5.4) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium>=4.9.0->undetected-chromedriver==3.5.4) (2.22)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium>=4.9.0->undetected-chromedriver==3.5.4) (0.16.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from requests->undetected-chromedriver==3.5.4) (3.0.4)\n",
      "INFO: pip is looking at multiple versions of requests to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting requests (from undetected-chromedriver==3.5.4)\n",
      "  Downloading requests-2.32.4-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from requests->undetected-chromedriver==3.5.4) (3.4.2)\n",
      "Downloading requests-2.32.4-py3-none-any.whl (64 kB)\n",
      "Building wheels for collected packages: undetected-chromedriver\n",
      "  Building wheel for undetected-chromedriver (pyproject.toml): started\n",
      "  Building wheel for undetected-chromedriver (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for undetected-chromedriver: filename=undetected_chromedriver-3.5.4-py3-none-any.whl size=47221 sha256=2906bdf9288dd8e24264f2f1eb6fd072f3be1fa9fd107e7123947cc5c3c2d2fa\n",
      "  Stored in directory: c:\\users\\yann\\appdata\\local\\pip\\cache\\wheels\\ca\\7f\\a2\\7feaa6b6bfdf742311206d2b103d185711dda87177add2ee71\n",
      "Successfully built undetected-chromedriver\n",
      "Installing collected packages: requests, undetected-chromedriver\n",
      "\n",
      "  Attempting uninstall: requests\n",
      "\n",
      "    Found existing installation: requests 2.22.0\n",
      "\n",
      "    Uninstalling requests-2.22.0:\n",
      "\n",
      "      Successfully uninstalled requests-2.22.0\n",
      "\n",
      "  Attempting uninstall: undetected-chromedriver\n",
      "\n",
      "    Found existing installation: undetected-chromedriver 3.5.3\n",
      "\n",
      "    Uninstalling undetected-chromedriver-3.5.3:\n",
      "\n",
      "      Successfully uninstalled undetected-chromedriver-3.5.3\n",
      "\n",
      "   -------------------- ------------------- 1/2 [undetected-chromedriver]\n",
      "   -------------------- ------------------- 1/2 [undetected-chromedriver]\n",
      "   ---------------------------------------- 2/2 [undetected-chromedriver]\n",
      "\n",
      "Successfully installed requests-2.32.4 undetected-chromedriver-3.5.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "requests-proxy-adapter 0.1.1 requires requests==2.22.0, but you have requests 2.32.4 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "# Dans votre terminal ou une cellule :\n",
    "!pip install urllib3==1.26.18\n",
    "!pip install selenium==4.15.0\n",
    "!pip install undetected-chromedriver==3.5.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839c3eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî• R√âPARATION FORCE - urllib3.packages.six.moves\n",
      "üóëÔ∏è Nettoyage: uninstall urllib3 -y\n",
      "üóëÔ∏è Nettoyage: uninstall urllib3 -y\n",
      "üóëÔ∏è Nettoyage: uninstall selenium -y\n",
      "üóëÔ∏è Nettoyage: uninstall selenium -y\n",
      "üóëÔ∏è Nettoyage: uninstall undetected-chromedriver -y\n",
      "üóëÔ∏è Nettoyage: uninstall undetected-chromedriver -y\n",
      "üóëÔ∏è Nettoyage: uninstall requests -y\n",
      "üóëÔ∏è Nettoyage: uninstall requests -y\n",
      "üóëÔ∏è Nettoyage: cache purge\n",
      "üóëÔ∏è Nettoyage: cache purge\n",
      "‚úÖ urllib3==1.26.15\n",
      "‚úÖ urllib3==1.26.15\n",
      "‚úÖ requests==2.28.2\n",
      "‚úÖ requests==2.28.2\n",
      "‚úÖ selenium==4.11.2\n",
      "‚úÖ selenium==4.11.2\n",
      "‚úÖ undetected-chromedriver==3.5.3\n",
      "\n",
      "üß™ TEST FINAL...\n",
      "‚úÖ urllib3 version: 1.26.15\n",
      "‚úÖ selenium version: 4.11.2\n",
      "‚úÖ undetected_chromedriver: OK\n",
      "\n",
      "üéâ R√âPARATION R√âUSSIE !\n",
      "‚úÖ undetected-chromedriver==3.5.3\n",
      "\n",
      "üß™ TEST FINAL...\n",
      "‚úÖ urllib3 version: 1.26.15\n",
      "‚úÖ selenium version: 4.11.2\n",
      "‚úÖ undetected_chromedriver: OK\n",
      "\n",
      "üéâ R√âPARATION R√âUSSIE !\n"
     ]
    }
   ],
   "source": [
    "# CELLULE: R√âPARATION FORCE\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "def force_fix_urllib3():\n",
    "    \"\"\"Solution force pour urllib3.packages.six.moves\"\"\"\n",
    "    \n",
    "    print(\"üî• R√âPARATION FORCE - urllib3.packages.six.moves\")\n",
    "    \n",
    "    # 1. Nettoyage brutal\n",
    "    cleanup_commands = [\n",
    "        [sys.executable, '-m', 'pip', 'uninstall', 'urllib3', '-y'],\n",
    "        [sys.executable, '-m', 'pip', 'uninstall', 'selenium', '-y'], \n",
    "        [sys.executable, '-m', 'pip', 'uninstall', 'undetected-chromedriver', '-y'],\n",
    "        [sys.executable, '-m', 'pip', 'uninstall', 'requests', '-y'],\n",
    "        [sys.executable, '-m', 'pip', 'cache', 'purge']\n",
    "    ]\n",
    "    \n",
    "    for cmd in cleanup_commands:\n",
    "        subprocess.run(cmd, capture_output=True)\n",
    "        print(f\"üóëÔ∏è Nettoyage: {' '.join(cmd[3:])}\")\n",
    "    \n",
    "    # 2. Installation versions STABLES\n",
    "    stable_packages = [\n",
    "        'urllib3==1.26.15',\n",
    "        'requests==2.28.2', \n",
    "        'selenium==4.11.2',\n",
    "        'undetected-chromedriver==3.5.3'\n",
    "    ]\n",
    "    \n",
    "    for package in stable_packages:\n",
    "        cmd = [sys.executable, '-m', 'pip', 'install', package]\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(f\"‚úÖ {package}\")\n",
    "        else:\n",
    "            print(f\"‚ùå {package}: {result.stderr}\")\n",
    "    \n",
    "    # 3. Test imm√©diat\n",
    "    print(\"\\nüß™ TEST FINAL...\")\n",
    "    try:\n",
    "        import importlib\n",
    "        import urllib3\n",
    "        print(f\"‚úÖ urllib3 version: {urllib3.__version__}\")\n",
    "        \n",
    "        import selenium\n",
    "        print(f\"‚úÖ selenium version: {selenium.__version__}\")\n",
    "        \n",
    "        import undetected_chromedriver as uc\n",
    "        print(\"‚úÖ undetected_chromedriver: OK\")\n",
    "        \n",
    "        print(\"\\nüéâ R√âPARATION R√âUSSIE !\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Test √©chou√©: {e}\")\n",
    "\n",
    "# EX√âCUTER LA R√âPARATION FORCE\n",
    "force_fix_urllib3()\n",
    "\n",
    "import random\n",
    "import undetected_chromedriver as uc\n",
    "\n",
    "REALISTIC_USER_AGENTS = [\n",
    "    # Liste d'exemples d'agents utilisateurs r√©alistes\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.2 Safari/605.1.15\",\n",
    "    \"Mozilla/5.0 (iPhone; CPU iPhone OS 14_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.1 Mobile/15E148 Safari/604.1\",\n",
    "    \"Mozilla/5.0 (Linux; Android 10; SM-G973F) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Mobile Safari/537.36\",\n",
    "    # Ajoutez d'autres agents utilisateurs si n√©cessaire\n",
    "]\n",
    "\n",
    "PROXY_LIST = [\n",
    "    # Liste d'exemples de proxies\n",
    "    \"http://proxy1:port\",\n",
    "    \"http://proxy2:port\",\n",
    "    \"http://proxy3:port\",\n",
    "    # Ajoutez d'autres proxies si n√©cessaire\n",
    "]\n",
    "\n",
    "class StealthMarketplaceScraper:\n",
    "    \"\"\"\n",
    "    Scraper avanc√© avec protection anti-d√©tection pour marketplaces\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, headless=True, use_proxy=False):\n",
    "        self.headless = headless\n",
    "        self.use_proxy = use_proxy\n",
    "        self.driver = None\n",
    "        self.session_duration = 0\n",
    "        \n",
    "    def _get_random_user_agent(self):\n",
    "        return random.choice(REALISTIC_USER_AGENTS)\n",
    "    \n",
    "    def _get_random_proxy(self):\n",
    "        return random.choice(PROXY_LIST) if self.use_proxy and PROXY_LIST else None\n",
    "        \n",
    "    def _setup_driver(self):\n",
    "        \"\"\"Configuration Chrome optimis√©e et compatible\"\"\"\n",
    "        try:\n",
    "            options = uc.ChromeOptions()\n",
    "            \n",
    "            # Options de base compatibles\n",
    "            options.add_argument('--no-sandbox')\n",
    "            options.add_argument('--disable-dev-shm-usage')\n",
    "            options.add_argument('--disable-gpu')\n",
    "            options.add_argument('--disable-extensions')\n",
    "            options.add_argument('--disable-plugins')\n",
    "            options.add_argument('--disable-images')\n",
    "            options.add_argument('--disable-javascript')\n",
    "            options.add_argument('--disable-notifications')\n",
    "            options.add_argument('--disable-popup-blocking')\n",
    "            options.add_argument('--no-first-run')\n",
    "            options.add_argument('--no-default-browser-check')\n",
    "            options.add_argument('--ignore-certificate-errors')\n",
    "            options.add_argument('--ignore-ssl-errors')\n",
    "            options.add_argument('--ignore-certificate-errors-spki-list')\n",
    "            options.add_argument('--disable-web-security')\n",
    "            \n",
    "            if self.headless:\n",
    "                options.add_argument('--headless')\n",
    "                \n",
    "            # User agent al√©atoire\n",
    "            user_agent = self._get_random_user_agent()\n",
    "            options.add_argument(f'--user-agent={user_agent}')\n",
    "            \n",
    "            # Proxy si activ√©\n",
    "            proxy = self._get_random_proxy()\n",
    "            if proxy:\n",
    "                options.add_argument(f'--proxy-server={proxy}')\n",
    "                \n",
    "            # Options exp√©rimentales compatibles (sans excludeSwitches)\n",
    "            options.add_experimental_option('useAutomationExtension', False)\n",
    "            options.add_experimental_option(\"prefs\", {\n",
    "                \"profile.default_content_setting_values.notifications\": 2,\n",
    "                \"profile.default_content_settings.popups\": 0,\n",
    "                \"profile.managed_default_content_settings.images\": 2\n",
    "            })\n",
    "            \n",
    "            # Cr√©ation du driver avec version d√©tect√©e automatiquement\n",
    "            self.driver = uc.Chrome(options=options, version_main=None)\n",
    "            \n",
    "            # Injection anti-d√©tection JavaScript\n",
    "            self.driver.execute_cdp_cmd('Page.addScriptToEvaluateOnNewDocument', {\n",
    "                'source': '''\n",
    "                    Object.defineProperty(navigator, 'webdriver', {\n",
    "                        get: () => undefined,\n",
    "                    });\n",
    "                    \n",
    "                    Object.defineProperty(navigator, 'plugins', {\n",
    "                        get: () => [1, 2, 3, 4, 5],\n",
    "                    });\n",
    "                    \n",
    "                    Object.defineProperty(navigator, 'languages', {\n",
    "                        get: () => ['en-US', 'en'],\n",
    "                    });\n",
    "                    \n",
    "                    window.chrome = {\n",
    "                        runtime: {},\n",
    "                    };\n",
    "                '''\n",
    "            })\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur cr√©ation driver: {e}\")\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b76c7499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imports termin√©s - Syst√®me anti-d√©tection pr√™t ! üïµÔ∏è\n"
     ]
    }
   ],
   "source": [
    "import undetected_chromedriver as uc\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium_stealth import stealth\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from fake_useragent import UserAgent\n",
    "import logging\n",
    "from typing import List, Dict, Optional\n",
    "import os\n",
    "\n",
    "# Configuration du logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Configuration globale\n",
    "ua = UserAgent()\n",
    "\n",
    "# Liste de proxies gratuits (remplacer par des proxies premium en production)\n",
    "PROXY_LIST = [\n",
    "    # \"http://proxy1:port\",\n",
    "    # \"http://proxy2:port\", \n",
    "    # Ajoutez vos proxies ici\n",
    "]\n",
    "\n",
    "# User agents r√©alistes\n",
    "REALISTIC_USER_AGENTS = [\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15'\n",
    "]\n",
    "\n",
    "print(\"‚úÖ Imports termin√©s - Syst√®me anti-d√©tection pr√™t ! üïµÔ∏è\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23ef9f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 16:10:40,189 - INFO - ‚úÖ MarketplaceScraper initialis√©\n",
      "2025-06-27 16:10:40,192 - INFO - üîß Initialisation du scraper anti-d√©tection...\n",
      "2025-06-27 16:10:40,192 - INFO - üîß Initialisation du scraper anti-d√©tection...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Scraper initialis√© avec succ√®s !\n",
      "üïµÔ∏è Scraper anti-d√©tection initialis√© !\n"
     ]
    }
   ],
   "source": [
    "class MarketplaceScraper:\n",
    "    \"\"\"\n",
    "    Scraper principal pour les marketplaces et reviews.\n",
    "    Supporte diff√©rentes plateformes avec rotation d'User-Agent.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, delay_range=(1, 3)):\n",
    "        self.session = requests.Session()\n",
    "        self.delay_min, self.delay_max = delay_range\n",
    "        self.ua = UserAgent()\n",
    "        \n",
    "        # Headers par d√©faut\n",
    "        self.session.headers.update({\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "            'Accept-Language': 'fr-FR,fr;q=0.9,en;q=0.8',\n",
    "            'Accept-Encoding': 'gzip, deflate',\n",
    "            'Connection': 'keep-alive',\n",
    "            'Upgrade-Insecure-Requests': '1',\n",
    "        })\n",
    "        \n",
    "        logger.info(\"‚úÖ MarketplaceScraper initialis√©\")\n",
    "    \n",
    "    def _get_page(self, url: str, retries: int = 3) -> Optional[BeautifulSoup]:\n",
    "        \"\"\"\n",
    "        R√©cup√®re une page web avec gestion d'erreurs et d√©lais.\n",
    "        \"\"\"\n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                # Rotation d'User-Agent √† chaque requ√™te\n",
    "                self.session.headers['User-Agent'] = self.ua.random\n",
    "                \n",
    "                response = self.session.get(url, timeout=10)\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                # D√©lai al√©atoire entre requ√™tes\n",
    "                delay = random.uniform(self.delay_min, self.delay_max)\n",
    "                time.sleep(delay)\n",
    "                \n",
    "                return BeautifulSoup(response.content, 'html.parser')\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Erreur tentative {attempt + 1}/{retries} pour {url}: {e}\")\n",
    "                if attempt < retries - 1:\n",
    "                    time.sleep(2 ** attempt)  # Backoff exponentiel\n",
    "                else:\n",
    "                    logger.error(f\"√âchec d√©finitif pour {url}\")\n",
    "                    return None\n",
    "    \n",
    "    def _extract_date(self, date_text: str) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Extrait et normalise les dates depuis diff√©rents formats.\n",
    "        \"\"\"\n",
    "        if not date_text:\n",
    "            return None\n",
    "            \n",
    "        date_text = date_text.strip().lower()\n",
    "        \n",
    "        # Patterns de dates fran√ßais\n",
    "        patterns = [\n",
    "            (r'(\\d{1,2})\\s+(janvier|f√©vrier|mars|avril|mai|juin|juillet|ao√ªt|septembre|octobre|novembre|d√©cembre)\\s+(\\d{4})', '%d %B %Y'),\n",
    "            (r'(\\d{1,2})/(\\d{1,2})/(\\d{4})', '%d/%m/%Y'),\n",
    "            (r'(\\d{4})-(\\d{1,2})-(\\d{1,2})', '%Y-%m-%d'),\n",
    "        ]\n",
    "        \n",
    "        # Mapping des mois fran√ßais\n",
    "        months_fr = {\n",
    "            'janvier': 'january', 'f√©vrier': 'february', 'mars': 'march',\n",
    "            'avril': 'april', 'mai': 'may', 'juin': 'june',\n",
    "            'juillet': 'july', 'ao√ªt': 'august', 'septembre': 'september',\n",
    "            'octobre': 'october', 'novembre': 'november', 'd√©cembre': 'december'\n",
    "        }\n",
    "        \n",
    "        for month_fr, month_en in months_fr.items():\n",
    "            date_text = date_text.replace(month_fr, month_en)\n",
    "        \n",
    "        for pattern, fmt in patterns:\n",
    "            match = re.search(pattern, date_text)\n",
    "            if match:\n",
    "                try:\n",
    "                    date_obj = datetime.strptime(match.group(), fmt)\n",
    "                    return date_obj.strftime('%Y-%m-%d')\n",
    "                except:\n",
    "                    continue\n",
    "        \n",
    "        return None\n",
    "\n",
    "class StealthMarketplaceScraper:\n",
    "    \"\"\"\n",
    "    Scraper anti-d√©tection pour marketplaces avec Selenium.\n",
    "    Inclut rotation d'IP, fake user agents, et comportement humain.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, headless=True, use_proxy=False):\n",
    "        self.headless = headless\n",
    "        self.use_proxy = use_proxy\n",
    "        self.driver = None\n",
    "        self.current_proxy = None\n",
    "        \n",
    "        logger.info(\"üîß Initialisation du scraper anti-d√©tection...\")\n",
    "        \n",
    "    def _get_random_user_agent(self):\n",
    "        \"\"\"Retourne un User Agent al√©atoire r√©aliste.\"\"\"\n",
    "        return random.choice(REALISTIC_USER_AGENTS)\n",
    "    \n",
    "    def _get_random_proxy(self):\n",
    "        \"\"\"Retourne un proxy al√©atoire.\"\"\"\n",
    "        if PROXY_LIST:\n",
    "            return random.choice(PROXY_LIST)\n",
    "        return None\n",
    "    \n",
    "    def _setup_driver(self):\n",
    "        \"\"\"Configure le driver Chrome avec toutes les protections anti-d√©tection.\"\"\"\n",
    "        options = Options()\n",
    "        \n",
    "        # Configuration anti-d√©tection\n",
    "        options.add_argument('--no-sandbox')\n",
    "        options.add_argument('--disable-dev-shm-usage')\n",
    "        options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "        options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "        options.add_experimental_option('useAutomationExtension', False)\n",
    "        \n",
    "        # User Agent al√©atoire\n",
    "        user_agent = self._get_random_user_agent()\n",
    "        options.add_argument(f'--user-agent={user_agent}')\n",
    "        logger.info(f\"üé≠ User Agent: {user_agent[:50]}...\")\n",
    "        \n",
    "        # Proxy si activ√©\n",
    "        if self.use_proxy:\n",
    "            proxy = self._get_random_proxy()\n",
    "            if proxy:\n",
    "                options.add_argument(f'--proxy-server={proxy}')\n",
    "                self.current_proxy = proxy\n",
    "                logger.info(f\"üåê Proxy: {proxy}\")\n",
    "        \n",
    "        # Mode headless si demand√©\n",
    "        if self.headless:\n",
    "            options.add_argument('--headless')\n",
    "        \n",
    "        # Taille de fen√™tre r√©aliste\n",
    "        options.add_argument('--window-size=1920,1080')\n",
    "        \n",
    "        # D√©sactiver les images pour plus de rapidit√© (optionnel)\n",
    "        # options.add_argument('--blink-settings=imagesEnabled=false')\n",
    "        \n",
    "        try:\n",
    "            # Utiliser undetected-chromedriver pour √©viter la d√©tection\n",
    "            self.driver = uc.Chrome(options=options)\n",
    "            \n",
    "            # Configuration Selenium Stealth pour plus de protection\n",
    "            stealth(self.driver,\n",
    "                   languages=[\"en-US\", \"en\"],\n",
    "                   vendor=\"Google Inc.\",\n",
    "                   platform=\"Win32\",\n",
    "                   webgl_vendor=\"Intel Inc.\",\n",
    "                   renderer=\"Intel Iris OpenGL Engine\",\n",
    "                   fix_hairline=True)\n",
    "            \n",
    "            # Masquer le fait que c'est un webdriver\n",
    "            self.driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
    "            \n",
    "            logger.info(\"‚úÖ Driver Chrome configur√© avec protections anti-d√©tection\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Erreur lors de la configuration du driver: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def _human_like_delay(self, min_delay=1, max_delay=3):\n",
    "        \"\"\"D√©lai al√©atoire pour simuler un comportement humain.\"\"\"\n",
    "        delay = random.uniform(min_delay, max_delay)\n",
    "        time.sleep(delay)\n",
    "    \n",
    "    def _simulate_human_behavior(self):\n",
    "        \"\"\"Simule des comportements humains al√©atoires.\"\"\"\n",
    "        actions = ActionChains(self.driver)\n",
    "        \n",
    "        # Mouvement al√©atoire de la souris\n",
    "        if random.random() < 0.3:  # 30% de chance\n",
    "            x_offset = random.randint(-100, 100)\n",
    "            y_offset = random.randint(-100, 100)\n",
    "            actions.move_by_offset(x_offset, y_offset).perform()\n",
    "            time.sleep(random.uniform(0.1, 0.5))\n",
    "        \n",
    "        # Scroll al√©atoire\n",
    "        if random.random() < 0.4:  # 40% de chance\n",
    "            scroll_amount = random.randint(100, 500)\n",
    "            self.driver.execute_script(f\"window.scrollBy(0, {scroll_amount});\")\n",
    "            time.sleep(random.uniform(0.5, 1.5))\n",
    "    \n",
    "    def start_session(self):\n",
    "        \"\"\"D√©marre une session de scraping.\"\"\"\n",
    "        self._setup_driver()\n",
    "        logger.info(\"üöÄ Session de scraping d√©marr√©e\")\n",
    "    \n",
    "    def close_session(self):\n",
    "        \"\"\"Ferme la session de scraping.\"\"\"\n",
    "        if self.driver:\n",
    "            self.driver.quit()\n",
    "            logger.info(\"üîö Session ferm√©e\")\n",
    "        \n",
    "    def get_page(self, url: str, wait_time: int = 10) -> bool:\n",
    "        \"\"\"\n",
    "        Navigue vers une page avec comportement humain simul√©.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(f\"üåê Navigation vers: {url}\")\n",
    "            \n",
    "            # Navigation avec d√©lai humain\n",
    "            self.driver.get(url)\n",
    "            self._human_like_delay(2, 4)\n",
    "            \n",
    "            # Attendre que la page se charge\n",
    "            WebDriverWait(self.driver, wait_time).until(\n",
    "                EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n",
    "            )\n",
    "            \n",
    "            # Comportement humain al√©atoire\n",
    "            self._simulate_human_behavior()\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Erreur lors de la navigation: {e}\")\n",
    "            return False\n",
    "\n",
    "# Test de la classe\n",
    "scraper = MarketplaceScraper()\n",
    "print(\"üéØ Scraper initialis√© avec succ√®s !\")\n",
    "\n",
    "# Test de la classe anti-d√©tection\n",
    "stealth_scraper = StealthMarketplaceScraper(headless=False)  # Visible pour le test\n",
    "print(\"üïµÔ∏è Scraper anti-d√©tection initialis√© !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "876bc2f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ MarketplaceProductScraper pr√™t avec anti-d√©tection compl√®te !\n"
     ]
    }
   ],
   "source": [
    "class MarketplaceProductScraper(StealthMarketplaceScraper):\n",
    "    \"\"\"\n",
    "    Scraper sp√©cialis√© pour produits et reviews de marketplaces.\n",
    "    \"\"\"\n",
    "    \n",
    "    def scrape_amazon_style_products(self, search_term: str, max_pages: int = 3) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Scrape des produits type Amazon (utilise un site de d√©monstration).\n",
    "        \"\"\"\n",
    "        logger.info(f\"üõçÔ∏è Scraping produits pour: {search_term}\")\n",
    "        \n",
    "        if not self.driver:\n",
    "            self.start_session()\n",
    "        \n",
    "        products_data = []\n",
    "        \n",
    "        try:\n",
    "            # Utiliser un site de d√©monstration e-commerce\n",
    "            base_url = \"https://webscraper.io/test-sites/e-commerce/allinone\"\n",
    "            \n",
    "            if not self.get_page(base_url):\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            # Attendre et trouver les produits\n",
    "            products = WebDriverWait(self.driver, 10).until(\n",
    "                EC.presence_of_all_elements_located((By.CSS_SELECTOR, \".product-wrapper\"))\n",
    "            )\n",
    "            \n",
    "            for i, product in enumerate(products[:20]):  # Limiter √† 20 produits\n",
    "                try:\n",
    "                    # Simuler un comportement humain\n",
    "                    if i % 5 == 0:\n",
    "                        self._simulate_human_behavior()\n",
    "                    \n",
    "                    # Extraire les donn√©es du produit\n",
    "                    title_elem = product.find_element(By.CSS_SELECTOR, \".title\")\n",
    "                    price_elem = product.find_element(By.CSS_SELECTOR, \".price\")\n",
    "                    \n",
    "                    title = title_elem.text.strip()\n",
    "                    price_text = price_elem.text.strip()\n",
    "                    \n",
    "                    # Nettoyer le prix\n",
    "                    price = re.findall(r'[\\d.]+', price_text)\n",
    "                    price = float(price[0]) if price else 0.0\n",
    "                    \n",
    "                    # Essayer de trouver la description et l'image\n",
    "                    try:\n",
    "                        desc_elem = product.find_element(By.CSS_SELECTOR, \".description\")\n",
    "                        description = desc_elem.text.strip()\n",
    "                    except:\n",
    "                        description = \"\"\n",
    "                    \n",
    "                    try:\n",
    "                        img_elem = product.find_element(By.CSS_SELECTOR, \"img\")\n",
    "                        image_url = img_elem.get_attribute(\"src\")\n",
    "                    except:\n",
    "                        image_url = \"\"\n",
    "                    \n",
    "                    products_data.append({\n",
    "                        'product_id': f\"demo_{i}\",\n",
    "                        'title': title,\n",
    "                        'price': price,\n",
    "                        'description': description,\n",
    "                        'image_url': image_url,\n",
    "                        'source': 'webscraper.io',\n",
    "                        'search_term': search_term,\n",
    "                        'scraped_at': datetime.now().isoformat(),\n",
    "                        'user_agent': self.driver.execute_script(\"return navigator.userAgent;\"),\n",
    "                        'proxy': self.current_proxy\n",
    "                    })\n",
    "                    \n",
    "                    # D√©lai humain entre produits\n",
    "                    self._human_like_delay(0.5, 1.5)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Erreur extraction produit {i}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            logger.info(f\"‚úÖ {len(products_data)} produits extraits\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Erreur scraping produits: {e}\")\n",
    "        \n",
    "        return pd.DataFrame(products_data)\n",
    "    \n",
    "    def scrape_product_reviews(self, product_url: str, max_reviews: int = 50) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Scrape les reviews d'un produit avec dates et sentiments.\n",
    "        \"\"\"\n",
    "        logger.info(f\"üìù Scraping reviews pour: {product_url}\")\n",
    "        \n",
    "        if not self.driver:\n",
    "            self.start_session()\n",
    "        \n",
    "        reviews_data = []\n",
    "        \n",
    "        try:\n",
    "            # Aller sur la page du produit\n",
    "            if not self.get_page(product_url):\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            # Simuler des reviews (site de d√©monstration n'a pas de vraies reviews)\n",
    "            # En production, adapter les s√©lecteurs CSS selon le site cible\n",
    "            \n",
    "            for i in range(min(max_reviews, 20)):  # Simuler jusqu'√† 20 reviews\n",
    "                # G√©n√©rer des reviews r√©alistes pour la d√©monstration\n",
    "                review_texts = [\n",
    "                    \"Great product, highly recommend!\",\n",
    "                    \"Good value for money, works as expected.\",\n",
    "                    \"Not bad but could be better quality.\",\n",
    "                    \"Excellent service and fast delivery!\",\n",
    "                    \"Product broke after a few days, disappointed.\",\n",
    "                    \"Amazing quality, will buy again!\",\n",
    "                    \"Okay product, nothing special.\",\n",
    "                    \"Love it! Exactly what I was looking for.\",\n",
    "                    \"Poor quality, would not recommend.\",\n",
    "                    \"Perfect! Exceeded my expectations.\"\n",
    "                ]\n",
    "                \n",
    "                reviewer_names = [\n",
    "                    \"John D.\", \"Sarah M.\", \"Mike K.\", \"Emma L.\", \"David R.\",\n",
    "                    \"Lisa P.\", \"Tom W.\", \"Anna S.\", \"Chris B.\", \"Maria G.\"\n",
    "                ]\n",
    "                \n",
    "                # Simuler une review\n",
    "                review_text = random.choice(review_texts)\n",
    "                reviewer = random.choice(reviewer_names)\n",
    "                rating = random.randint(1, 5)\n",
    "                \n",
    "                # G√©n√©rer une date r√©aliste (derniers 6 mois)\n",
    "                days_ago = random.randint(1, 180)\n",
    "                review_date = (datetime.now() - timedelta(days=days_ago)).strftime('%Y-%m-%d')\n",
    "                \n",
    "                reviews_data.append({\n",
    "                    'review_id': f\"review_{i}\",\n",
    "                    'product_url': product_url,\n",
    "                    'reviewer_name': reviewer,\n",
    "                    'review_text': review_text,\n",
    "                    'rating': rating,\n",
    "                    'review_date': review_date,\n",
    "                    'helpful_votes': random.randint(0, 50),\n",
    "                    'verified_purchase': random.choice([True, False]),\n",
    "                    'scraped_at': datetime.now().isoformat(),\n",
    "                    'source': 'demo_marketplace'\n",
    "                })\n",
    "                \n",
    "                # D√©lai humain\n",
    "                self._human_like_delay(0.3, 1.0)\n",
    "            \n",
    "            logger.info(f\"‚úÖ {len(reviews_data)} reviews g√©n√©r√©es (d√©monstration)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Erreur scraping reviews: {e}\")\n",
    "        \n",
    "        return pd.DataFrame(reviews_data)\n",
    "    \n",
    "    def scrape_trustpilot_reviews(self, company_name: str, max_reviews: int = 100) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Scrape des reviews Trustpilot avec anti-d√©tection.\n",
    "        \"\"\"\n",
    "        logger.info(f\"‚≠ê Scraping Trustpilot pour: {company_name}\")\n",
    "        \n",
    "        if not self.driver:\n",
    "            self.start_session()\n",
    "        \n",
    "        reviews_data = []\n",
    "        \n",
    "        try:\n",
    "            # URL Trustpilot\n",
    "            url = f\"https://www.trustpilot.com/review/{company_name.lower().replace(' ', '-')}\"\n",
    "            \n",
    "            if not self.get_page(url):\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            # Attendre que les reviews se chargent\n",
    "            time.sleep(3)\n",
    "            \n",
    "            # Chercher les reviews (s√©lecteurs peuvent changer)\n",
    "            try:\n",
    "                reviews = self.driver.find_elements(By.CSS_SELECTOR, \"[data-service-review-card-paper]\")\n",
    "                \n",
    "                for i, review in enumerate(reviews[:max_reviews]):\n",
    "                    try:\n",
    "                        # Simuler comportement humain\n",
    "                        if i % 10 == 0:\n",
    "                            self._simulate_human_behavior()\n",
    "                        \n",
    "                        # Extraire les donn√©es (adapter selon le HTML actuel)\n",
    "                        review_text = review.find_element(By.CSS_SELECTOR, \"[data-service-review-text-typography]\").text\n",
    "                        rating_elem = review.find_element(By.CSS_SELECTOR, \"[data-service-review-rating]\")\n",
    "                        rating = len(rating_elem.find_elements(By.CSS_SELECTOR, \"svg[data-star-fill='true']\"))\n",
    "                        \n",
    "                        # Date de la review\n",
    "                        try:\n",
    "                            date_elem = review.find_element(By.CSS_SELECTOR, \"time\")\n",
    "                            review_date = date_elem.get_attribute(\"datetime\")[:10]\n",
    "                        except:\n",
    "                            review_date = datetime.now().strftime('%Y-%m-%d')\n",
    "                        \n",
    "                        # Nom du reviewer\n",
    "                        try:\n",
    "                            name_elem = review.find_element(By.CSS_SELECTOR, \"[data-consumer-name-typography]\")\n",
    "                            reviewer_name = name_elem.text\n",
    "                        except:\n",
    "                            reviewer_name = f\"Anonymous_{i}\"\n",
    "                        \n",
    "                        reviews_data.append({\n",
    "                            'review_id': f\"trustpilot_{company_name}_{i}\",\n",
    "                            'company': company_name,\n",
    "                            'reviewer_name': reviewer_name,\n",
    "                            'review_text': review_text,\n",
    "                            'rating': rating,\n",
    "                            'review_date': review_date,\n",
    "                            'source': 'trustpilot.com',\n",
    "                            'scraped_at': datetime.now().isoformat()\n",
    "                        })\n",
    "                        \n",
    "                        self._human_like_delay(0.5, 2.0)\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"Erreur extraction review {i}: {e}\")\n",
    "                        continue\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Aucune review trouv√©e ou structure HTML diff√©rente: {e}\")\n",
    "            \n",
    "            logger.info(f\"‚úÖ {len(reviews_data)} reviews Trustpilot extraites\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Erreur scraping Trustpilot: {e}\")\n",
    "        \n",
    "        return pd.DataFrame(reviews_data)\n",
    "\n",
    "# Initialisation du scraper\n",
    "print(\"üéØ MarketplaceProductScraper pr√™t avec anti-d√©tection compl√®te !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "368873ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 16:10:40,504 - INFO - üìÅ Dossiers de donn√©es cr√©√©s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Fonctions utilitaires charg√©es !\n"
     ]
    }
   ],
   "source": [
    "# Fonctions utilitaires pour la sauvegarde et l'analyse\n",
    "def save_scraped_data(df: pd.DataFrame, filename: str, data_dir: str = \"../data/raw\"):\n",
    "    \"\"\"Sauvegarde les donn√©es scrap√©es avec timestamp.\"\"\"\n",
    "    if df.empty:\n",
    "        logger.warning(\"Aucune donn√©e √† sauvegarder\")\n",
    "        return\n",
    "    \n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filepath = os.path.join(data_dir, f\"{timestamp}_{filename}\")\n",
    "    \n",
    "    df.to_csv(filepath, index=False, encoding='utf-8')\n",
    "    logger.info(f\"üíæ Donn√©es sauvegard√©es: {filepath} ({len(df)} enregistrements)\")\n",
    "    \n",
    "    return filepath\n",
    "\n",
    "def analyze_scraped_data(df: pd.DataFrame):\n",
    "    \"\"\"Analyse rapide des donn√©es scrap√©es.\"\"\"\n",
    "    if df.empty:\n",
    "        print(\"‚ùå Aucune donn√©e √† analyser\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nüìä Analyse des donn√©es scrap√©es:\")\n",
    "    print(f\"   Total des enregistrements: {len(df)}\")\n",
    "    print(f\"   Colonnes: {list(df.columns)}\")\n",
    "    \n",
    "    if 'source' in df.columns:\n",
    "        print(f\"   Sources: {df['source'].value_counts().to_dict()}\")\n",
    "    \n",
    "    if 'rating' in df.columns:\n",
    "        print(f\"   Note moyenne: {df['rating'].mean():.2f}\")\n",
    "        print(f\"   Distribution des notes: {df['rating'].value_counts().sort_index().to_dict()}\")\n",
    "    \n",
    "    if 'price' in df.columns:\n",
    "        print(f\"   Prix moyen: ${df['price'].mean():.2f}\")\n",
    "        print(f\"   Prix min/max: ${df['price'].min():.2f} - ${df['price'].max():.2f}\")\n",
    "    \n",
    "    print(f\"\\nüîç Aper√ßu des donn√©es:\")\n",
    "    return df.head()\n",
    "\n",
    "def setup_data_directories():\n",
    "    \"\"\"Cr√©e les dossiers n√©cessaires pour les donn√©es.\"\"\"\n",
    "    directories = [\"../data/raw\", \"../data/processed\", \"../logs\"]\n",
    "    for directory in directories:\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "        \n",
    "    logger.info(\"üìÅ Dossiers de donn√©es cr√©√©s\")\n",
    "\n",
    "# Configuration initiale\n",
    "setup_data_directories()\n",
    "print(\"üéØ Fonctions utilitaires charg√©es !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5bdaf40c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üö® Pour lancer le test, ex√©cutez: test_results = test_marketplace_scraper()\n",
      "‚ö†Ô∏è  Assurez-vous d'avoir Chrome install√© et une connexion internet stable\n"
     ]
    }
   ],
   "source": [
    "# üß™ TEST DU SCRAPER - D√©monstration compl√®te\n",
    "def test_marketplace_scraper():\n",
    "    \"\"\"\n",
    "    Test complet du scraper avec toutes les protections anti-d√©tection.\n",
    "    \"\"\"\n",
    "    logger.info(\"üß™ D√©marrage des tests du scraper...\")\n",
    "    \n",
    "    # Initialiser le scraper (headless=False pour voir le navigateur)\n",
    "    scraper = MarketplaceProductScraper(headless=False, use_proxy=False)\n",
    "    \n",
    "    try:\n",
    "        # D√©marrer la session\n",
    "        scraper.start_session()\n",
    "        \n",
    "        # Test 1: Scraper des produits\n",
    "        print(\"\\nüõçÔ∏è Test 1: Scraping de produits...\")\n",
    "        products_df = scraper.scrape_amazon_style_products(\"laptop\", max_pages=1)\n",
    "        \n",
    "        if not products_df.empty:\n",
    "            save_scraped_data(products_df, \"products_demo.csv\")\n",
    "            analyze_scraped_data(products_df)\n",
    "        \n",
    "        # Test 2: Scraper des reviews (simul√©es)\n",
    "        print(\"\\nüìù Test 2: Scraping de reviews...\")\n",
    "        reviews_df = scraper.scrape_product_reviews(\"https://webscraper.io/test-sites/e-commerce/allinone\", max_reviews=10)\n",
    "        \n",
    "        if not reviews_df.empty:\n",
    "            save_scraped_data(reviews_df, \"reviews_demo.csv\")\n",
    "            analyze_scraped_data(reviews_df)\n",
    "        \n",
    "        # Test 3: Trustpilot (optionnel - n√©cessite une vraie entreprise)\n",
    "        # print(\"\\n‚≠ê Test 3: Scraping Trustpilot...\")\n",
    "        # trustpilot_df = scraper.scrape_trustpilot_reviews(\"amazon\", max_reviews=5)\n",
    "        \n",
    "        print(\"\\n‚úÖ Tests termin√©s avec succ√®s !\")\n",
    "        \n",
    "        return {\n",
    "            'products': products_df,\n",
    "            'reviews': reviews_df\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Erreur durant les tests: {e}\")\n",
    "        return None\n",
    "        \n",
    "    finally:\n",
    "        # Toujours fermer le navigateur\n",
    "        scraper.close_session()\n",
    "\n",
    "# ‚ö†Ô∏è ATTENTION: D√©commentez la ligne suivante pour lancer le test\n",
    "# Cela ouvrira un navigateur Chrome et commencera le scraping\n",
    "print(\"üö® Pour lancer le test, ex√©cutez: test_results = test_marketplace_scraper()\")\n",
    "print(\"‚ö†Ô∏è  Assurez-vous d'avoir Chrome install√© et une connexion internet stable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ace7ab4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç DIAGNOSTIC CHROME BINARY...\n",
      "‚úÖ Chrome trouv√©: C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe\n",
      "üîß Configuration driver corrig√©...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 16:10:42,867 - INFO - patching driver executable C:\\Users\\Yann\\appdata\\roaming\\undetected_chromedriver\\undetected_chromedriver.exe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Driver cr√©√© avec succ√®s !\n",
      "üéâ TEST RAPIDE...\n",
      "‚úÖ Navigation fonctionne !\n",
      "‚úÖ Navigation fonctionne !\n"
     ]
    }
   ],
   "source": [
    "# CELLULE: FIX CHROME BINARY LOCATION\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def fix_chrome_binary():\n",
    "    \"\"\"R√©pare la configuration Chrome Binary\"\"\"\n",
    "    \n",
    "    print(\"üîç DIAGNOSTIC CHROME BINARY...\")\n",
    "    \n",
    "    # 1. Trouver Chrome automatiquement\n",
    "    possible_chrome_paths = [\n",
    "        r\"C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe\",\n",
    "        r\"C:\\Program Files (x86)\\Google\\Chrome\\Application\\chrome.exe\",\n",
    "        r\"C:\\Users\\{}\\AppData\\Local\\Google\\Chrome\\Application\\chrome.exe\".format(os.getenv('USERNAME')),\n",
    "        r\"C:\\Program Files\\BraveSoftware\\Brave-Browser\\Application\\brave.exe\",\n",
    "        r\"C:\\Program Files (x86)\\Microsoft\\Edge\\Application\\msedge.exe\"\n",
    "    ]\n",
    "    \n",
    "    chrome_path = None\n",
    "    for path in possible_chrome_paths:\n",
    "        if os.path.exists(path):\n",
    "            chrome_path = path\n",
    "            print(f\"‚úÖ Chrome trouv√©: {path}\")\n",
    "            break\n",
    "    \n",
    "    if not chrome_path:\n",
    "        print(\"‚ùå Chrome non trouv√© - Installation automatique...\")\n",
    "        install_chrome()\n",
    "        return\n",
    "    \n",
    "    # 2. Configuration Chrome corrig√©e\n",
    "    return create_fixed_driver(chrome_path)\n",
    "\n",
    "def install_chrome():\n",
    "    \"\"\"Installe Chrome automatiquement\"\"\"\n",
    "    \n",
    "    print(\"üì• Installation Chrome...\")\n",
    "    \n",
    "    # URL de t√©l√©chargement Chrome\n",
    "    chrome_url = \"https://dl.google.com/chrome/install/latest/chrome_installer.exe\"\n",
    "    \n",
    "    try:\n",
    "        import requests\n",
    "        response = requests.get(chrome_url)\n",
    "        \n",
    "        installer_path = \"chrome_installer.exe\"\n",
    "        with open(installer_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        \n",
    "        # Lancer l'installation\n",
    "        subprocess.run([installer_path, '/silent', '/install'], check=True)\n",
    "        os.remove(installer_path)\n",
    "        \n",
    "        print(\"‚úÖ Chrome install√© avec succ√®s !\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur installation: {e}\")\n",
    "        print(\"üîó Installez manuellement: https://www.google.com/chrome/\")\n",
    "\n",
    "def create_fixed_driver(chrome_path):\n",
    "    \"\"\"Cr√©e un driver avec le bon chemin Chrome\"\"\"\n",
    "    \n",
    "    print(\"üîß Configuration driver corrig√©...\")\n",
    "    \n",
    "    try:\n",
    "        import undetected_chromedriver as uc\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        \n",
    "        # Options Chrome corrig√©es\n",
    "        options = Options()\n",
    "        options.binary_location = str(chrome_path)  # FORCE STRING\n",
    "        options.add_argument('--no-sandbox')\n",
    "        options.add_argument('--disable-dev-shm-usage')\n",
    "        options.add_argument('--disable-gpu')\n",
    "        options.add_argument('--remote-debugging-port=9222')\n",
    "        options.add_argument('--disable-web-security')\n",
    "        options.add_argument('--disable-features=VizDisplayCompositor')\n",
    "        \n",
    "        # Driver undetected avec options corrig√©es\n",
    "        driver = uc.Chrome(\n",
    "            options=options,\n",
    "            driver_executable_path=None,  # Auto-detection\n",
    "            browser_executable_path=chrome_path,  # Path explicite\n",
    "            version_main=None,  # Auto-detect version\n",
    "            headless=False\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ Driver cr√©√© avec succ√®s !\")\n",
    "        return driver\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur driver: {e}\")\n",
    "        return None\n",
    "\n",
    "# EX√âCUTER LA R√âPARATION\n",
    "fixed_driver = fix_chrome_binary()\n",
    "\n",
    "if fixed_driver:\n",
    "    print(\"üéâ TEST RAPIDE...\")\n",
    "    try:\n",
    "        fixed_driver.get(\"https://httpbin.org/headers\")\n",
    "        print(\"‚úÖ Navigation fonctionne !\")\n",
    "        fixed_driver.quit()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Test √©chou√©: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f908d2be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 16:10:56,654 - INFO - üß™ D√©marrage des tests du scraper...\n",
      "2025-06-27 16:10:56,655 - INFO - üîß Initialisation du scraper anti-d√©tection...\n",
      "2025-06-27 16:10:56,655 - INFO - üé≠ User Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0...\n",
      "2025-06-27 16:10:56,655 - INFO - üîß Initialisation du scraper anti-d√©tection...\n",
      "2025-06-27 16:10:56,655 - INFO - üé≠ User Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0...\n",
      "2025-06-27 16:10:58,548 - INFO - patching driver executable C:\\Users\\Yann\\appdata\\roaming\\undetected_chromedriver\\undetected_chromedriver.exe\n",
      "2025-06-27 16:10:58,548 - INFO - patching driver executable C:\\Users\\Yann\\appdata\\roaming\\undetected_chromedriver\\undetected_chromedriver.exe\n",
      "2025-06-27 16:10:59,610 - ERROR - ‚ùå Erreur lors de la configuration du driver: Message: invalid argument: cannot parse capability: goog:chromeOptions\n",
      "from invalid argument: unrecognized chrome option: excludeSwitches\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x0x384493+62419]\n",
      "\tGetHandleVerifier [0x0x3844d4+62484]\n",
      "\t(No symbol) [0x0x1c2133]\n",
      "\t(No symbol) [0x0x1e9723]\n",
      "\t(No symbol) [0x0x1eaeb0]\n",
      "\t(No symbol) [0x0x1e5fea]\n",
      "\t(No symbol) [0x0x239832]\n",
      "\t(No symbol) [0x0x23931c]\n",
      "\t(No symbol) [0x0x23aa20]\n",
      "\t(No symbol) [0x0x23a82a]\n",
      "\t(No symbol) [0x0x22f266]\n",
      "\t(No symbol) [0x0x1fe852]\n",
      "\t(No symbol) [0x0x1ff6f4]\n",
      "\tGetHandleVerifier [0x0x5f4773+2619059]\n",
      "\tGetHandleVerifier [0x0x5efb8a+2599626]\n",
      "\tGetHandleVerifier [0x0x3ab03a+221050]\n",
      "\tGetHandleVerifier [0x0x39b2b8+156152]\n",
      "\tGetHandleVerifier [0x0x3a1c6d+183213]\n",
      "\tGetHandleVerifier [0x0x38c378+94904]\n",
      "\tGetHandleVerifier [0x0x38c502+95298]\n",
      "\tGetHandleVerifier [0x0x37765a+9626]\n",
      "\tBaseThreadInitThunk [0x0x76775d49+25]\n",
      "\tRtlInitializeExceptionChain [0x0x778ed09b+107]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x0x778ed021+561]\n",
      "\n",
      "2025-06-27 16:10:59,611 - ERROR - ‚ùå Erreur durant les tests: Message: invalid argument: cannot parse capability: goog:chromeOptions\n",
      "from invalid argument: unrecognized chrome option: excludeSwitches\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x0x384493+62419]\n",
      "\tGetHandleVerifier [0x0x3844d4+62484]\n",
      "\t(No symbol) [0x0x1c2133]\n",
      "\t(No symbol) [0x0x1e9723]\n",
      "\t(No symbol) [0x0x1eaeb0]\n",
      "\t(No symbol) [0x0x1e5fea]\n",
      "\t(No symbol) [0x0x239832]\n",
      "\t(No symbol) [0x0x23931c]\n",
      "\t(No symbol) [0x0x23aa20]\n",
      "\t(No symbol) [0x0x23a82a]\n",
      "\t(No symbol) [0x0x22f266]\n",
      "\t(No symbol) [0x0x1fe852]\n",
      "\t(No symbol) [0x0x1ff6f4]\n",
      "\tGetHandleVerifier [0x0x5f4773+2619059]\n",
      "\tGetHandleVerifier [0x0x5efb8a+2599626]\n",
      "\tGetHandleVerifier [0x0x3ab03a+221050]\n",
      "\tGetHandleVerifier [0x0x39b2b8+156152]\n",
      "\tGetHandleVerifier [0x0x3a1c6d+183213]\n",
      "\tGetHandleVerifier [0x0x38c378+94904]\n",
      "\tGetHandleVerifier [0x0x38c502+95298]\n",
      "\tGetHandleVerifier [0x0x37765a+9626]\n",
      "\tBaseThreadInitThunk [0x0x76775d49+25]\n",
      "\tRtlInitializeExceptionChain [0x0x778ed09b+107]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x0x778ed021+561]\n",
      "\n",
      "2025-06-27 16:10:59,610 - ERROR - ‚ùå Erreur lors de la configuration du driver: Message: invalid argument: cannot parse capability: goog:chromeOptions\n",
      "from invalid argument: unrecognized chrome option: excludeSwitches\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x0x384493+62419]\n",
      "\tGetHandleVerifier [0x0x3844d4+62484]\n",
      "\t(No symbol) [0x0x1c2133]\n",
      "\t(No symbol) [0x0x1e9723]\n",
      "\t(No symbol) [0x0x1eaeb0]\n",
      "\t(No symbol) [0x0x1e5fea]\n",
      "\t(No symbol) [0x0x239832]\n",
      "\t(No symbol) [0x0x23931c]\n",
      "\t(No symbol) [0x0x23aa20]\n",
      "\t(No symbol) [0x0x23a82a]\n",
      "\t(No symbol) [0x0x22f266]\n",
      "\t(No symbol) [0x0x1fe852]\n",
      "\t(No symbol) [0x0x1ff6f4]\n",
      "\tGetHandleVerifier [0x0x5f4773+2619059]\n",
      "\tGetHandleVerifier [0x0x5efb8a+2599626]\n",
      "\tGetHandleVerifier [0x0x3ab03a+221050]\n",
      "\tGetHandleVerifier [0x0x39b2b8+156152]\n",
      "\tGetHandleVerifier [0x0x3a1c6d+183213]\n",
      "\tGetHandleVerifier [0x0x38c378+94904]\n",
      "\tGetHandleVerifier [0x0x38c502+95298]\n",
      "\tGetHandleVerifier [0x0x37765a+9626]\n",
      "\tBaseThreadInitThunk [0x0x76775d49+25]\n",
      "\tRtlInitializeExceptionChain [0x0x778ed09b+107]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x0x778ed021+561]\n",
      "\n",
      "2025-06-27 16:10:59,611 - ERROR - ‚ùå Erreur durant les tests: Message: invalid argument: cannot parse capability: goog:chromeOptions\n",
      "from invalid argument: unrecognized chrome option: excludeSwitches\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x0x384493+62419]\n",
      "\tGetHandleVerifier [0x0x3844d4+62484]\n",
      "\t(No symbol) [0x0x1c2133]\n",
      "\t(No symbol) [0x0x1e9723]\n",
      "\t(No symbol) [0x0x1eaeb0]\n",
      "\t(No symbol) [0x0x1e5fea]\n",
      "\t(No symbol) [0x0x239832]\n",
      "\t(No symbol) [0x0x23931c]\n",
      "\t(No symbol) [0x0x23aa20]\n",
      "\t(No symbol) [0x0x23a82a]\n",
      "\t(No symbol) [0x0x22f266]\n",
      "\t(No symbol) [0x0x1fe852]\n",
      "\t(No symbol) [0x0x1ff6f4]\n",
      "\tGetHandleVerifier [0x0x5f4773+2619059]\n",
      "\tGetHandleVerifier [0x0x5efb8a+2599626]\n",
      "\tGetHandleVerifier [0x0x3ab03a+221050]\n",
      "\tGetHandleVerifier [0x0x39b2b8+156152]\n",
      "\tGetHandleVerifier [0x0x3a1c6d+183213]\n",
      "\tGetHandleVerifier [0x0x38c378+94904]\n",
      "\tGetHandleVerifier [0x0x38c502+95298]\n",
      "\tGetHandleVerifier [0x0x37765a+9626]\n",
      "\tBaseThreadInitThunk [0x0x76775d49+25]\n",
      "\tRtlInitializeExceptionChain [0x0x778ed09b+107]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x0x778ed021+561]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_results = test_marketplace_scraper()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bac6014",
   "metadata": {},
   "source": [
    "## üîß Configurations Avanc√©es\n",
    "\n",
    "### Rotation de Proxies\n",
    "Pour ajouter des proxies et √©viter les bans IP :\n",
    "\n",
    "```python\n",
    "PROXY_LIST = [\n",
    "    \"http://username:password@proxy1.com:8080\",\n",
    "    \"http://username:password@proxy2.com:8080\", \n",
    "    \"socks5://proxy3.com:1080\"\n",
    "]\n",
    "```\n",
    "\n",
    "### Sites Support√©s (Adaptables)\n",
    "- **E-commerce**: Amazon, eBay, Shopify stores\n",
    "- **Reviews**: Trustpilot, Google Reviews, Yelp\n",
    "- **Social Commerce**: Facebook Marketplace, Instagram Shopping\n",
    "\n",
    "### ‚ö†Ô∏è Consid√©rations L√©gales et √âthiques\n",
    "1. **Respectez les robots.txt** des sites\n",
    "2. **Limitez la fr√©quence** des requ√™tes\n",
    "3. **Utilisez des APIs officielles** quand disponibles\n",
    "4. **Respectez les ToS** des plateformes\n",
    "5. **Ne surchargez pas** les serveurs\n",
    "\n",
    "### üõ°Ô∏è Protections Impl√©ment√©es\n",
    "- ‚úÖ **User-Agent Rotation** - 5+ agents r√©alistes\n",
    "- ‚úÖ **D√©lais Humains** - 1-3s entre actions\n",
    "- ‚úÖ **Comportement Humain** - Mouvements souris, scroll\n",
    "- ‚úÖ **Headers R√©alistes** - Accept, Language, etc.\n",
    "- ‚úÖ **Selenium Stealth** - √âvite la d√©tection webdriver\n",
    "- ‚úÖ **Proxy Support** - Rotation d'IP\n",
    "- ‚úÖ **Error Handling** - Retry automatique\n",
    "- ‚úÖ **Session Management** - Cookies et state\n",
    "\n",
    "### üìä Donn√©es R√©cup√©r√©es\n",
    "- **Produits**: Titre, prix, description, images, ratings\n",
    "- **Reviews**: Texte, note, date, nom reviewer, votes utiles\n",
    "- **M√©tadonn√©es**: Source, timestamp, user-agent, proxy utilis√©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "587a9ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üö® Pour lancer le scraping, ex√©cutez:\n",
      "   results = quick_scrape('smartphone')\n",
      "   results = quick_scrape('headphones')\n",
      "\n",
      "üí° Personnalisez CONFIG au-dessus pour adapter le comportement\n"
     ]
    }
   ],
   "source": [
    "# üöÄ LANCEMENT RAPIDE - Modifiez selon vos besoins\n",
    "\n",
    "# Configuration personnalisable\n",
    "CONFIG = {\n",
    "    'headless': False,          # True = invisible, False = visible (pour d√©buguer)\n",
    "    'use_proxy': False,         # True si vous avez configur√© des proxies\n",
    "    'max_products': 20,         # Nombre max de produits √† scraper\n",
    "    'max_reviews': 50,          # Nombre max de reviews par produit\n",
    "    'delay_min': 1,            # D√©lai minimum entre actions (secondes)\n",
    "    'delay_max': 3,            # D√©lai maximum entre actions (secondes)\n",
    "    'save_data': True          # Sauvegarder automatiquement les donn√©es\n",
    "}\n",
    "\n",
    "def quick_scrape(search_term: str = \"laptop\", company_name: str = \"amazon\"):\n",
    "    \"\"\"\n",
    "    Fonction de scraping rapide avec configuration personnalisable.\n",
    "    \n",
    "    Args:\n",
    "        search_term: Terme de recherche pour les produits\n",
    "        company_name: Nom d'entreprise pour les reviews Trustpilot\n",
    "    \"\"\"\n",
    "    print(f\"üîç D√©marrage du scraping pour: {search_term}\")\n",
    "    \n",
    "    # Initialiser le scraper avec la config\n",
    "    scraper = MarketplaceProductScraper(\n",
    "        headless=CONFIG['headless'], \n",
    "        use_proxy=CONFIG['use_proxy']\n",
    "    )\n",
    "    \n",
    "    all_data = {}\n",
    "    \n",
    "    try:\n",
    "        scraper.start_session()\n",
    "        \n",
    "        # Scraping des produits\n",
    "        print(\"üõçÔ∏è Scraping des produits...\")\n",
    "        products = scraper.scrape_amazon_style_products(\n",
    "            search_term, \n",
    "            max_pages=1\n",
    "        )\n",
    "        \n",
    "        if not products.empty and CONFIG['save_data']:\n",
    "            filepath = save_scraped_data(products, f\"products_{search_term}.csv\")\n",
    "            all_data['products'] = products\n",
    "            print(f\"üìÅ Produits sauvegard√©s: {len(products)} items\")\n",
    "        \n",
    "        # Scraping des reviews simul√©es\n",
    "        print(\"üìù Scraping des reviews...\")\n",
    "        reviews = scraper.scrape_product_reviews(\n",
    "            \"https://webscraper.io/test-sites/e-commerce/allinone\",\n",
    "            max_reviews=CONFIG['max_reviews']\n",
    "        )\n",
    "        \n",
    "        if not reviews.empty and CONFIG['save_data']:\n",
    "            filepath = save_scraped_data(reviews, f\"reviews_{search_term}.csv\")\n",
    "            all_data['reviews'] = reviews\n",
    "            print(f\"üìÅ Reviews sauvegard√©es: {len(reviews)} items\")\n",
    "        \n",
    "        # Affichage des r√©sultats\n",
    "        print(\"\\nüìä R√©sultats du scraping:\")\n",
    "        for data_type, df in all_data.items():\n",
    "            print(f\"   {data_type}: {len(df)} enregistrements\")\n",
    "            \n",
    "        return all_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur: {e}\")\n",
    "        return None\n",
    "        \n",
    "    finally:\n",
    "        scraper.close_session()\n",
    "\n",
    "# üéØ EX√âCUTION\n",
    "print(\"üö® Pour lancer le scraping, ex√©cutez:\")\n",
    "print(\"   results = quick_scrape('smartphone')\")\n",
    "print(\"   results = quick_scrape('headphones')\")\n",
    "print(\"\\nüí° Personnalisez CONFIG au-dessus pour adapter le comportement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8846d2d4",
   "metadata": {},
   "source": [
    "## üéØ **SITES R√âELS vs D√âMONSTRATION**\n",
    "\n",
    "### ‚ùå **Ce qui est actuellement en DEMO :**\n",
    "- `webscraper.io/test-sites` - Site de test pour apprendre\n",
    "- Reviews simul√©es avec `random.choice()` \n",
    "- Donn√©es g√©n√©r√©es al√©atoirement\n",
    "\n",
    "### ‚úÖ **Ce qui est R√âEL :**\n",
    "- Structure anti-d√©tection Selenium\n",
    "- Rotation User-Agent r√©elle\n",
    "- Support proxy fonctionnel\n",
    "- Gestion des d√©lais humains\n",
    "\n",
    "### üö® **VRAIES IMPL√âMENTATIONS ci-dessous :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "75734744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üõí VRAI scraper Amazon avec vraies balises CSS cr√©√© !\n"
     ]
    }
   ],
   "source": [
    "# üõí VRAIE IMPL√âMENTATION AMAZON - S√©lecteurs CSS r√©els\n",
    "class RealAmazonScraper(StealthMarketplaceScraper):\n",
    "    \"\"\"\n",
    "    Scraper pour le VRAI Amazon avec vraies balises CSS.\n",
    "    ‚ö†Ô∏è ATTENTION: Utilisez avec mod√©ration pour respecter les ToS d'Amazon\n",
    "    \"\"\"\n",
    "    \n",
    "    def scrape_real_amazon_products(self, search_term: str, max_pages: int = 2) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Scrape de vrais produits Amazon avec vraies balises.\n",
    "        \"\"\"\n",
    "        logger.info(f\"üõí VRAI scraping Amazon pour: {search_term}\")\n",
    "        \n",
    "        if not self.driver:\n",
    "            self.start_session()\n",
    "        \n",
    "        products_data = []\n",
    "        \n",
    "        try:\n",
    "            for page in range(1, max_pages + 1):\n",
    "                # VRAIE URL Amazon avec pagination\n",
    "                url = f\"https://www.amazon.com/s?k={search_term}&page={page}\"\n",
    "                \n",
    "                if not self.get_page(url):\n",
    "                    continue\n",
    "                \n",
    "                # Attendre que les produits se chargent\n",
    "                time.sleep(3)\n",
    "                \n",
    "                # VRAIES balises CSS Amazon (mises √† jour 2024/2025)\n",
    "                product_containers = self.driver.find_elements(\n",
    "                    By.CSS_SELECTOR, \n",
    "                    \"[data-component-type='s-search-result']\"\n",
    "                )\n",
    "                \n",
    "                for i, container in enumerate(product_containers):\n",
    "                    try:\n",
    "                        # Titre du produit - VRAIE balise Amazon\n",
    "                        title_elem = container.find_element(\n",
    "                            By.CSS_SELECTOR, \n",
    "                            \"h2 a span, .a-size-mini span, .a-size-base-plus\"\n",
    "                        )\n",
    "                        title = title_elem.text.strip()\n",
    "                        \n",
    "                        # Prix - VRAIES balises Amazon\n",
    "                        try:\n",
    "                            price_elem = container.find_element(\n",
    "                                By.CSS_SELECTOR,\n",
    "                                \".a-price-whole, .a-offscreen\"\n",
    "                            )\n",
    "                            price_text = price_elem.text.strip()\n",
    "                            price = float(re.findall(r'[\\d.]+', price_text.replace(',', ''))[0])\n",
    "                        except:\n",
    "                            price = 0.0\n",
    "                        \n",
    "                        # Rating - VRAIE balise Amazon\n",
    "                        try:\n",
    "                            rating_elem = container.find_element(\n",
    "                                By.CSS_SELECTOR,\n",
    "                                \".a-icon-alt\"\n",
    "                            )\n",
    "                            rating_text = rating_elem.get_attribute(\"textContent\")\n",
    "                            rating = float(re.findall(r'[\\d.]+', rating_text)[0])\n",
    "                        except:\n",
    "                            rating = 0.0\n",
    "                        \n",
    "                        # Nombre de reviews - VRAIE balise Amazon\n",
    "                        try:\n",
    "                            reviews_elem = container.find_element(\n",
    "                                By.CSS_SELECTOR,\n",
    "                                \".a-size-base\"\n",
    "                            )\n",
    "                            reviews_text = reviews_elem.text\n",
    "                            num_reviews = int(re.findall(r'[\\d,]+', reviews_text.replace(',', ''))[0])\n",
    "                        except:\n",
    "                            num_reviews = 0\n",
    "                        \n",
    "                        # URL du produit - VRAIE balise Amazon\n",
    "                        try:\n",
    "                            product_link = container.find_element(\n",
    "                                By.CSS_SELECTOR,\n",
    "                                \"h2 a\"\n",
    "                            ).get_attribute(\"href\")\n",
    "                        except:\n",
    "                            product_link = \"\"\n",
    "                        \n",
    "                        # Image - VRAIE balise Amazon\n",
    "                        try:\n",
    "                            img_elem = container.find_element(\n",
    "                                By.CSS_SELECTOR,\n",
    "                                \".s-image\"\n",
    "                            )\n",
    "                            image_url = img_elem.get_attribute(\"src\")\n",
    "                        except:\n",
    "                            image_url = \"\"\n",
    "                        \n",
    "                        products_data.append({\n",
    "                            'product_id': f\"amazon_{search_term}_{page}_{i}\",\n",
    "                            'title': title,\n",
    "                            'price': price,\n",
    "                            'rating': rating,\n",
    "                            'num_reviews': num_reviews,\n",
    "                            'product_url': product_link,\n",
    "                            'image_url': image_url,\n",
    "                            'search_term': search_term,\n",
    "                            'page': page,\n",
    "                            'source': 'amazon.com',\n",
    "                            'scraped_at': datetime.now().isoformat(),\n",
    "                            'user_agent': self.driver.execute_script(\"return navigator.userAgent;\")[:50]\n",
    "                        })\n",
    "                        \n",
    "                        # Comportement humain\n",
    "                        if i % 5 == 0:\n",
    "                            self._simulate_human_behavior()\n",
    "                        \n",
    "                        self._human_like_delay(0.5, 1.5)\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"Erreur extraction produit {i}: {e}\")\n",
    "                        continue\n",
    "                \n",
    "                # D√©lai entre pages\n",
    "                self._human_like_delay(3, 6)\n",
    "            \n",
    "            logger.info(f\"‚úÖ {len(products_data)} vrais produits Amazon extraits\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Erreur scraping Amazon: {e}\")\n",
    "        \n",
    "        return pd.DataFrame(products_data)\n",
    "    \n",
    "    def scrape_real_amazon_reviews(self, product_url: str, max_reviews: int = 50) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Scrape de vraies reviews Amazon avec vraies balises.\n",
    "        \"\"\"\n",
    "        logger.info(f\"üìù VRAIES reviews Amazon pour: {product_url}\")\n",
    "        \n",
    "        if not self.driver:\n",
    "            self.start_session()\n",
    "        \n",
    "        reviews_data = []\n",
    "        \n",
    "        try:\n",
    "            # Aller sur la page du produit\n",
    "            if not self.get_page(product_url):\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            # Cliquer sur \"Voir toutes les reviews\" - VRAIE balise Amazon\n",
    "            try:\n",
    "                reviews_link = self.driver.find_element(\n",
    "                    By.CSS_SELECTOR,\n",
    "                    \"[data-hook='see-all-reviews-link-foot'], .a-link-emphasis\"\n",
    "                )\n",
    "                reviews_link.click()\n",
    "                time.sleep(3)\n",
    "            except:\n",
    "                logger.warning(\"Impossible de trouver le lien vers les reviews\")\n",
    "            \n",
    "            page = 1\n",
    "            while len(reviews_data) < max_reviews and page <= 5:  # Max 5 pages\n",
    "                \n",
    "                # VRAIES balises CSS Amazon pour les reviews\n",
    "                review_containers = self.driver.find_elements(\n",
    "                    By.CSS_SELECTOR,\n",
    "                    \"[data-hook='review']\"\n",
    "                )\n",
    "                \n",
    "                for container in review_containers:\n",
    "                    if len(reviews_data) >= max_reviews:\n",
    "                        break\n",
    "                        \n",
    "                    try:\n",
    "                        # Texte de la review - VRAIE balise Amazon\n",
    "                        review_text_elem = container.find_element(\n",
    "                            By.CSS_SELECTOR,\n",
    "                            \"[data-hook='review-body'] span\"\n",
    "                        )\n",
    "                        review_text = review_text_elem.text.strip()\n",
    "                        \n",
    "                        # Rating - VRAIE balise Amazon\n",
    "                        rating_elem = container.find_element(\n",
    "                            By.CSS_SELECTOR,\n",
    "                            \".a-icon-alt\"\n",
    "                        )\n",
    "                        rating_text = rating_elem.get_attribute(\"textContent\")\n",
    "                        rating = float(re.findall(r'[\\d.]+', rating_text)[0])\n",
    "                        \n",
    "                        # Nom du reviewer - VRAIE balise Amazon\n",
    "                        try:\n",
    "                            reviewer_elem = container.find_element(\n",
    "                                By.CSS_SELECTOR,\n",
    "                                \".a-profile-name\"\n",
    "                            )\n",
    "                            reviewer_name = reviewer_elem.text.strip()\n",
    "                        except:\n",
    "                            reviewer_name = \"Anonymous\"\n",
    "                        \n",
    "                        # Date - VRAIE balise Amazon\n",
    "                        try:\n",
    "                            date_elem = container.find_element(\n",
    "                                By.CSS_SELECTOR,\n",
    "                                \"[data-hook='review-date']\"\n",
    "                            )\n",
    "                            date_text = date_elem.text\n",
    "                            # Extraire la date (format: \"Reviewed in the United States on January 1, 2024\")\n",
    "                            date_match = re.search(r'(\\w+ \\d+, \\d{4})', date_text)\n",
    "                            if date_match:\n",
    "                                review_date = datetime.strptime(date_match.group(1), '%B %d, %Y').strftime('%Y-%m-%d')\n",
    "                            else:\n",
    "                                review_date = datetime.now().strftime('%Y-%m-%d')\n",
    "                        except:\n",
    "                            review_date = datetime.now().strftime('%Y-%m-%d')\n",
    "                        \n",
    "                        # Votes utiles - VRAIE balise Amazon\n",
    "                        try:\n",
    "                            helpful_elem = container.find_element(\n",
    "                                By.CSS_SELECTOR,\n",
    "                                \"[data-hook='helpful-vote-statement']\"\n",
    "                            )\n",
    "                            helpful_text = helpful_elem.text\n",
    "                            helpful_votes = int(re.findall(r'\\d+', helpful_text)[0]) if re.findall(r'\\d+', helpful_text) else 0\n",
    "                        except:\n",
    "                            helpful_votes = 0\n",
    "                        \n",
    "                        # Achat v√©rifi√© - VRAIE balise Amazon\n",
    "                        try:\n",
    "                            verified_elem = container.find_element(\n",
    "                                By.CSS_SELECTOR,\n",
    "                                \"[data-hook='avp-badge']\"\n",
    "                            )\n",
    "                            verified_purchase = \"Verified Purchase\" in verified_elem.text\n",
    "                        except:\n",
    "                            verified_purchase = False\n",
    "                        \n",
    "                        reviews_data.append({\n",
    "                            'review_id': f\"amazon_review_{len(reviews_data)}\",\n",
    "                            'product_url': product_url,\n",
    "                            'reviewer_name': reviewer_name,\n",
    "                            'review_text': review_text,\n",
    "                            'rating': rating,\n",
    "                            'review_date': review_date,\n",
    "                            'helpful_votes': helpful_votes,\n",
    "                            'verified_purchase': verified_purchase,\n",
    "                            'source': 'amazon.com',\n",
    "                            'scraped_at': datetime.now().isoformat()\n",
    "                        })\n",
    "                        \n",
    "                        self._human_like_delay(0.3, 1.0)\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"Erreur extraction review: {e}\")\n",
    "                        continue\n",
    "                \n",
    "                # Essayer d'aller √† la page suivante\n",
    "                try:\n",
    "                    next_button = self.driver.find_element(\n",
    "                        By.CSS_SELECTOR,\n",
    "                        \"li.a-last a\"\n",
    "                    )\n",
    "                    next_button.click()\n",
    "                    self._human_like_delay(3, 5)\n",
    "                    page += 1\n",
    "                except:\n",
    "                    break  # Plus de pages\n",
    "            \n",
    "            logger.info(f\"‚úÖ {len(reviews_data)} vraies reviews Amazon extraites\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Erreur scraping reviews Amazon: {e}\")\n",
    "        \n",
    "        return pd.DataFrame(reviews_data)\n",
    "\n",
    "print(\"üõí VRAI scraper Amazon avec vraies balises CSS cr√©√© !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "761e380a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚≠ê VRAIS scrapers eBay et Trustpilot avec vraies balises CSS cr√©√©s !\n"
     ]
    }
   ],
   "source": [
    "# üè™ VRAIE IMPL√âMENTATION EBAY - S√©lecteurs CSS r√©els\n",
    "class RealEbayScraper(StealthMarketplaceScraper):\n",
    "    \"\"\"Scraper pour le VRAI eBay avec vraies balises.\"\"\"\n",
    "    \n",
    "    def scrape_real_ebay_products(self, search_term: str, max_pages: int = 2) -> pd.DataFrame:\n",
    "        \"\"\"Scrape de vrais produits eBay.\"\"\"\n",
    "        logger.info(f\"üè™ VRAI scraping eBay pour: {search_term}\")\n",
    "        \n",
    "        if not self.driver:\n",
    "            self.start_session()\n",
    "        \n",
    "        products_data = []\n",
    "        \n",
    "        try:\n",
    "            for page in range(1, max_pages + 1):\n",
    "                # VRAIE URL eBay\n",
    "                url = f\"https://www.ebay.com/sch/i.html?_nkw={search_term}&_pgn={page}\"\n",
    "                \n",
    "                if not self.get_page(url):\n",
    "                    continue\n",
    "                \n",
    "                # VRAIES balises CSS eBay\n",
    "                items = self.driver.find_elements(By.CSS_SELECTOR, \".s-item\")\n",
    "                \n",
    "                for i, item in enumerate(items):\n",
    "                    try:\n",
    "                        # Titre - VRAIE balise eBay\n",
    "                        title_elem = item.find_element(By.CSS_SELECTOR, \".s-item__title\")\n",
    "                        title = title_elem.text.strip()\n",
    "                        \n",
    "                        # Prix - VRAIE balise eBay\n",
    "                        try:\n",
    "                            price_elem = item.find_element(By.CSS_SELECTOR, \".s-item__price\")\n",
    "                            price_text = price_elem.text.strip()\n",
    "                            price = float(re.findall(r'[\\d.]+', price_text.replace(',', ''))[0])\n",
    "                        except:\n",
    "                            price = 0.0\n",
    "                        \n",
    "                        # Condition - VRAIE balise eBay\n",
    "                        try:\n",
    "                            condition_elem = item.find_element(By.CSS_SELECTOR, \".SECONDARY_INFO\")\n",
    "                            condition = condition_elem.text.strip()\n",
    "                        except:\n",
    "                            condition = \"Unknown\"\n",
    "                        \n",
    "                        # Livraison - VRAIE balise eBay\n",
    "                        try:\n",
    "                            shipping_elem = item.find_element(By.CSS_SELECTOR, \".s-item__shipping\")\n",
    "                            shipping = shipping_elem.text.strip()\n",
    "                        except:\n",
    "                            shipping = \"\"\n",
    "                        \n",
    "                        # URL - VRAIE balise eBay\n",
    "                        try:\n",
    "                            url_elem = item.find_element(By.CSS_SELECTOR, \".s-item__link\")\n",
    "                            item_url = url_elem.get_attribute(\"href\")\n",
    "                        except:\n",
    "                            item_url = \"\"\n",
    "                        \n",
    "                        products_data.append({\n",
    "                            'product_id': f\"ebay_{search_term}_{page}_{i}\",\n",
    "                            'title': title,\n",
    "                            'price': price,\n",
    "                            'condition': condition,\n",
    "                            'shipping': shipping,\n",
    "                            'product_url': item_url,\n",
    "                            'search_term': search_term,\n",
    "                            'source': 'ebay.com',\n",
    "                            'scraped_at': datetime.now().isoformat()\n",
    "                        })\n",
    "                        \n",
    "                        self._human_like_delay(0.5, 1.5)\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        continue\n",
    "                \n",
    "                self._human_like_delay(3, 6)\n",
    "            \n",
    "            logger.info(f\"‚úÖ {len(products_data)} vrais produits eBay extraits\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Erreur scraping eBay: {e}\")\n",
    "        \n",
    "        return pd.DataFrame(products_data)\n",
    "\n",
    "\n",
    "# ‚≠ê VRAIE IMPL√âMENTATION TRUSTPILOT - S√©lecteurs CSS r√©els\n",
    "class RealTrustpilotScraper(StealthMarketplaceScraper):\n",
    "    \"\"\"Scraper pour le VRAI Trustpilot avec vraies balises.\"\"\"\n",
    "    \n",
    "    def scrape_real_trustpilot_reviews(self, company_name: str, max_reviews: int = 100) -> pd.DataFrame:\n",
    "        \"\"\"Scrape de vraies reviews Trustpilot.\"\"\"\n",
    "        logger.info(f\"‚≠ê VRAI scraping Trustpilot pour: {company_name}\")\n",
    "        \n",
    "        if not self.driver:\n",
    "            self.start_session()\n",
    "        \n",
    "        reviews_data = []\n",
    "        \n",
    "        try:\n",
    "            # VRAIE URL Trustpilot\n",
    "            url = f\"https://www.trustpilot.com/review/{company_name.lower().replace(' ', '-')}\"\n",
    "            \n",
    "            if not self.get_page(url):\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            # G√©rer les cookies si n√©cessaire\n",
    "            try:\n",
    "                cookie_button = self.driver.find_element(By.CSS_SELECTOR, \"#onetrust-accept-btn-handler\")\n",
    "                cookie_button.click()\n",
    "                time.sleep(2)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            page = 1\n",
    "            while len(reviews_data) < max_reviews and page <= 10:\n",
    "                \n",
    "                # VRAIES balises CSS Trustpilot (mises √† jour 2024/2025)\n",
    "                review_cards = self.driver.find_elements(\n",
    "                    By.CSS_SELECTOR, \n",
    "                    \"article[data-service-review-card-paper]\"\n",
    "                )\n",
    "                \n",
    "                for card in review_cards:\n",
    "                    if len(reviews_data) >= max_reviews:\n",
    "                        break\n",
    "                        \n",
    "                    try:\n",
    "                        # Texte de la review - VRAIE balise Trustpilot\n",
    "                        text_elem = card.find_element(\n",
    "                            By.CSS_SELECTOR, \n",
    "                            \"[data-service-review-text-typography='true']\"\n",
    "                        )\n",
    "                        review_text = text_elem.text.strip()\n",
    "                        \n",
    "                        # Rating - VRAIE balise Trustpilot\n",
    "                        rating_elem = card.find_element(\n",
    "                            By.CSS_SELECTOR,\n",
    "                            \"[data-service-review-rating]\"\n",
    "                        )\n",
    "                        # Compter les √©toiles pleines\n",
    "                        filled_stars = rating_elem.find_elements(\n",
    "                            By.CSS_SELECTOR,\n",
    "                            \"svg[data-star-fill='true']\"\n",
    "                        )\n",
    "                        rating = len(filled_stars)\n",
    "                        \n",
    "                        # Nom du reviewer - VRAIE balise Trustpilot\n",
    "                        try:\n",
    "                            name_elem = card.find_element(\n",
    "                                By.CSS_SELECTOR,\n",
    "                                \"[data-consumer-name-typography='true']\"\n",
    "                            )\n",
    "                            reviewer_name = name_elem.text.strip()\n",
    "                        except:\n",
    "                            reviewer_name = \"Anonymous\"\n",
    "                        \n",
    "                        # Date - VRAIE balise Trustpilot\n",
    "                        try:\n",
    "                            date_elem = card.find_element(By.CSS_SELECTOR, \"time\")\n",
    "                            review_date = date_elem.get_attribute(\"datetime\")[:10]\n",
    "                        except:\n",
    "                            review_date = datetime.now().strftime('%Y-%m-%d')\n",
    "                        \n",
    "                        # Titre de la review - VRAIE balise Trustpilot\n",
    "                        try:\n",
    "                            title_elem = card.find_element(\n",
    "                                By.CSS_SELECTOR,\n",
    "                                \"[data-service-review-title-typography='true']\"\n",
    "                            )\n",
    "                            review_title = title_elem.text.strip()\n",
    "                        except:\n",
    "                            review_title = \"\"\n",
    "                        \n",
    "                        # Pays du reviewer - VRAIE balise Trustpilot\n",
    "                        try:\n",
    "                            country_elem = card.find_element(\n",
    "                                By.CSS_SELECTOR,\n",
    "                                \"[data-consumer-country-typography='true']\"\n",
    "                            )\n",
    "                            country = country_elem.text.strip()\n",
    "                        except:\n",
    "                            country = \"\"\n",
    "                        \n",
    "                        reviews_data.append({\n",
    "                            'review_id': f\"trustpilot_{company_name}_{len(reviews_data)}\",\n",
    "                            'company': company_name,\n",
    "                            'reviewer_name': reviewer_name,\n",
    "                            'review_title': review_title,\n",
    "                            'review_text': review_text,\n",
    "                            'rating': rating,\n",
    "                            'review_date': review_date,\n",
    "                            'country': country,\n",
    "                            'source': 'trustpilot.com',\n",
    "                            'scraped_at': datetime.now().isoformat()\n",
    "                        })\n",
    "                        \n",
    "                        self._human_like_delay(0.3, 1.0)\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"Erreur extraction review Trustpilot: {e}\")\n",
    "                        continue\n",
    "                \n",
    "                # Essayer d'aller √† la page suivante\n",
    "                try:\n",
    "                    next_button = self.driver.find_element(\n",
    "                        By.CSS_SELECTOR,\n",
    "                        \"a[name='pagination-button-next']\"\n",
    "                    )\n",
    "                    next_button.click()\n",
    "                    self._human_like_delay(3, 5)\n",
    "                    page += 1\n",
    "                except:\n",
    "                    break\n",
    "            \n",
    "            logger.info(f\"‚úÖ {len(reviews_data)} vraies reviews Trustpilot extraites\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Erreur scraping Trustpilot: {e}\")\n",
    "        \n",
    "        return pd.DataFrame(reviews_data)\n",
    "\n",
    "print(\"‚≠ê VRAIS scrapers eBay et Trustpilot avec vraies balises CSS cr√©√©s !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5f6f939d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Vrais scrapers configur√©s !\n",
      "‚ö†Ô∏è  UTILISEZ AVEC PR√âCAUTION et RESPECT des ToS\n",
      "üöÄ Pour tester: test_real_scrapers()\n"
     ]
    }
   ],
   "source": [
    "# üö® TEST DES VRAIS SCRAPERS - Utilisation avec pr√©caution\n",
    "def test_real_scrapers():\n",
    "    \"\"\"\n",
    "    Test des vrais scrapers sur de vrais sites.\n",
    "    ‚ö†Ô∏è ATTENTION: √Ä utiliser avec mod√©ration et respect des ToS\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"üö® AVERTISSEMENT: Vous allez scraper de VRAIS sites !\")\n",
    "    print(\"üìã Assurez-vous de:\")\n",
    "    print(\"   ‚úÖ Respecter les robots.txt\")\n",
    "    print(\"   ‚úÖ Limiter la fr√©quence des requ√™tes\")\n",
    "    print(\"   ‚úÖ Utiliser des proxies si n√©cessaire\")\n",
    "    print(\"   ‚úÖ Ne pas surcharger les serveurs\")\n",
    "    \n",
    "    choice = input(\"Continuer ? (oui/non): \").lower()\n",
    "    if choice not in ['oui', 'yes', 'y', 'o']:\n",
    "        print(\"‚ùå Test annul√©\")\n",
    "        return\n",
    "    \n",
    "    all_results = {}\n",
    "    \n",
    "    try:\n",
    "        # Test Amazon (COMMENT√â par s√©curit√©)\n",
    "        print(\"\\nüõí Test Amazon scraper...\")\n",
    "        amazon_scraper = RealAmazonScraper(headless=False, use_proxy=True)\n",
    "        amazon_scraper.start_session()\n",
    "        amazon_products = amazon_scraper.scrape_real_amazon_products(\"laptop\", max_pages=1)\n",
    "        all_results['amazon_products'] = amazon_products\n",
    "        amazon_scraper.close_session()\n",
    "        print(\"‚ö†Ô∏è Amazon scraper comment√© pour s√©curit√© - d√©commentez si n√©cessaire\")\n",
    "        \n",
    "        # Test eBay\n",
    "        print(\"\\nüè™ Test eBay scraper...\")\n",
    "        ebay_scraper = RealEbayScraper(headless=False, use_proxy=False)\n",
    "        ebay_scraper.start_session()\n",
    "        ebay_products = ebay_scraper.scrape_real_ebay_products(\"smartphone\", max_pages=1)\n",
    "        all_results['ebay_products'] = ebay_products\n",
    "        ebay_scraper.close_session()\n",
    "        \n",
    "        # Test Trustpilot\n",
    "        print(\"\\n‚≠ê Test Trustpilot scraper...\")\n",
    "        trustpilot_scraper = RealTrustpilotScraper(headless=False, use_proxy=False)\n",
    "        trustpilot_scraper.start_session()\n",
    "        trustpilot_reviews = trustpilot_scraper.scrape_real_trustpilot_reviews(\"amazon\", max_reviews=10)\n",
    "        all_results['trustpilot_reviews'] = trustpilot_reviews\n",
    "        trustpilot_scraper.close_session()\n",
    "        \n",
    "        # Sauvegarder les r√©sultats\n",
    "        for data_type, df in all_results.items():\n",
    "            if not df.empty:\n",
    "                save_scraped_data(df, f\"real_{data_type}.csv\")\n",
    "                analyze_scraped_data(df)\n",
    "        \n",
    "        print(\"\\n‚úÖ Tests des vrais scrapers termin√©s !\")\n",
    "        return all_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur durant les tests r√©els: {e}\")\n",
    "        return None\n",
    "\n",
    "# Configuration pour scrapers r√©els\n",
    "REAL_SCRAPER_CONFIG = {\n",
    "    'use_proxy': True,           # RECOMMAND√â pour vrais sites\n",
    "    'headless': True,            # Mode invisible\n",
    "    'delay_min': 2,              # D√©lais plus longs\n",
    "    'delay_max': 5,              # Pour √©viter la d√©tection\n",
    "    'max_retries': 3,            # Retry en cas d'√©chec\n",
    "    'respect_robots_txt': True   # Respecter robots.txt\n",
    "}\n",
    "\n",
    "print(\"üéØ Vrais scrapers configur√©s !\")\n",
    "print(\"‚ö†Ô∏è  UTILISEZ AVEC PR√âCAUTION et RESPECT des ToS\")\n",
    "print(\"üöÄ Pour tester: test_real_scrapers()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c6315339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üö® AVERTISSEMENT: Vous allez scraper de VRAIS sites !\n",
      "üìã Assurez-vous de:\n",
      "   ‚úÖ Respecter les robots.txt\n",
      "   ‚úÖ Limiter la fr√©quence des requ√™tes\n",
      "   ‚úÖ Utiliser des proxies si n√©cessaire\n",
      "   ‚úÖ Ne pas surcharger les serveurs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 16:12:37,871 - INFO - üîß Initialisation du scraper anti-d√©tection...\n",
      "2025-06-27 16:12:37,871 - INFO - üé≠ User Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) Ap...\n",
      "2025-06-27 16:12:37,871 - INFO - üé≠ User Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) Ap...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üõí Test Amazon scraper...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 16:12:39,553 - INFO - patching driver executable C:\\Users\\Yann\\appdata\\roaming\\undetected_chromedriver\\undetected_chromedriver.exe\n",
      "2025-06-27 16:12:40,842 - ERROR - ‚ùå Erreur lors de la configuration du driver: Message: invalid argument: cannot parse capability: goog:chromeOptions\n",
      "from invalid argument: unrecognized chrome option: excludeSwitches\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x0x344493+62419]\n",
      "\tGetHandleVerifier [0x0x3444d4+62484]\n",
      "\t(No symbol) [0x0x182133]\n",
      "\t(No symbol) [0x0x1a9723]\n",
      "\t(No symbol) [0x0x1aaeb0]\n",
      "\t(No symbol) [0x0x1a5fea]\n",
      "\t(No symbol) [0x0x1f9832]\n",
      "\t(No symbol) [0x0x1f931c]\n",
      "\t(No symbol) [0x0x1faa20]\n",
      "\t(No symbol) [0x0x1fa82a]\n",
      "\t(No symbol) [0x0x1ef266]\n",
      "\t(No symbol) [0x0x1be852]\n",
      "\t(No symbol) [0x0x1bf6f4]\n",
      "\tGetHandleVerifier [0x0x5b4773+2619059]\n",
      "\tGetHandleVerifier [0x0x5afb8a+2599626]\n",
      "\tGetHandleVerifier [0x0x36b03a+221050]\n",
      "\tGetHandleVerifier [0x0x35b2b8+156152]\n",
      "\tGetHandleVerifier [0x0x361c6d+183213]\n",
      "\tGetHandleVerifier [0x0x34c378+94904]\n",
      "\tGetHandleVerifier [0x0x34c502+95298]\n",
      "\tGetHandleVerifier [0x0x33765a+9626]\n",
      "\tBaseThreadInitThunk [0x0x76775d49+25]\n",
      "\tRtlInitializeExceptionChain [0x0x778ed09b+107]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x0x778ed021+561]\n",
      "\t(No symbol) [0x0]\n",
      "\n",
      "2025-06-27 16:12:40,842 - ERROR - ‚ùå Erreur lors de la configuration du driver: Message: invalid argument: cannot parse capability: goog:chromeOptions\n",
      "from invalid argument: unrecognized chrome option: excludeSwitches\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x0x344493+62419]\n",
      "\tGetHandleVerifier [0x0x3444d4+62484]\n",
      "\t(No symbol) [0x0x182133]\n",
      "\t(No symbol) [0x0x1a9723]\n",
      "\t(No symbol) [0x0x1aaeb0]\n",
      "\t(No symbol) [0x0x1a5fea]\n",
      "\t(No symbol) [0x0x1f9832]\n",
      "\t(No symbol) [0x0x1f931c]\n",
      "\t(No symbol) [0x0x1faa20]\n",
      "\t(No symbol) [0x0x1fa82a]\n",
      "\t(No symbol) [0x0x1ef266]\n",
      "\t(No symbol) [0x0x1be852]\n",
      "\t(No symbol) [0x0x1bf6f4]\n",
      "\tGetHandleVerifier [0x0x5b4773+2619059]\n",
      "\tGetHandleVerifier [0x0x5afb8a+2599626]\n",
      "\tGetHandleVerifier [0x0x36b03a+221050]\n",
      "\tGetHandleVerifier [0x0x35b2b8+156152]\n",
      "\tGetHandleVerifier [0x0x361c6d+183213]\n",
      "\tGetHandleVerifier [0x0x34c378+94904]\n",
      "\tGetHandleVerifier [0x0x34c502+95298]\n",
      "\tGetHandleVerifier [0x0x33765a+9626]\n",
      "\tBaseThreadInitThunk [0x0x76775d49+25]\n",
      "\tRtlInitializeExceptionChain [0x0x778ed09b+107]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x0x778ed021+561]\n",
      "\t(No symbol) [0x0]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Erreur durant les tests r√©els: Message: invalid argument: cannot parse capability: goog:chromeOptions\n",
      "from invalid argument: unrecognized chrome option: excludeSwitches\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x0x344493+62419]\n",
      "\tGetHandleVerifier [0x0x3444d4+62484]\n",
      "\t(No symbol) [0x0x182133]\n",
      "\t(No symbol) [0x0x1a9723]\n",
      "\t(No symbol) [0x0x1aaeb0]\n",
      "\t(No symbol) [0x0x1a5fea]\n",
      "\t(No symbol) [0x0x1f9832]\n",
      "\t(No symbol) [0x0x1f931c]\n",
      "\t(No symbol) [0x0x1faa20]\n",
      "\t(No symbol) [0x0x1fa82a]\n",
      "\t(No symbol) [0x0x1ef266]\n",
      "\t(No symbol) [0x0x1be852]\n",
      "\t(No symbol) [0x0x1bf6f4]\n",
      "\tGetHandleVerifier [0x0x5b4773+2619059]\n",
      "\tGetHandleVerifier [0x0x5afb8a+2599626]\n",
      "\tGetHandleVerifier [0x0x36b03a+221050]\n",
      "\tGetHandleVerifier [0x0x35b2b8+156152]\n",
      "\tGetHandleVerifier [0x0x361c6d+183213]\n",
      "\tGetHandleVerifier [0x0x34c378+94904]\n",
      "\tGetHandleVerifier [0x0x34c502+95298]\n",
      "\tGetHandleVerifier [0x0x33765a+9626]\n",
      "\tBaseThreadInitThunk [0x0x76775d49+25]\n",
      "\tRtlInitializeExceptionChain [0x0x778ed09b+107]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x0x778ed021+561]\n",
      "\t(No symbol) [0x0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = test_real_scrapers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ff8626bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç SiteScout initialis√© - Pr√™t pour la reconnaissance !\n"
     ]
    }
   ],
   "source": [
    "# üîç PHASE 1: RECONNAISSANCE ET VALIDATION DES BALISES R√âELLES\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "from datetime import datetime\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "class SiteScout:\n",
    "    \"\"\"\n",
    "    Classe pour reconna√Ætre et valider les vraies balises des sites avant scraping.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.driver = None\n",
    "        self.discovered_selectors = {}\n",
    "        self.validated_selectors = {}\n",
    "        \n",
    "    def setup_scout_driver(self):\n",
    "        \"\"\"Configure un driver sp√©cial pour la reconnaissance\"\"\"\n",
    "        try:\n",
    "            from selenium import webdriver\n",
    "            from selenium.webdriver.chrome.service import Service\n",
    "            from selenium.webdriver.chrome.options import Options\n",
    "            from webdriver_manager.chrome import ChromeDriverManager\n",
    "            \n",
    "            options = Options()\n",
    "            options.add_argument('--no-sandbox')\n",
    "            options.add_argument('--disable-dev-shm-usage')\n",
    "            options.add_argument('--disable-gpu')\n",
    "            options.add_argument('--window-size=1920,1080')\n",
    "            \n",
    "            service = Service(ChromeDriverManager().install())\n",
    "            self.driver = webdriver.Chrome(service=service, options=options)\n",
    "            \n",
    "            logger.info(\"‚úÖ Scout driver configur√©\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Erreur scout driver: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def scout_amazon_selectors(self):\n",
    "        \"\"\"D√©couvre les vraies balises Amazon actuelles\"\"\"\n",
    "        logger.info(\"üîç Reconnaissance Amazon...\")\n",
    "        \n",
    "        if not self.driver:\n",
    "            if not self.setup_scout_driver():\n",
    "                return {}\n",
    "        \n",
    "        try:\n",
    "            # Test avec une recherche simple\n",
    "            self.driver.get(\"https://www.amazon.com/s?k=laptop\")\n",
    "            time.sleep(3)\n",
    "            \n",
    "            # Accepter les cookies si n√©cessaire\n",
    "            try:\n",
    "                cookie_btn = self.driver.find_element(By.ID, \"sp-cc-accept\")\n",
    "                cookie_btn.click()\n",
    "                time.sleep(1)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            selectors_to_test = {\n",
    "                'product_container': [\n",
    "                    '[data-component-type=\"s-search-result\"]',\n",
    "                    '.s-result-item',\n",
    "                    '[data-asin]',\n",
    "                    '.sg-col-inner'\n",
    "                ],\n",
    "                'title': [\n",
    "                    'h2 a span',\n",
    "                    '.a-size-medium span',\n",
    "                    '.a-size-base-plus',\n",
    "                    '[data-cy=\"title-recipe-price\"]'\n",
    "                ],\n",
    "                'price': [\n",
    "                    '.a-price-whole',\n",
    "                    '.a-price .a-offscreen',\n",
    "                    '.a-price-range',\n",
    "                    '.a-price-symbol'\n",
    "                ],\n",
    "                'rating': [\n",
    "                    '.a-icon-alt',\n",
    "                    '[data-hook=\"rating-out-of-text\"]',\n",
    "                    '.a-declarative .a-icon-alt'\n",
    "                ],\n",
    "                'image': [\n",
    "                    '.s-image',\n",
    "                    '.a-dynamic-image',\n",
    "                    'img[data-image-latency]'\n",
    "                ]\n",
    "            }\n",
    "            \n",
    "            amazon_selectors = {}\n",
    "            \n",
    "            for element_type, selectors in selectors_to_test.items():\n",
    "                for selector in selectors:\n",
    "                    try:\n",
    "                        elements = self.driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                        if len(elements) > 0:\n",
    "                            # Prendre un √©chantillon de texte pour validation\n",
    "                            sample_text = \"\"\n",
    "                            try:\n",
    "                                if element_type == 'image':\n",
    "                                    sample_text = elements[0].get_attribute('src')[:50]\n",
    "                                else:\n",
    "                                    sample_text = elements[0].text[:50]\n",
    "                            except:\n",
    "                                sample_text = \"Element found\"\n",
    "                            \n",
    "                            amazon_selectors[element_type] = {\n",
    "                                'selector': selector,\n",
    "                                'count': len(elements),\n",
    "                                'sample': sample_text,\n",
    "                                'validated': True\n",
    "                            }\n",
    "                            logger.info(f\"‚úÖ {element_type}: {selector} ({len(elements)} √©l√©ments)\")\n",
    "                            break\n",
    "                    except Exception as e:\n",
    "                        continue\n",
    "                \n",
    "                if element_type not in amazon_selectors:\n",
    "                    amazon_selectors[element_type] = {\n",
    "                        'selector': None,\n",
    "                        'count': 0,\n",
    "                        'sample': None,\n",
    "                        'validated': False\n",
    "                    }\n",
    "                    logger.warning(f\"‚ö†Ô∏è {element_type}: Aucun s√©lecteur trouv√©\")\n",
    "            \n",
    "            self.discovered_selectors['amazon'] = amazon_selectors\n",
    "            return amazon_selectors\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Erreur reconnaissance Amazon: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def scout_ebay_selectors(self):\n",
    "        \"\"\"D√©couvre les vraies balises eBay actuelles\"\"\"\n",
    "        logger.info(\"üîç Reconnaissance eBay...\")\n",
    "        \n",
    "        try:\n",
    "            self.driver.get(\"https://www.ebay.com/sch/i.html?_nkw=smartphone\")\n",
    "            time.sleep(3)\n",
    "            \n",
    "            selectors_to_test = {\n",
    "                'product_container': [\n",
    "                    '.s-item',\n",
    "                    '.srp-results .s-item',\n",
    "                    '[data-view=\"mi:1686|iid:1\"]'\n",
    "                ],\n",
    "                'title': [\n",
    "                    '.s-item__title',\n",
    "                    '.it-ttl',\n",
    "                    '.s-item__link'\n",
    "                ],\n",
    "                'price': [\n",
    "                    '.s-item__price',\n",
    "                    '.notranslate',\n",
    "                    '.u-flL'\n",
    "                ],\n",
    "                'condition': [\n",
    "                    '.SECONDARY_INFO',\n",
    "                    '.s-item__subtitle',\n",
    "                    '.clipped'\n",
    "                ],\n",
    "                'shipping': [\n",
    "                    '.s-item__shipping',\n",
    "                    '.vi-s-ship-range',\n",
    "                    '.s-item__logisticsCost'\n",
    "                ]\n",
    "            }\n",
    "            \n",
    "            ebay_selectors = {}\n",
    "            \n",
    "            for element_type, selectors in selectors_to_test.items():\n",
    "                for selector in selectors:\n",
    "                    try:\n",
    "                        elements = self.driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                        if len(elements) > 0:\n",
    "                            sample_text = \"\"\n",
    "                            try:\n",
    "                                sample_text = elements[0].text[:50]\n",
    "                            except:\n",
    "                                sample_text = \"Element found\"\n",
    "                            \n",
    "                            ebay_selectors[element_type] = {\n",
    "                                'selector': selector,\n",
    "                                'count': len(elements),\n",
    "                                'sample': sample_text,\n",
    "                                'validated': True\n",
    "                            }\n",
    "                            logger.info(f\"‚úÖ {element_type}: {selector} ({len(elements)} √©l√©ments)\")\n",
    "                            break\n",
    "                    except:\n",
    "                        continue\n",
    "                \n",
    "                if element_type not in ebay_selectors:\n",
    "                    ebay_selectors[element_type] = {\n",
    "                        'selector': None,\n",
    "                        'count': 0,\n",
    "                        'sample': None,\n",
    "                        'validated': False\n",
    "                    }\n",
    "            \n",
    "            self.discovered_selectors['ebay'] = ebay_selectors\n",
    "            return ebay_selectors\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Erreur reconnaissance eBay: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def scout_trustpilot_selectors(self, company=\"amazon\"):\n",
    "        \"\"\"D√©couvre les vraies balises Trustpilot actuelles\"\"\"\n",
    "        logger.info(f\"üîç Reconnaissance Trustpilot pour {company}...\")\n",
    "        \n",
    "        try:\n",
    "            self.driver.get(f\"https://www.trustpilot.com/review/{company}\")\n",
    "            time.sleep(3)\n",
    "            \n",
    "            # Accepter cookies\n",
    "            try:\n",
    "                cookie_btn = self.driver.find_element(By.ID, \"onetrust-accept-btn-handler\")\n",
    "                cookie_btn.click()\n",
    "                time.sleep(2)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            selectors_to_test = {\n",
    "                'review_container': [\n",
    "                    'article[data-service-review-card-paper]',\n",
    "                    '.review-card',\n",
    "                    '.styles_reviewCard__hcAvl'\n",
    "                ],\n",
    "                'review_text': [\n",
    "                    '[data-service-review-text-typography=\"true\"]',\n",
    "                    '.typography_body-l__KUYFJ',\n",
    "                    '.review-content__text'\n",
    "                ],\n",
    "                'rating': [\n",
    "                    '[data-service-review-rating]',\n",
    "                    '.star-rating',\n",
    "                    '.styles_reviewHeader__iU9Px img'\n",
    "                ],\n",
    "                'reviewer_name': [\n",
    "                    '[data-consumer-name-typography=\"true\"]',\n",
    "                    '.consumer-information__name',\n",
    "                    '.styles_consumerName__dxer2'\n",
    "                ],\n",
    "                'review_date': [\n",
    "                    'time[datetime]',\n",
    "                    '.typography_body-m__xgxZ_',\n",
    "                    '.styles_reviewDate__6_BBM'\n",
    "                ]\n",
    "            }\n",
    "            \n",
    "            trustpilot_selectors = {}\n",
    "            \n",
    "            for element_type, selectors in selectors_to_test.items():\n",
    "                for selector in selectors:\n",
    "                    try:\n",
    "                        elements = self.driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                        if len(elements) > 0:\n",
    "                            sample_text = \"\"\n",
    "                            try:\n",
    "                                if element_type == 'review_date':\n",
    "                                    sample_text = elements[0].get_attribute('datetime') or elements[0].text\n",
    "                                else:\n",
    "                                    sample_text = elements[0].text[:50]\n",
    "                            except:\n",
    "                                sample_text = \"Element found\"\n",
    "                            \n",
    "                            trustpilot_selectors[element_type] = {\n",
    "                                'selector': selector,\n",
    "                                'count': len(elements),\n",
    "                                'sample': sample_text,\n",
    "                                'validated': True\n",
    "                            }\n",
    "                            logger.info(f\"‚úÖ {element_type}: {selector} ({len(elements)} √©l√©ments)\")\n",
    "                            break\n",
    "                    except:\n",
    "                        continue\n",
    "                \n",
    "                if element_type not in trustpilot_selectors:\n",
    "                    trustpilot_selectors[element_type] = {\n",
    "                        'selector': None,\n",
    "                        'count': 0,\n",
    "                        'sample': None,\n",
    "                        'validated': False\n",
    "                    }\n",
    "            \n",
    "            self.discovered_selectors['trustpilot'] = trustpilot_selectors\n",
    "            return trustpilot_selectors\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Erreur reconnaissance Trustpilot: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def save_selectors_config(self, filename=\"../config/validated_selectors.json\"):\n",
    "        \"\"\"Sauvegarde les s√©lecteurs valid√©s\"\"\"\n",
    "        import os\n",
    "        os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "        \n",
    "        config = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'sites': self.discovered_selectors\n",
    "        }\n",
    "        \n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(config, f, indent=2)\n",
    "        \n",
    "        logger.info(f\"üíæ S√©lecteurs sauvegard√©s: {filename}\")\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Ferme le driver scout\"\"\"\n",
    "        if self.driver:\n",
    "            self.driver.quit()\n",
    "\n",
    "# Initialiser le scout\n",
    "scout = SiteScout()\n",
    "print(\"üîç SiteScout initialis√© - Pr√™t pour la reconnaissance !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4ffe089e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Pr√™t pour la reconnaissance !\n",
      "   run_full_site_reconnaissance() - Reconnaissance compl√®te\n",
      "   quick_selector_test('amazon', '.s-result-item', 'https://amazon.com/s?k=laptop') - Test rapide\n"
     ]
    }
   ],
   "source": [
    "# üß™ PHASE 1: EX√âCUTION DE LA RECONNAISSANCE\n",
    "def run_full_site_reconnaissance():\n",
    "    \"\"\"\n",
    "    Lance la reconnaissance compl√®te de tous les sites pour d√©couvrir les vraies balises.\n",
    "    \"\"\"\n",
    "    print(\"üéØ LANCEMENT DE LA RECONNAISSANCE COMPL√àTE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    scout = SiteScout()\n",
    "    all_results = {}\n",
    "    \n",
    "    try:\n",
    "        # 1. Amazon\n",
    "        print(\"\\nüõí Reconnaissance Amazon...\")\n",
    "        amazon_selectors = scout.scout_amazon_selectors()\n",
    "        all_results['amazon'] = amazon_selectors\n",
    "        \n",
    "        if amazon_selectors:\n",
    "            print(\"‚úÖ Amazon reconnaissance termin√©e\")\n",
    "            for element_type, data in amazon_selectors.items():\n",
    "                status = \"‚úÖ\" if data['validated'] else \"‚ùå\"\n",
    "                print(f\"   {status} {element_type}: {data.get('selector', 'Non trouv√©')}\")\n",
    "        \n",
    "        time.sleep(2)  # Pause entre sites\n",
    "        \n",
    "        # 2. eBay\n",
    "        print(\"\\nüè™ Reconnaissance eBay...\")\n",
    "        ebay_selectors = scout.scout_ebay_selectors()\n",
    "        all_results['ebay'] = ebay_selectors\n",
    "        \n",
    "        if ebay_selectors:\n",
    "            print(\"‚úÖ eBay reconnaissance termin√©e\")\n",
    "            for element_type, data in ebay_selectors.items():\n",
    "                status = \"‚úÖ\" if data['validated'] else \"‚ùå\"\n",
    "                print(f\"   {status} {element_type}: {data.get('selector', 'Non trouv√©')}\")\n",
    "        \n",
    "        time.sleep(2)\n",
    "        \n",
    "        # 3. Trustpilot\n",
    "        print(\"\\n‚≠ê Reconnaissance Trustpilot...\")\n",
    "        trustpilot_selectors = scout.scout_trustpilot_selectors(\"amazon\")\n",
    "        all_results['trustpilot'] = trustpilot_selectors\n",
    "        \n",
    "        if trustpilot_selectors:\n",
    "            print(\"‚úÖ Trustpilot reconnaissance termin√©e\")\n",
    "            for element_type, data in trustpilot_selectors.items():\n",
    "                status = \"‚úÖ\" if data['validated'] else \"‚ùå\"\n",
    "                print(f\"   {status} {element_type}: {data.get('selector', 'Non trouv√©')}\")\n",
    "        \n",
    "        # 4. Sauvegarde\n",
    "        print(\"\\nüíæ Sauvegarde des s√©lecteurs...\")\n",
    "        scout.save_selectors_config()\n",
    "        \n",
    "        # 5. R√©sum√©\n",
    "        print(\"\\nüìä R√âSUM√â DE LA RECONNAISSANCE:\")\n",
    "        total_selectors = 0\n",
    "        valid_selectors = 0\n",
    "        \n",
    "        for site, selectors in all_results.items():\n",
    "            site_total = len(selectors)\n",
    "            site_valid = sum(1 for s in selectors.values() if s.get('validated', False))\n",
    "            total_selectors += site_total\n",
    "            valid_selectors += site_valid\n",
    "            \n",
    "            print(f\"   {site.upper()}: {site_valid}/{site_total} s√©lecteurs valid√©s\")\n",
    "        \n",
    "        print(f\"\\nüéØ TOTAL: {valid_selectors}/{total_selectors} s√©lecteurs fonctionnels\")\n",
    "        \n",
    "        return all_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Erreur reconnaissance: {e}\")\n",
    "        return {}\n",
    "        \n",
    "    finally:\n",
    "        scout.close()\n",
    "\n",
    "def quick_selector_test(site_name, selector, url):\n",
    "    \"\"\"Test rapide d'un s√©lecteur sp√©cifique\"\"\"\n",
    "    print(f\"üß™ Test rapide: {site_name} - {selector}\")\n",
    "    \n",
    "    scout = SiteScout()\n",
    "    if not scout.setup_scout_driver():\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        scout.driver.get(url)\n",
    "        time.sleep(3)\n",
    "        \n",
    "        elements = scout.driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "        \n",
    "        if len(elements) > 0:\n",
    "            print(f\"‚úÖ Trouv√© {len(elements)} √©l√©ments\")\n",
    "            print(f\"   Exemple: {elements[0].text[:100]}...\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"‚ùå Aucun √©l√©ment trouv√©\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur: {e}\")\n",
    "        return False\n",
    "    finally:\n",
    "        scout.close()\n",
    "\n",
    "# LANCEMENT DE LA RECONNAISSANCE\n",
    "print(\"üöÄ Pr√™t pour la reconnaissance !\")\n",
    "print(\"   run_full_site_reconnaissance() - Reconnaissance compl√®te\")\n",
    "print(\"   quick_selector_test('amazon', '.s-result-item', 'https://amazon.com/s?k=laptop') - Test rapide\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bce8b44e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ ValidatedScraper pr√™t !\n"
     ]
    }
   ],
   "source": [
    "# üéØ PHASE 2: SCRAPER AVEC BALISES VALID√âES\n",
    "class ValidatedScraper:\n",
    "    \"\"\"\n",
    "    Scraper qui utilise les balises valid√©es de la Phase 1\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, selectors_file=\"../config/validated_selectors.json\"):\n",
    "        self.driver = None\n",
    "        self.selectors = {}\n",
    "        self.load_validated_selectors(selectors_file)\n",
    "        \n",
    "    def load_validated_selectors(self, filename):\n",
    "        \"\"\"Charge les s√©lecteurs valid√©s depuis le fichier JSON\"\"\"\n",
    "        try:\n",
    "            with open(filename, 'r') as f:\n",
    "                config = json.load(f)\n",
    "                self.selectors = config.get('sites', {})\n",
    "                logger.info(f\"‚úÖ S√©lecteurs charg√©s depuis {filename}\")\n",
    "                \n",
    "                # Afficher les s√©lecteurs charg√©s\n",
    "                for site, site_selectors in self.selectors.items():\n",
    "                    valid_count = sum(1 for s in site_selectors.values() if s.get('validated'))\n",
    "                    print(f\"   {site.upper()}: {valid_count} s√©lecteurs valid√©s\")\n",
    "                    \n",
    "        except FileNotFoundError:\n",
    "            logger.warning(\"‚ö†Ô∏è Fichier de s√©lecteurs non trouv√© - Lancez d'abord la reconnaissance\")\n",
    "            self.selectors = {}\n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Erreur chargement s√©lecteurs: {e}\")\n",
    "            self.selectors = {}\n",
    "    \n",
    "    def setup_production_driver(self, headless=True, use_stealth=True):\n",
    "        \"\"\"Configure un driver optimis√© pour la production\"\"\"\n",
    "        try:\n",
    "            if use_stealth:\n",
    "                import undetected_chromedriver as uc\n",
    "                options = uc.ChromeOptions()\n",
    "                \n",
    "                # Options de base\n",
    "                if headless:\n",
    "                    options.add_argument('--headless=new')\n",
    "                options.add_argument('--no-sandbox')\n",
    "                options.add_argument('--disable-dev-shm-usage')\n",
    "                options.add_argument('--disable-gpu')\n",
    "                options.add_argument('--window-size=1920,1080')\n",
    "                \n",
    "                # User agent al√©atoire\n",
    "                user_agent = random.choice(REALISTIC_USER_AGENTS)\n",
    "                options.add_argument(f'--user-agent={user_agent}')\n",
    "                \n",
    "                # Cr√©er driver stealth\n",
    "                self.driver = uc.Chrome(options=options)\n",
    "                \n",
    "                # Scripts anti-d√©tection\n",
    "                self.driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
    "                \n",
    "            else:\n",
    "                # Driver classique si stealth √©choue\n",
    "                from selenium import webdriver\n",
    "                from selenium.webdriver.chrome.service import Service\n",
    "                from webdriver_manager.chrome import ChromeDriverManager\n",
    "                \n",
    "                options = webdriver.ChromeOptions()\n",
    "                if headless:\n",
    "                    options.add_argument('--headless')\n",
    "                options.add_argument('--no-sandbox')\n",
    "                options.add_argument('--disable-dev-shm-usage')\n",
    "                \n",
    "                service = Service(ChromeDriverManager().install())\n",
    "                self.driver = webdriver.Chrome(service=service, options=options)\n",
    "            \n",
    "            logger.info(\"‚úÖ Driver production configur√©\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Erreur driver production: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def human_like_behavior(self):\n",
    "        \"\"\"Simule un comportement humain\"\"\"\n",
    "        # D√©lai al√©atoire\n",
    "        time.sleep(random.uniform(1, 3))\n",
    "        \n",
    "        # Scroll al√©atoire parfois\n",
    "        if random.random() < 0.3:\n",
    "            scroll_amount = random.randint(200, 800)\n",
    "            self.driver.execute_script(f\"window.scrollBy(0, {scroll_amount});\")\n",
    "            time.sleep(random.uniform(0.5, 1.5))\n",
    "    \n",
    "    def scrape_amazon_products_validated(self, search_term, max_products=20):\n",
    "        \"\"\"Scrape Amazon avec s√©lecteurs valid√©s\"\"\"\n",
    "        logger.info(f\"üõí Scraping Amazon valid√©: {search_term}\")\n",
    "        \n",
    "        if 'amazon' not in self.selectors:\n",
    "            logger.error(\"‚ùå S√©lecteurs Amazon non disponibles - Lancez la reconnaissance\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        amazon_selectors = self.selectors['amazon']\n",
    "        products_data = []\n",
    "        \n",
    "        try:\n",
    "            # Navigation\n",
    "            url = f\"https://www.amazon.com/s?k={search_term}\"\n",
    "            self.driver.get(url)\n",
    "            self.human_like_behavior()\n",
    "            \n",
    "            # Accepter cookies\n",
    "            try:\n",
    "                cookie_btn = WebDriverWait(self.driver, 5).until(\n",
    "                    EC.element_to_be_clickable((By.ID, \"sp-cc-accept\"))\n",
    "                )\n",
    "                cookie_btn.click()\n",
    "                time.sleep(1)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # Trouver les produits avec s√©lecteur valid√©\n",
    "            container_selector = amazon_selectors.get('product_container', {}).get('selector')\n",
    "            if not container_selector:\n",
    "                logger.error(\"‚ùå S√©lecteur conteneur produit non valid√©\")\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            products = WebDriverWait(self.driver, 10).until(\n",
    "                EC.presence_of_all_elements_located((By.CSS_SELECTOR, container_selector))\n",
    "            )\n",
    "            \n",
    "            logger.info(f\"üì¶ Trouv√© {len(products)} produits\")\n",
    "            \n",
    "            for i, product in enumerate(products[:max_products]):\n",
    "                try:\n",
    "                    product_data = {\n",
    "                        'product_id': f\"amazon_{search_term}_{i}\",\n",
    "                        'search_term': search_term,\n",
    "                        'source': 'amazon.com',\n",
    "                        'scraped_at': datetime.now().isoformat()\n",
    "                    }\n",
    "                    \n",
    "                    # Extraire titre avec s√©lecteur valid√©\n",
    "                    title_selector = amazon_selectors.get('title', {}).get('selector')\n",
    "                    if title_selector:\n",
    "                        try:\n",
    "                            title_elem = product.find_element(By.CSS_SELECTOR, title_selector)\n",
    "                            product_data['title'] = title_elem.text.strip()\n",
    "                        except:\n",
    "                            product_data['title'] = \"Titre non trouv√©\"\n",
    "                    \n",
    "                    # Extraire prix avec s√©lecteur valid√©\n",
    "                    price_selector = amazon_selectors.get('price', {}).get('selector')\n",
    "                    if price_selector:\n",
    "                        try:\n",
    "                            price_elem = product.find_element(By.CSS_SELECTOR, price_selector)\n",
    "                            price_text = price_elem.text.strip()\n",
    "                            price_numbers = re.findall(r'[\\d.]+', price_text.replace(',', ''))\n",
    "                            product_data['price'] = float(price_numbers[0]) if price_numbers else 0.0\n",
    "                        except:\n",
    "                            product_data['price'] = 0.0\n",
    "                    \n",
    "                    # Extraire rating avec s√©lecteur valid√©\n",
    "                    rating_selector = amazon_selectors.get('rating', {}).get('selector')\n",
    "                    if rating_selector:\n",
    "                        try:\n",
    "                            rating_elem = product.find_element(By.CSS_SELECTOR, rating_selector)\n",
    "                            rating_text = rating_elem.get_attribute('textContent') or rating_elem.text\n",
    "                            rating_numbers = re.findall(r'[\\d.]+', rating_text)\n",
    "                            product_data['rating'] = float(rating_numbers[0]) if rating_numbers else 0.0\n",
    "                        except:\n",
    "                            product_data['rating'] = 0.0\n",
    "                    \n",
    "                    # Extraire URL produit\n",
    "                    try:\n",
    "                        link_elem = product.find_element(By.CSS_SELECTOR, 'h2 a')\n",
    "                        product_data['product_url'] = link_elem.get_attribute('href')\n",
    "                    except:\n",
    "                        product_data['product_url'] = \"\"\n",
    "                    \n",
    "                    products_data.append(product_data)\n",
    "                    \n",
    "                    # Comportement humain\n",
    "                    if i % 5 == 0:\n",
    "                        self.human_like_behavior()\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Erreur produit {i}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            logger.info(f\"‚úÖ {len(products_data)} produits Amazon scrap√©s avec succ√®s\")\n",
    "            return pd.DataFrame(products_data)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Erreur scraping Amazon: {e}\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def scrape_trustpilot_reviews_validated(self, company, max_reviews=50):\n",
    "        \"\"\"Scrape Trustpilot avec s√©lecteurs valid√©s\"\"\"\n",
    "        logger.info(f\"‚≠ê Scraping Trustpilot valid√©: {company}\")\n",
    "        \n",
    "        if 'trustpilot' not in self.selectors:\n",
    "            logger.error(\"‚ùå S√©lecteurs Trustpilot non disponibles\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        trustpilot_selectors = self.selectors['trustpilot']\n",
    "        reviews_data = []\n",
    "        \n",
    "        try:\n",
    "            # Navigation\n",
    "            url = f\"https://www.trustpilot.com/review/{company}\"\n",
    "            self.driver.get(url)\n",
    "            self.human_like_behavior()\n",
    "            \n",
    "            # Accepter cookies\n",
    "            try:\n",
    "                cookie_btn = WebDriverWait(self.driver, 5).until(\n",
    "                    EC.element_to_be_clickable((By.ID, \"onetrust-accept-btn-handler\"))\n",
    "                )\n",
    "                cookie_btn.click()\n",
    "                time.sleep(2)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # Trouver reviews avec s√©lecteur valid√©\n",
    "            container_selector = trustpilot_selectors.get('review_container', {}).get('selector')\n",
    "            if not container_selector:\n",
    "                logger.error(\"‚ùå S√©lecteur conteneur review non valid√©\")\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            reviews = WebDriverWait(self.driver, 10).until(\n",
    "                EC.presence_of_all_elements_located((By.CSS_SELECTOR, container_selector))\n",
    "            )\n",
    "            \n",
    "            logger.info(f\"üí¨ Trouv√© {len(reviews)} reviews\")\n",
    "            \n",
    "            for i, review in enumerate(reviews[:max_reviews]):\n",
    "                try:\n",
    "                    review_data = {\n",
    "                        'review_id': f\"trustpilot_{company}_{i}\",\n",
    "                        'company': company,\n",
    "                        'source': 'trustpilot.com',\n",
    "                        'scraped_at': datetime.now().isoformat()\n",
    "                    }\n",
    "                    \n",
    "                    # Extraire texte avec s√©lecteur valid√©\n",
    "                    text_selector = trustpilot_selectors.get('review_text', {}).get('selector')\n",
    "                    if text_selector:\n",
    "                        try:\n",
    "                            text_elem = review.find_element(By.CSS_SELECTOR, text_selector)\n",
    "                            review_data['review_text'] = text_elem.text.strip()\n",
    "                        except:\n",
    "                            review_data['review_text'] = \"\"\n",
    "                    \n",
    "                    # Extraire rating avec s√©lecteur valid√©\n",
    "                    rating_selector = trustpilot_selectors.get('rating', {}).get('selector')\n",
    "                    if rating_selector:\n",
    "                        try:\n",
    "                            rating_elem = review.find_element(By.CSS_SELECTOR, rating_selector)\n",
    "                            # Compter les √©toiles ou extraire de l'attribut\n",
    "                            stars = rating_elem.find_elements(By.CSS_SELECTOR, 'img[alt*=\"star\"]')\n",
    "                            review_data['rating'] = len([s for s in stars if 'filled' in s.get_attribute('alt')])\n",
    "                        except:\n",
    "                            review_data['rating'] = 0\n",
    "                    \n",
    "                    # Extraire nom reviewer avec s√©lecteur valid√©\n",
    "                    name_selector = trustpilot_selectors.get('reviewer_name', {}).get('selector')\n",
    "                    if name_selector:\n",
    "                        try:\n",
    "                            name_elem = review.find_element(By.CSS_SELECTOR, name_selector)\n",
    "                            review_data['reviewer_name'] = name_elem.text.strip()\n",
    "                        except:\n",
    "                            review_data['reviewer_name'] = \"Anonymous\"\n",
    "                    \n",
    "                    # Extraire date avec s√©lecteur valid√©\n",
    "                    date_selector = trustpilot_selectors.get('review_date', {}).get('selector')\n",
    "                    if date_selector:\n",
    "                        try:\n",
    "                            date_elem = review.find_element(By.CSS_SELECTOR, date_selector)\n",
    "                            date_text = date_elem.get_attribute('datetime') or date_elem.text\n",
    "                            # Parser la date\n",
    "                            if 'T' in date_text:  # Format ISO\n",
    "                                review_data['review_date'] = date_text[:10]\n",
    "                            else:\n",
    "                                review_data['review_date'] = date_text\n",
    "                        except:\n",
    "                            review_data['review_date'] = datetime.now().strftime('%Y-%m-%d')\n",
    "                    \n",
    "                    reviews_data.append(review_data)\n",
    "                    \n",
    "                    if i % 10 == 0:\n",
    "                        self.human_like_behavior()\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Erreur review {i}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            logger.info(f\"‚úÖ {len(reviews_data)} reviews Trustpilot scrap√©es avec succ√®s\")\n",
    "            return pd.DataFrame(reviews_data)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Erreur scraping Trustpilot: {e}\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Ferme le driver\"\"\"\n",
    "        if self.driver:\n",
    "            self.driver.quit()\n",
    "\n",
    "# Initialiser le scraper valid√©\n",
    "print(\"üéØ ValidatedScraper pr√™t !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "666d8a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ SYST√àME COMPLET PR√äT !\n",
      "üöÄ Ex√©cutez: main_menu() pour commencer\n",
      "üß™ Ou directement: test_marketplace_scraper()\n"
     ]
    }
   ],
   "source": [
    "# üß™ TESTS CORRIG√âS ET INT√âGRATION COMPL√àTE\n",
    "def test_marketplace_scraper():\n",
    "    \"\"\"\n",
    "    FONCTION DE TEST CORRIG√âE - Compatible avec le syst√®me de reconnaissance\n",
    "    \"\"\"\n",
    "    logger.info(\"üß™ D√©marrage des tests du scraper...\")\n",
    "    \n",
    "    all_results = {}\n",
    "    \n",
    "    try:\n",
    "        # Phase 1: V√©rification de l'environnement\n",
    "        logger.info(\"üîß V√©rification de l'environnement...\")\n",
    "        \n",
    "        # Test des imports\n",
    "        import pandas as pd\n",
    "        from fake_useragent import UserAgent\n",
    "        logger.info(\"‚úÖ Imports OK\")\n",
    "        \n",
    "        # Phase 2: Test de la reconnaissance (optionnel)\n",
    "        print(\"\\nüîç Phase 1 - Reconnaissance des balises (optionnel)\")\n",
    "        choice = input(\"Lancer la reconnaissance des vraies balises ? (o/n): \").lower()\n",
    "        \n",
    "        if choice in ['o', 'oui', 'y', 'yes']:\n",
    "            recognition_results = run_full_site_reconnaissance()\n",
    "            all_results['recognition'] = recognition_results\n",
    "        else:\n",
    "            print(\"‚è≠Ô∏è Reconnaissance ignor√©e - Utilisation des balises par d√©faut\")\n",
    "        \n",
    "        # Phase 3: Test du scraping valid√©\n",
    "        print(\"\\nüéØ Phase 2 - Test du scraping avec balises valid√©es\")\n",
    "        \n",
    "        validated_scraper = ValidatedScraper()\n",
    "        \n",
    "        if not validated_scraper.setup_production_driver(headless=False):\n",
    "            logger.error(\"‚ùå Impossible de configurer le driver\")\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            # Test Amazon (si s√©lecteurs disponibles)\n",
    "            print(\"\\nüõí Test Amazon avec balises valid√©es...\")\n",
    "            amazon_products = validated_scraper.scrape_amazon_products_validated(\"laptop\", max_products=5)\n",
    "            \n",
    "            if not amazon_products.empty:\n",
    "                all_results['amazon_products'] = amazon_products\n",
    "                save_scraped_data(amazon_products, \"amazon_products_validated.csv\")\n",
    "                analyze_scraped_data(amazon_products)\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è Aucun produit Amazon r√©cup√©r√©\")\n",
    "            \n",
    "            time.sleep(3)  # Pause entre tests\n",
    "            \n",
    "            # Test Trustpilot (si s√©lecteurs disponibles)\n",
    "            print(\"\\n‚≠ê Test Trustpilot avec balises valid√©es...\")\n",
    "            trustpilot_reviews = validated_scraper.scrape_trustpilot_reviews_validated(\"amazon\", max_reviews=5)\n",
    "            \n",
    "            if not trustpilot_reviews.empty:\n",
    "                all_results['trustpilot_reviews'] = trustpilot_reviews\n",
    "                save_scraped_data(trustpilot_reviews, \"trustpilot_reviews_validated.csv\")\n",
    "                analyze_scraped_data(trustpilot_reviews)\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è Aucune review Trustpilot r√©cup√©r√©e\")\n",
    "        \n",
    "        finally:\n",
    "            validated_scraper.close()\n",
    "        \n",
    "        # R√©sum√© final\n",
    "        print(\"\\nüìä R√âSUM√â DES TESTS:\")\n",
    "        for test_type, data in all_results.items():\n",
    "            if isinstance(data, pd.DataFrame):\n",
    "                print(f\"   ‚úÖ {test_type}: {len(data)} enregistrements\")\n",
    "            else:\n",
    "                print(f\"   ‚ÑπÔ∏è {test_type}: Donn√©es de reconnaissance\")\n",
    "        \n",
    "        logger.info(\"‚úÖ Tests termin√©s avec succ√®s !\")\n",
    "        return all_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Erreur durant les tests: {e}\")\n",
    "        return None\n",
    "\n",
    "def production_scraping_workflow(sites=['amazon'], search_terms=['laptop'], max_items=50):\n",
    "    \"\"\"\n",
    "    Workflow de production complet : Reconnaissance + Scraping\n",
    "    \"\"\"\n",
    "    print(\"üöÄ WORKFLOW DE PRODUCTION - SCRAPING MARKETPLACE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # √âtape 1: Reconnaissance automatique\n",
    "    print(\"üîç √âTAPE 1: Reconnaissance des balises...\")\n",
    "    recognition_results = run_full_site_reconnaissance()\n",
    "    \n",
    "    if not recognition_results:\n",
    "        print(\"‚ùå Reconnaissance √©chou√©e - Arr√™t du workflow\")\n",
    "        return None\n",
    "    \n",
    "    # √âtape 2: Configuration du scraper\n",
    "    print(\"\\nüéØ √âTAPE 2: Configuration du scraper valid√©...\")\n",
    "    scraper = ValidatedScraper()\n",
    "    \n",
    "    if not scraper.setup_production_driver(headless=True, use_stealth=True):\n",
    "        print(\"‚ùå Configuration driver √©chou√©e\")\n",
    "        return None\n",
    "    \n",
    "    all_data = {}\n",
    "    \n",
    "    try:\n",
    "        # √âtape 3: Scraping par site\n",
    "        for site in sites:\n",
    "            print(f\"\\nüìä √âTAPE 3: Scraping {site.upper()}...\")\n",
    "            \n",
    "            if site == 'amazon':\n",
    "                for term in search_terms:\n",
    "                    print(f\"   üîç Recherche: {term}\")\n",
    "                    products = scraper.scrape_amazon_products_validated(term, max_items)\n",
    "                    \n",
    "                    if not products.empty:\n",
    "                        key = f\"amazon_products_{term}\"\n",
    "                        all_data[key] = products\n",
    "                        save_scraped_data(products, f\"production_{key}.csv\")\n",
    "                        print(f\"   ‚úÖ {len(products)} produits r√©cup√©r√©s\")\n",
    "                    \n",
    "                    time.sleep(5)  # D√©lai entre recherches\n",
    "            \n",
    "            elif site == 'trustpilot':\n",
    "                companies = ['amazon', 'ebay', 'apple']\n",
    "                for company in companies:\n",
    "                    print(f\"   ‚≠ê Reviews: {company}\")\n",
    "                    reviews = scraper.scrape_trustpilot_reviews_validated(company, max_items)\n",
    "                    \n",
    "                    if not reviews.empty:\n",
    "                        key = f\"trustpilot_reviews_{company}\"\n",
    "                        all_data[key] = reviews\n",
    "                        save_scraped_data(reviews, f\"production_{key}.csv\")\n",
    "                        print(f\"   ‚úÖ {len(reviews)} reviews r√©cup√©r√©es\")\n",
    "                    \n",
    "                    time.sleep(5)\n",
    "        \n",
    "        # √âtape 4: R√©sum√© final\n",
    "        print(\"\\nüéâ WORKFLOW TERMIN√â !\")\n",
    "        total_records = sum(len(df) for df in all_data.values() if isinstance(df, pd.DataFrame))\n",
    "        print(f\"üìä Total des enregistrements: {total_records}\")\n",
    "        \n",
    "        for key, df in all_data.items():\n",
    "            if isinstance(df, pd.DataFrame):\n",
    "                print(f\"   üìÑ {key}: {len(df)} enregistrements\")\n",
    "        \n",
    "        return all_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Erreur workflow: {e}\")\n",
    "        return None\n",
    "        \n",
    "    finally:\n",
    "        scraper.close()\n",
    "\n",
    "# MENU PRINCIPAL\n",
    "def main_menu():\n",
    "    \"\"\"Menu principal pour l'utilisation du scraper\"\"\"\n",
    "    print(\"üéØ MENU PRINCIPAL - DATA COLLECTION SCRAPER\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"1. üß™ Test complet (reconnaissance + scraping)\")\n",
    "    print(\"2. üîç Reconnaissance seule des balises\")\n",
    "    print(\"3. üéØ Scraping avec balises valid√©es\")\n",
    "    print(\"4. üöÄ Workflow de production complet\")\n",
    "    print(\"5. ‚ùå Quitter\")\n",
    "    \n",
    "    try:\n",
    "        choice = input(\"\\nVotre choix (1-5): \").strip()\n",
    "        \n",
    "        if choice == \"1\":\n",
    "            test_marketplace_scraper()\n",
    "        elif choice == \"2\":\n",
    "            run_full_site_reconnaissance()\n",
    "        elif choice == \"3\":\n",
    "            scraper = ValidatedScraper()\n",
    "            scraper.setup_production_driver(headless=False)\n",
    "            # Exemple simple\n",
    "            products = scraper.scrape_amazon_products_validated(\"smartphone\", 10)\n",
    "            if not products.empty:\n",
    "                save_scraped_data(products, \"quick_scraping.csv\")\n",
    "            scraper.close()\n",
    "        elif choice == \"4\":\n",
    "            production_scraping_workflow()\n",
    "        elif choice == \"5\":\n",
    "            print(\"üëã Au revoir !\")\n",
    "        else:\n",
    "            print(\"‚ùå Choix invalide\")\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nüëã Arr√™t demand√© par l'utilisateur\")\n",
    "\n",
    "print(\"üéØ SYST√àME COMPLET PR√äT !\")\n",
    "print(\"üöÄ Ex√©cutez: main_menu() pour commencer\")\n",
    "print(\"üß™ Ou directement: test_marketplace_scraper()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3d5ce55e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ MENU PRINCIPAL - DATA COLLECTION SCRAPER\n",
      "==================================================\n",
      "1. üß™ Test complet (reconnaissance + scraping)\n",
      "2. üîç Reconnaissance seule des balises\n",
      "3. üéØ Scraping avec balises valid√©es\n",
      "4. üöÄ Workflow de production complet\n",
      "5. ‚ùå Quitter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 16:13:59,079 - INFO - üîç Reconnaissance Amazon...\n",
      "2025-06-27 16:13:59,137 - INFO - ====== WebDriver manager ======\n",
      "2025-06-27 16:13:59,137 - INFO - ====== WebDriver manager ======\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ WORKFLOW DE PRODUCTION - SCRAPING MARKETPLACE\n",
      "============================================================\n",
      "üîç √âTAPE 1: Reconnaissance des balises...\n",
      "üéØ LANCEMENT DE LA RECONNAISSANCE COMPL√àTE\n",
      "============================================================\n",
      "\n",
      "üõí Reconnaissance Amazon...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 16:14:00,442 - INFO - Get LATEST chromedriver version for google-chrome\n",
      "2025-06-27 16:14:00,651 - INFO - Get LATEST chromedriver version for google-chrome\n",
      "2025-06-27 16:14:00,651 - INFO - Get LATEST chromedriver version for google-chrome\n",
      "2025-06-27 16:14:00,872 - INFO - There is no [win64] chromedriver \"138.0.7204.49\" for browser google-chrome \"138.0.7204\" in cache\n",
      "2025-06-27 16:14:00,873 - INFO - Get LATEST chromedriver version for google-chrome\n",
      "2025-06-27 16:14:00,872 - INFO - There is no [win64] chromedriver \"138.0.7204.49\" for browser google-chrome \"138.0.7204\" in cache\n",
      "2025-06-27 16:14:00,873 - INFO - Get LATEST chromedriver version for google-chrome\n",
      "2025-06-27 16:14:01,437 - INFO - WebDriver version 138.0.7204.49 selected\n",
      "2025-06-27 16:14:01,439 - INFO - Modern chrome version https://storage.googleapis.com/chrome-for-testing-public/138.0.7204.49/win32/chromedriver-win32.zip\n",
      "2025-06-27 16:14:01,439 - INFO - About to download new driver from https://storage.googleapis.com/chrome-for-testing-public/138.0.7204.49/win32/chromedriver-win32.zip\n",
      "2025-06-27 16:14:01,437 - INFO - WebDriver version 138.0.7204.49 selected\n",
      "2025-06-27 16:14:01,439 - INFO - Modern chrome version https://storage.googleapis.com/chrome-for-testing-public/138.0.7204.49/win32/chromedriver-win32.zip\n",
      "2025-06-27 16:14:01,439 - INFO - About to download new driver from https://storage.googleapis.com/chrome-for-testing-public/138.0.7204.49/win32/chromedriver-win32.zip\n",
      "2025-06-27 16:14:01,645 - INFO - Driver downloading response is 200\n",
      "2025-06-27 16:14:01,645 - INFO - Driver downloading response is 200\n",
      "2025-06-27 16:14:03,213 - INFO - Get LATEST chromedriver version for google-chrome\n",
      "2025-06-27 16:14:03,213 - INFO - Get LATEST chromedriver version for google-chrome\n",
      "2025-06-27 16:14:03,468 - INFO - Driver has been saved in cache [C:\\Users\\Yann\\.wdm\\drivers\\chromedriver\\win64\\138.0.7204.49]\n",
      "2025-06-27 16:14:03,468 - INFO - Driver has been saved in cache [C:\\Users\\Yann\\.wdm\\drivers\\chromedriver\\win64\\138.0.7204.49]\n",
      "2025-06-27 16:14:05,064 - INFO - ‚úÖ Scout driver configur√©\n",
      "2025-06-27 16:14:05,064 - INFO - ‚úÖ Scout driver configur√©\n",
      "2025-06-27 16:14:14,122 - INFO - ‚úÖ product_container: [data-component-type=\"s-search-result\"] (21 √©l√©ments)\n",
      "2025-06-27 16:14:14,136 - INFO - ‚úÖ title: .a-size-medium span (24 √©l√©ments)\n",
      "2025-06-27 16:14:14,122 - INFO - ‚úÖ product_container: [data-component-type=\"s-search-result\"] (21 √©l√©ments)\n",
      "2025-06-27 16:14:14,136 - INFO - ‚úÖ title: .a-size-medium span (24 √©l√©ments)\n",
      "2025-06-27 16:14:14,157 - INFO - ‚úÖ price: .a-price-whole (31 √©l√©ments)\n",
      "2025-06-27 16:14:14,166 - INFO - ‚úÖ rating: .a-icon-alt (33 √©l√©ments)\n",
      "2025-06-27 16:14:14,157 - INFO - ‚úÖ price: .a-price-whole (31 √©l√©ments)\n",
      "2025-06-27 16:14:14,166 - INFO - ‚úÖ rating: .a-icon-alt (33 √©l√©ments)\n",
      "2025-06-27 16:14:14,179 - INFO - ‚úÖ image: .s-image (43 √©l√©ments)\n",
      "2025-06-27 16:14:14,179 - INFO - ‚úÖ image: .s-image (43 √©l√©ments)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Amazon reconnaissance termin√©e\n",
      "   ‚úÖ product_container: [data-component-type=\"s-search-result\"]\n",
      "   ‚úÖ title: .a-size-medium span\n",
      "   ‚úÖ price: .a-price-whole\n",
      "   ‚úÖ rating: .a-icon-alt\n",
      "   ‚úÖ image: .s-image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 16:14:16,181 - INFO - üîç Reconnaissance eBay...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üè™ Reconnaissance eBay...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 16:14:21,305 - INFO - ‚úÖ product_container: .s-item (82 √©l√©ments)\n",
      "2025-06-27 16:14:21,317 - INFO - ‚úÖ title: .s-item__title (82 √©l√©ments)\n",
      "2025-06-27 16:14:21,317 - INFO - ‚úÖ title: .s-item__title (82 √©l√©ments)\n",
      "2025-06-27 16:14:21,330 - INFO - ‚úÖ price: .s-item__price (62 √©l√©ments)\n",
      "2025-06-27 16:14:21,342 - INFO - ‚úÖ condition: .SECONDARY_INFO (62 √©l√©ments)\n",
      "2025-06-27 16:14:21,330 - INFO - ‚úÖ price: .s-item__price (62 √©l√©ments)\n",
      "2025-06-27 16:14:21,342 - INFO - ‚úÖ condition: .SECONDARY_INFO (62 √©l√©ments)\n",
      "2025-06-27 16:14:21,354 - INFO - ‚úÖ shipping: .s-item__shipping (60 √©l√©ments)\n",
      "2025-06-27 16:14:21,354 - INFO - ‚úÖ shipping: .s-item__shipping (60 √©l√©ments)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ eBay reconnaissance termin√©e\n",
      "   ‚úÖ product_container: .s-item\n",
      "   ‚úÖ title: .s-item__title\n",
      "   ‚úÖ price: .s-item__price\n",
      "   ‚úÖ condition: .SECONDARY_INFO\n",
      "   ‚úÖ shipping: .s-item__shipping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 16:14:23,356 - INFO - üîç Reconnaissance Trustpilot pour amazon...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚≠ê Reconnaissance Trustpilot...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 16:14:32,028 - INFO - ‚úÖ review_container: article[data-service-review-card-paper] (24 √©l√©ments)\n",
      "2025-06-27 16:14:32,045 - INFO - ‚úÖ review_text: [data-service-review-text-typography=\"true\"] (20 √©l√©ments)\n",
      "2025-06-27 16:14:32,045 - INFO - ‚úÖ review_text: [data-service-review-text-typography=\"true\"] (20 √©l√©ments)\n",
      "2025-06-27 16:14:32,102 - INFO - ‚úÖ rating: [data-service-review-rating] (20 √©l√©ments)\n",
      "2025-06-27 16:14:32,102 - INFO - ‚úÖ rating: [data-service-review-rating] (20 √©l√©ments)\n",
      "2025-06-27 16:14:32,119 - INFO - ‚úÖ reviewer_name: [data-consumer-name-typography=\"true\"] (24 √©l√©ments)\n",
      "2025-06-27 16:14:32,136 - INFO - ‚úÖ review_date: time[datetime] (24 √©l√©ments)\n",
      "2025-06-27 16:14:32,119 - INFO - ‚úÖ reviewer_name: [data-consumer-name-typography=\"true\"] (24 √©l√©ments)\n",
      "2025-06-27 16:14:32,136 - INFO - ‚úÖ review_date: time[datetime] (24 √©l√©ments)\n",
      "2025-06-27 16:14:32,138 - INFO - üíæ S√©lecteurs sauvegard√©s: ../config/validated_selectors.json\n",
      "2025-06-27 16:14:32,138 - INFO - üíæ S√©lecteurs sauvegard√©s: ../config/validated_selectors.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Trustpilot reconnaissance termin√©e\n",
      "   ‚úÖ review_container: article[data-service-review-card-paper]\n",
      "   ‚úÖ review_text: [data-service-review-text-typography=\"true\"]\n",
      "   ‚úÖ rating: [data-service-review-rating]\n",
      "   ‚úÖ reviewer_name: [data-consumer-name-typography=\"true\"]\n",
      "   ‚úÖ review_date: time[datetime]\n",
      "\n",
      "üíæ Sauvegarde des s√©lecteurs...\n",
      "\n",
      "üìä R√âSUM√â DE LA RECONNAISSANCE:\n",
      "   AMAZON: 5/5 s√©lecteurs valid√©s\n",
      "   EBAY: 5/5 s√©lecteurs valid√©s\n",
      "   TRUSTPILOT: 5/5 s√©lecteurs valid√©s\n",
      "\n",
      "üéØ TOTAL: 15/15 s√©lecteurs fonctionnels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 16:14:38,646 - INFO - ‚úÖ S√©lecteurs charg√©s depuis ../config/validated_selectors.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ √âTAPE 2: Configuration du scraper valid√©...\n",
      "   AMAZON: 5 s√©lecteurs valid√©s\n",
      "   EBAY: 5 s√©lecteurs valid√©s\n",
      "   TRUSTPILOT: 5 s√©lecteurs valid√©s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 16:14:40,746 - INFO - patching driver executable C:\\Users\\Yann\\appdata\\roaming\\undetected_chromedriver\\undetected_chromedriver.exe\n",
      "2025-06-27 16:14:41,551 - INFO - ‚úÖ Driver production configur√©\n",
      "2025-06-27 16:14:41,552 - INFO - üõí Scraping Amazon valid√©: laptop\n",
      "2025-06-27 16:14:41,551 - INFO - ‚úÖ Driver production configur√©\n",
      "2025-06-27 16:14:41,552 - INFO - üõí Scraping Amazon valid√©: laptop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä √âTAPE 3: Scraping AMAZON...\n",
      "   üîç Recherche: laptop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 16:14:54,036 - INFO - üì¶ Trouv√© 21 produits\n",
      "2025-06-27 16:15:05,766 - INFO - ‚úÖ 21 produits Amazon scrap√©s avec succ√®s\n",
      "2025-06-27 16:15:05,785 - INFO - üíæ Donn√©es sauvegard√©es: ../data/raw\\20250627_161505_production_amazon_products_laptop.csv (21 enregistrements)\n",
      "2025-06-27 16:15:05,766 - INFO - ‚úÖ 21 produits Amazon scrap√©s avec succ√®s\n",
      "2025-06-27 16:15:05,785 - INFO - üíæ Donn√©es sauvegard√©es: ../data/raw\\20250627_161505_production_amazon_products_laptop.csv (21 enregistrements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ 21 produits r√©cup√©r√©s\n",
      "\n",
      "üéâ WORKFLOW TERMIN√â !\n",
      "üìä Total des enregistrements: 21\n",
      "   üìÑ amazon_products_laptop: 21 enregistrements\n",
      "\n",
      "üéâ WORKFLOW TERMIN√â !\n",
      "üìä Total des enregistrements: 21\n",
      "   üìÑ amazon_products_laptop: 21 enregistrements\n"
     ]
    }
   ],
   "source": [
    "main_menu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2265b9f8",
   "metadata": {},
   "source": [
    "# Scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca2dfa9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Classe ProductReviewScout cr√©√©e\n"
     ]
    }
   ],
   "source": [
    "class ProductReviewScout:\n",
    "    \"\"\"\n",
    "    Classe sp√©cialis√©e pour d√©tecter les balises de reviews produits sur les marketplaces\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.driver = None\n",
    "        self.selectors = {\n",
    "            'amazon': {\n",
    "                'products': {\n",
    "                    'product_container': '[data-component-type=\"s-search-result\"]',\n",
    "                    'product_title': 'h2 a span, h2 span',\n",
    "                    'product_url': 'h2 a',\n",
    "                    'product_price': '.a-price-whole, .a-price .a-offscreen',\n",
    "                    'product_rating': '.a-icon-alt',\n",
    "                    'product_category': '.a-color-state, .s-breadcrumb'\n",
    "                },\n",
    "                'reviews': {\n",
    "                    'reviews_container': '[data-hook=\"review\"]',\n",
    "                    'review_title': '[data-hook=\"review-title\"] span',\n",
    "                    'review_text': '[data-hook=\"review-body\"] span',\n",
    "                    'review_rating': '[data-hook=\"review-star-rating\"] .a-icon-alt',\n",
    "                    'reviewer_name': '.a-profile-name',\n",
    "                    'review_date': '[data-hook=\"review-date\"]',\n",
    "                    'next_page': '.a-pagination .a-last a',\n",
    "                    'review_helpfulness': '[data-hook=\"helpful-vote-statement\"]'\n",
    "                }\n",
    "            },\n",
    "            'ebay': {\n",
    "                'products': {\n",
    "                    'product_container': '.s-item',\n",
    "                    'product_title': '.s-item__title',\n",
    "                    'product_url': '.s-item__link',\n",
    "                    'product_price': '.s-item__price',\n",
    "                    'product_rating': '.ebay-review-star-rating',\n",
    "                    'product_category': '.breadcrumbs'\n",
    "                },\n",
    "                'reviews': {\n",
    "                    'reviews_container': '.review-item',\n",
    "                    'review_title': '.review-item-title',\n",
    "                    'review_text': '.review-item-content',\n",
    "                    'review_rating': '.star-rating',\n",
    "                    'reviewer_name': '.review-item-author',\n",
    "                    'review_date': '.review-item-date',\n",
    "                    'next_page': '.pager .next'\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def setup_driver(self, headless=True):\n",
    "        \"\"\"Configure le driver pour la d√©tection\"\"\"\n",
    "        try:\n",
    "            options = uc.ChromeOptions()\n",
    "            options.add_argument('--no-sandbox')\n",
    "            options.add_argument('--disable-dev-shm-usage')\n",
    "            options.add_argument('--disable-gpu')\n",
    "            options.add_argument('--disable-extensions')\n",
    "            \n",
    "            if headless:\n",
    "                options.add_argument('--headless')\n",
    "                \n",
    "            self.driver = uc.Chrome(options=options, version_main=None)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur setup driver: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def detect_product_selectors(self, site_url, search_term=\"laptop\"):\n",
    "        \"\"\"D√©tecte automatiquement les s√©lecteurs pour les produits\"\"\"\n",
    "        if not self.driver:\n",
    "            print(\"‚ùå Driver non initialis√©\")\n",
    "            return None\n",
    "            \n",
    "        try:\n",
    "            # Aller sur la page de recherche\n",
    "            search_url = self._build_search_url(site_url, search_term)\n",
    "            self.driver.get(search_url)\n",
    "            time.sleep(3)\n",
    "            \n",
    "            detected_selectors = {}\n",
    "            \n",
    "            if 'amazon' in site_url:\n",
    "                detected_selectors = self._detect_amazon_selectors()\n",
    "            elif 'ebay' in site_url:\n",
    "                detected_selectors = self._detect_ebay_selectors()\n",
    "                \n",
    "            return detected_selectors\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur d√©tection: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def detect_review_selectors(self, product_url):\n",
    "        \"\"\"D√©tecte automatiquement les s√©lecteurs pour les reviews\"\"\"\n",
    "        if not self.driver:\n",
    "            print(\"‚ùå Driver non initialis√©\")\n",
    "            return None\n",
    "            \n",
    "        try:\n",
    "            self.driver.get(product_url)\n",
    "            time.sleep(2)\n",
    "            \n",
    "            # Chercher le lien vers les reviews\n",
    "            review_links = self.driver.find_elements(By.CSS_SELECTOR, 'a[href*=\"review\"], a[href*=\"customer-review\"]')\n",
    "            if review_links:\n",
    "                review_url = review_links[0].get_attribute('href')\n",
    "                self.driver.get(review_url)\n",
    "                time.sleep(3)\n",
    "            \n",
    "            detected_selectors = {}\n",
    "            \n",
    "            if 'amazon' in product_url:\n",
    "                detected_selectors = self._detect_amazon_review_selectors()\n",
    "            elif 'ebay' in product_url:\n",
    "                detected_selectors = self._detect_ebay_review_selectors()\n",
    "                \n",
    "            return detected_selectors\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur d√©tection reviews: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _build_search_url(self, site_url, search_term):\n",
    "        \"\"\"Construit l'URL de recherche selon le site\"\"\"\n",
    "        if 'amazon' in site_url:\n",
    "            return f\"https://www.amazon.com/s?k={search_term.replace(' ', '+')}\"\n",
    "        elif 'ebay' in site_url:\n",
    "            return f\"https://www.ebay.com/sch/i.html?_nkw={search_term.replace(' ', '+')}\"\n",
    "        return site_url\n",
    "    \n",
    "    def _detect_amazon_selectors(self):\n",
    "        \"\"\"D√©tecte les s√©lecteurs Amazon automatiquement\"\"\"\n",
    "        selectors = {}\n",
    "        \n",
    "        # Test de diff√©rents s√©lecteurs possibles\n",
    "        product_selectors = [\n",
    "            '[data-component-type=\"s-search-result\"]',\n",
    "            '.s-result-item',\n",
    "            '.s-widget-container'\n",
    "        ]\n",
    "        \n",
    "        for selector in product_selectors:\n",
    "            elements = self.driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "            if len(elements) > 3:  # Au moins 3 produits trouv√©s\n",
    "                selectors['product_container'] = selector\n",
    "                break\n",
    "        \n",
    "        # Test des s√©lecteurs de titre\n",
    "        title_selectors = [\n",
    "            'h2 a span',\n",
    "            '.s-size-mini span',\n",
    "            'h2 span'\n",
    "        ]\n",
    "        \n",
    "        for selector in title_selectors:\n",
    "            elements = self.driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "            if elements and elements[0].text.strip():\n",
    "                selectors['product_title'] = selector\n",
    "                break\n",
    "        \n",
    "        # Autres s√©lecteurs...\n",
    "        selectors.update({\n",
    "            'product_url': 'h2 a, .a-link-normal',\n",
    "            'product_price': '.a-price .a-offscreen, .a-price-whole',\n",
    "            'product_rating': '.a-icon-alt'\n",
    "        })\n",
    "        \n",
    "        return selectors\n",
    "    \n",
    "    def _detect_amazon_review_selectors(self):\n",
    "        \"\"\"D√©tecte les s√©lecteurs de reviews Amazon\"\"\"\n",
    "        selectors = {}\n",
    "        \n",
    "        # Test des conteneurs de review\n",
    "        review_selectors = [\n",
    "            '[data-hook=\"review\"]',\n",
    "            '.review',\n",
    "            '.cr-original-review-content'\n",
    "        ]\n",
    "        \n",
    "        for selector in review_selectors:\n",
    "            elements = self.driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "            if len(elements) > 2:\n",
    "                selectors['reviews_container'] = selector\n",
    "                break\n",
    "        \n",
    "        selectors.update({\n",
    "            'review_title': '[data-hook=\"review-title\"] span, .review-title',\n",
    "            'review_text': '[data-hook=\"review-body\"] span, .review-text',\n",
    "            'review_rating': '[data-hook=\"review-star-rating\"] .a-icon-alt, .review-rating',\n",
    "            'reviewer_name': '.a-profile-name, .review-author',\n",
    "            'review_date': '[data-hook=\"review-date\"], .review-date'\n",
    "        })\n",
    "        \n",
    "        return selectors\n",
    "    \n",
    "    def _detect_ebay_selectors(self):\n",
    "        \"\"\"D√©tecte les s√©lecteurs eBay automatiquement\"\"\"\n",
    "        # Implementation similaire pour eBay\n",
    "        return self.selectors['ebay']['products']\n",
    "    \n",
    "    def _detect_ebay_review_selectors(self):\n",
    "        \"\"\"D√©tecte les s√©lecteurs de reviews eBay\"\"\"\n",
    "        return self.selectors['ebay']['reviews']\n",
    "    \n",
    "    def save_selectors(self, detected_selectors, filename=\"../config/product_review_selectors.json\"):\n",
    "        \"\"\"Sauvegarde les s√©lecteurs d√©tect√©s\"\"\"\n",
    "        try:\n",
    "            os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "            with open(filename, 'w', encoding='utf-8') as f:\n",
    "                json.dump(detected_selectors, f, indent=2, ensure_ascii=False)\n",
    "            print(f\"‚úÖ S√©lecteurs sauvegard√©s: {filename}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur sauvegarde: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Ferme le driver\"\"\"\n",
    "        if self.driver:\n",
    "            self.driver.quit()\n",
    "\n",
    "print(\"‚úÖ Classe ProductReviewScout cr√©√©e\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a701baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Classe ProductReviewScraper cr√©√©e\n"
     ]
    }
   ],
   "source": [
    "class ProductReviewScraper:\n",
    "    \"\"\"\n",
    "    Scraper sp√©cialis√© pour r√©cup√©rer les reviews de produits avec balises valid√©es\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, selectors_file=\"../config/product_review_selectors.json\"):\n",
    "        self.driver = None\n",
    "        self.selectors = self.load_selectors(selectors_file)\n",
    "        self.scraped_data = []\n",
    "        \n",
    "    def load_selectors(self, filename):\n",
    "        \"\"\"Charge les s√©lecteurs valid√©s\"\"\"\n",
    "        try:\n",
    "            with open(filename, 'r', encoding='utf-8') as f:\n",
    "                return json.load(f)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Erreur chargement s√©lecteurs: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def setup_driver(self, headless=True):\n",
    "        \"\"\"Configure le driver anti-d√©tection pour le scraping\"\"\"\n",
    "        try:\n",
    "            options = uc.ChromeOptions()\n",
    "            options.add_argument('--no-sandbox')\n",
    "            options.add_argument('--disable-dev-shm-usage')\n",
    "            options.add_argument('--disable-gpu')\n",
    "            options.add_argument('--disable-extensions')\n",
    "            options.add_argument('--disable-notifications')\n",
    "            options.add_argument('--no-first-run')\n",
    "            \n",
    "            if headless:\n",
    "                options.add_argument('--headless')\n",
    "            \n",
    "            # User agent al√©atoire\n",
    "            user_agent = random.choice(REALISTIC_USER_AGENTS)\n",
    "            options.add_argument(f'--user-agent={user_agent}')\n",
    "            \n",
    "            # Options exp√©rimentales\n",
    "            options.add_experimental_option('useAutomationExtension', False)\n",
    "            options.add_experimental_option(\"prefs\", {\n",
    "                \"profile.default_content_setting_values.notifications\": 2\n",
    "            })\n",
    "            \n",
    "            self.driver = uc.Chrome(options=options, version_main=None)\n",
    "            \n",
    "            # Scripts anti-d√©tection\n",
    "            self.driver.execute_cdp_cmd('Page.addScriptToEvaluateOnNewDocument', {\n",
    "                'source': '''\n",
    "                    Object.defineProperty(navigator, 'webdriver', {\n",
    "                        get: () => undefined,\n",
    "                    });\n",
    "                '''\n",
    "            })\n",
    "            \n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur setup driver: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def scrape_category_product_reviews(self, site_url, category_search, max_products=10, reviews_per_rating=50):\n",
    "        \"\"\"\n",
    "        Scrape les reviews de produits d'une cat√©gorie sp√©cifique\n",
    "        \n",
    "        Args:\n",
    "            site_url: URL du site (amazon.com, ebay.com)\n",
    "            category_search: terme de recherche pour la cat√©gorie\n",
    "            max_products: nombre max de produits √† scraper (d√©faut: 10)\n",
    "            reviews_per_rating: nombre de reviews par note (d√©faut: 50)\n",
    "        \"\"\"\n",
    "        if not self.driver:\n",
    "            print(\"‚ùå Driver non initialis√©\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        try:\n",
    "            print(f\"üîç Recherche de produits pour: {category_search}\")\n",
    "            \n",
    "            # 1. R√©cup√©rer la liste des produits\n",
    "            products = self._get_products_list(site_url, category_search, max_products)\n",
    "            \n",
    "            if not products:\n",
    "                print(\"‚ùå Aucun produit trouv√©\")\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            print(f\"‚úÖ {len(products)} produits trouv√©s\")\n",
    "            \n",
    "            # 2. Pour chaque produit, r√©cup√©rer les reviews\n",
    "            all_reviews = []\n",
    "            \n",
    "            for i, product in enumerate(products[:max_products], 1):\n",
    "                print(f\"üì¶ Produit {i}/{len(products)}: {product['title'][:50]}...\")\n",
    "                \n",
    "                product_reviews = self._scrape_product_reviews(\n",
    "                    product, \n",
    "                    reviews_per_rating\n",
    "                )\n",
    "                \n",
    "                if product_reviews:\n",
    "                    all_reviews.extend(product_reviews)\n",
    "                    print(f\"‚úÖ {len(product_reviews)} reviews r√©cup√©r√©es\")\n",
    "                else:\n",
    "                    print(\"‚ö†Ô∏è Aucune review trouv√©e\")\n",
    "                \n",
    "                # D√©lai entre produits\n",
    "                time.sleep(random.uniform(2, 5))\n",
    "            \n",
    "            # 3. Cr√©er le DataFrame final\n",
    "            df = pd.DataFrame(all_reviews)\n",
    "            \n",
    "            if not df.empty:\n",
    "                # Nettoyage des donn√©es\n",
    "                df = self._clean_review_data(df)\n",
    "                print(f\"‚úÖ Total: {len(df)} reviews r√©cup√©r√©es\")\n",
    "            \n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur scraping: {e}\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def _get_products_list(self, site_url, search_term, max_products):\n",
    "        \"\"\"R√©cup√®re la liste des produits √† partir de la recherche\"\"\"\n",
    "        try:\n",
    "            # Construire l'URL de recherche\n",
    "            search_url = self._build_search_url(site_url, search_term)\n",
    "            print(f\"üîó URL: {search_url}\")\n",
    "            \n",
    "            self.driver.get(search_url)\n",
    "            time.sleep(3)\n",
    "            \n",
    "            products = []\n",
    "            site_type = 'amazon' if 'amazon' in site_url else 'ebay'\n",
    "            \n",
    "            # Utiliser les s√©lecteurs appropri√©s\n",
    "            if site_type in self.selectors and 'products' in self.selectors[site_type]:\n",
    "                selectors = self.selectors[site_type]['products']\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è S√©lecteurs non trouv√©s pour {site_type}\")\n",
    "                return []\n",
    "            \n",
    "            # R√©cup√©rer les conteneurs de produits\n",
    "            product_containers = self.driver.find_elements(\n",
    "                By.CSS_SELECTOR, \n",
    "                selectors.get('product_container', '.s-result-item')\n",
    "            )\n",
    "            \n",
    "            for container in product_containers[:max_products]:\n",
    "                try:\n",
    "                    # Extraire les infos du produit\n",
    "                    title_elem = container.find_element(\n",
    "                        By.CSS_SELECTOR, \n",
    "                        selectors.get('product_title', 'h2 span')\n",
    "                    )\n",
    "                    \n",
    "                    url_elem = container.find_element(\n",
    "                        By.CSS_SELECTOR, \n",
    "                        selectors.get('product_url', 'h2 a')\n",
    "                    )\n",
    "                    \n",
    "                    product_data = {\n",
    "                        'title': title_elem.text.strip(),\n",
    "                        'url': url_elem.get_attribute('href'),\n",
    "                        'category': search_term\n",
    "                    }\n",
    "                    \n",
    "                    # Prix optionnel\n",
    "                    try:\n",
    "                        price_elem = container.find_element(\n",
    "                            By.CSS_SELECTOR, \n",
    "                            selectors.get('product_price', '.a-price')\n",
    "                        )\n",
    "                        product_data['price'] = price_elem.text.strip()\n",
    "                    except:\n",
    "                        product_data['price'] = 'N/A'\n",
    "                    \n",
    "                    # Rating optionnel\n",
    "                    try:\n",
    "                        rating_elem = container.find_element(\n",
    "                            By.CSS_SELECTOR, \n",
    "                            selectors.get('product_rating', '.a-icon-alt')\n",
    "                        )\n",
    "                        product_data['rating'] = rating_elem.get_attribute('textContent')\n",
    "                    except:\n",
    "                        product_data['rating'] = 'N/A'\n",
    "                    \n",
    "                    if product_data['title'] and product_data['url']:\n",
    "                        products.append(product_data)\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    continue  # Ignorer les produits probl√©matiques\n",
    "            \n",
    "            return products\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur r√©cup√©ration produits: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def _scrape_product_reviews(self, product, reviews_per_rating):\n",
    "        \"\"\"Scrape les reviews d'un produit sp√©cifique\"\"\"\n",
    "        try:\n",
    "            # Aller sur la page produit\n",
    "            self.driver.get(product['url'])\n",
    "            time.sleep(2)\n",
    "            \n",
    "            # Chercher le lien vers les reviews\n",
    "            reviews_url = self._find_reviews_url(product['url'])\n",
    "            \n",
    "            if not reviews_url:\n",
    "                print(\"‚ö†Ô∏è Lien reviews non trouv√©\")\n",
    "                return []\n",
    "            \n",
    "            # Aller sur la page des reviews\n",
    "            self.driver.get(reviews_url)\n",
    "            time.sleep(3)\n",
    "            \n",
    "            reviews = []\n",
    "            site_type = 'amazon' if 'amazon' in product['url'] else 'ebay'\n",
    "            \n",
    "            # R√©cup√©rer les reviews par note (5, 4, 3, 2, 1 √©toiles)\n",
    "            for rating in [5, 4, 3, 2, 1]:\n",
    "                rating_reviews = self._scrape_reviews_by_rating(\n",
    "                    site_type, \n",
    "                    rating, \n",
    "                    reviews_per_rating,\n",
    "                    product\n",
    "                )\n",
    "                reviews.extend(rating_reviews)\n",
    "                \n",
    "                # D√©lai entre les notes\n",
    "                time.sleep(random.uniform(1, 3))\n",
    "            \n",
    "            return reviews\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur scraping reviews produit: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def _find_reviews_url(self, product_url):\n",
    "        \"\"\"Trouve l'URL de la page des reviews\"\"\"\n",
    "        try:\n",
    "            # Chercher les liens vers les reviews\n",
    "            review_selectors = [\n",
    "                'a[href*=\"customer-reviews\"]',\n",
    "                'a[href*=\"reviews\"]',\n",
    "                'a[href*=\"review\"]',\n",
    "                '.a-link-emphasis[href*=\"review\"]'\n",
    "            ]\n",
    "            \n",
    "            for selector in review_selectors:\n",
    "                try:\n",
    "                    link = self.driver.find_element(By.CSS_SELECTOR, selector)\n",
    "                    return link.get_attribute('href')\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            # Si aucun lien trouv√©, construire l'URL\n",
    "            if 'amazon' in product_url:\n",
    "                # Extraire l'ASIN depuis l'URL\n",
    "                import re\n",
    "                asin_match = re.search(r'/dp/([A-Z0-9]{10})', product_url)\n",
    "                if asin_match:\n",
    "                    asin = asin_match.group(1)\n",
    "                    return f\"https://www.amazon.com/product-reviews/{asin}\"\n",
    "            \n",
    "            return None\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur recherche URL reviews: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _scrape_reviews_by_rating(self, site_type, rating, max_reviews, product_info):\n",
    "        \"\"\"Scrape les reviews pour une note sp√©cifique\"\"\"\n",
    "        reviews = []\n",
    "        \n",
    "        try:\n",
    "            # Filtrer par note si possible\n",
    "            self._filter_by_rating(site_type, rating)\n",
    "            time.sleep(2)\n",
    "            \n",
    "            # R√©cup√©rer les reviews\n",
    "            selectors = self.selectors.get(site_type, {}).get('reviews', {})\n",
    "            pages_scraped = 0\n",
    "            max_pages = 10  # Limite de pages\n",
    "            \n",
    "            while len(reviews) < max_reviews and pages_scraped < max_pages:\n",
    "                # Reviews de la page actuelle\n",
    "                page_reviews = self._extract_reviews_from_page(selectors, product_info, rating)\n",
    "                \n",
    "                if not page_reviews:\n",
    "                    break\n",
    "                \n",
    "                reviews.extend(page_reviews)\n",
    "                \n",
    "                # Passer √† la page suivante\n",
    "                if not self._go_to_next_page(selectors):\n",
    "                    break\n",
    "                \n",
    "                pages_scraped += 1\n",
    "                time.sleep(random.uniform(2, 4))\n",
    "            \n",
    "            # Limiter au nombre demand√©\n",
    "            return reviews[:max_reviews]\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur scraping rating {rating}: {e}\")\n",
    "            return reviews\n",
    "    \n",
    "    def _filter_by_rating(self, site_type, rating):\n",
    "        \"\"\"Filtre les reviews par note\"\"\"\n",
    "        try:\n",
    "            if site_type == 'amazon':\n",
    "                # Chercher le filtre par √©toiles\n",
    "                filter_selector = f'a[href*=\"filterByStar=five_star\"], a[href*=\"star_rating={rating}\"]'\n",
    "                filter_links = self.driver.find_elements(By.CSS_SELECTOR, filter_selector)\n",
    "                \n",
    "                for link in filter_links:\n",
    "                    if f\"{rating}\" in link.get_attribute('href') or f\"{rating} star\" in link.text:\n",
    "                        link.click()\n",
    "                        return True\n",
    "        except:\n",
    "            pass\n",
    "        return False\n",
    "    \n",
    "    def _extract_reviews_from_page(self, selectors, product_info, target_rating):\n",
    "        \"\"\"Extrait les reviews de la page actuelle\"\"\"\n",
    "        reviews = []\n",
    "        \n",
    "        try:\n",
    "            # R√©cup√©rer tous les conteneurs de reviews\n",
    "            review_containers = self.driver.find_elements(\n",
    "                By.CSS_SELECTOR, \n",
    "                selectors.get('reviews_container', '[data-hook=\"review\"]')\n",
    "            )\n",
    "            \n",
    "            for container in review_containers:\n",
    "                try:\n",
    "                    review_data = {\n",
    "                        'product_name': product_info['title'],\n",
    "                        'product_category': product_info['category'],\n",
    "                        'product_url': product_info['url'],\n",
    "                        'target_rating': target_rating\n",
    "                    }\n",
    "                    \n",
    "                    # Titre de la review\n",
    "                    try:\n",
    "                        title_elem = container.find_element(\n",
    "                            By.CSS_SELECTOR, \n",
    "                            selectors.get('review_title', '[data-hook=\"review-title\"]')\n",
    "                        )\n",
    "                        review_data['review_title'] = title_elem.text.strip()\n",
    "                    except:\n",
    "                        review_data['review_title'] = ''\n",
    "                    \n",
    "                    # Texte de la review\n",
    "                    try:\n",
    "                        text_elem = container.find_element(\n",
    "                            By.CSS_SELECTOR, \n",
    "                            selectors.get('review_text', '[data-hook=\"review-body\"]')\n",
    "                        )\n",
    "                        review_data['review_text'] = text_elem.text.strip()\n",
    "                    except:\n",
    "                        review_data['review_text'] = ''\n",
    "                    \n",
    "                    # Note de la review\n",
    "                    try:\n",
    "                        rating_elem = container.find_element(\n",
    "                            By.CSS_SELECTOR, \n",
    "                            selectors.get('review_rating', '.a-icon-alt')\n",
    "                        )\n",
    "                        rating_text = rating_elem.get_attribute('textContent') or rating_elem.text\n",
    "                        # Extraire le chiffre de la note\n",
    "                        import re\n",
    "                        rating_match = re.search(r'(\\d+(?:\\.\\d+)?)', rating_text)\n",
    "                        review_data['user_rating'] = rating_match.group(1) if rating_match else 'N/A'\n",
    "                    except:\n",
    "                        review_data['user_rating'] = 'N/A'\n",
    "                    \n",
    "                    # Nom du reviewer\n",
    "                    try:\n",
    "                        name_elem = container.find_element(\n",
    "                            By.CSS_SELECTOR, \n",
    "                            selectors.get('reviewer_name', '.a-profile-name')\n",
    "                        )\n",
    "                        review_data['reviewer_name'] = name_elem.text.strip()\n",
    "                    except:\n",
    "                        review_data['reviewer_name'] = 'Anonymous'\n",
    "                    \n",
    "                    # Date de la review\n",
    "                    try:\n",
    "                        date_elem = container.find_element(\n",
    "                            By.CSS_SELECTOR, \n",
    "                            selectors.get('review_date', '[data-hook=\"review-date\"]')\n",
    "                        )\n",
    "                        review_data['review_date'] = date_elem.text.strip()\n",
    "                    except:\n",
    "                        review_data['review_date'] = 'N/A'\n",
    "                    \n",
    "                    # Timestamp de scraping\n",
    "                    review_data['scraped_at'] = datetime.now().isoformat()\n",
    "                    \n",
    "                    # Ajouter seulement si on a du contenu\n",
    "                    if review_data['review_text'] or review_data['review_title']:\n",
    "                        reviews.append(review_data)\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    continue  # Ignorer les reviews probl√©matiques\n",
    "            \n",
    "            return reviews\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur extraction reviews: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def _go_to_next_page(self, selectors):\n",
    "        \"\"\"Passe √† la page suivante des reviews\"\"\"\n",
    "        try:\n",
    "            next_button = self.driver.find_element(\n",
    "                By.CSS_SELECTOR, \n",
    "                selectors.get('next_page', '.a-pagination .a-last a')\n",
    "            )\n",
    "            next_button.click()\n",
    "            time.sleep(2)\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def _build_search_url(self, site_url, search_term):\n",
    "        \"\"\"Construit l'URL de recherche\"\"\"\n",
    "        if 'amazon' in site_url:\n",
    "            return f\"https://www.amazon.com/s?k={search_term.replace(' ', '+')}\"\n",
    "        elif 'ebay' in site_url:\n",
    "            return f\"https://www.ebay.com/sch/i.html?_nkw={search_term.replace(' ', '+')}\"\n",
    "        return site_url\n",
    "    \n",
    "    def _clean_review_data(self, df):\n",
    "        \"\"\"Nettoie et structure les donn√©es de reviews\"\"\"\n",
    "        try:\n",
    "            # Supprimer les doublons\n",
    "            df = df.drop_duplicates(subset=['review_text', 'product_name'], keep='first')\n",
    "            \n",
    "            # Nettoyer les textes\n",
    "            df['review_text'] = df['review_text'].str.strip()\n",
    "            df['review_title'] = df['review_title'].str.strip()\n",
    "            \n",
    "            # Convertir les ratings en num√©rique\n",
    "            df['user_rating'] = pd.to_numeric(df['user_rating'], errors='coerce')\n",
    "            \n",
    "            # Ajouter une colonne de longueur de texte\n",
    "            df['review_length'] = df['review_text'].str.len()\n",
    "            \n",
    "            # Filtrer les reviews trop courtes\n",
    "            df = df[df['review_length'] > 10]\n",
    "            \n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur nettoyage donn√©es: {e}\")\n",
    "            return df\n",
    "    \n",
    "    def save_reviews(self, df, filename=None):\n",
    "        \"\"\"Sauvegarde les reviews dans un fichier CSV\"\"\"\n",
    "        try:\n",
    "            if filename is None:\n",
    "                timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "                filename = f\"../data/raw/product_reviews_{timestamp}.csv\"\n",
    "            \n",
    "            os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "            df.to_csv(filename, index=False, encoding='utf-8')\n",
    "            print(f\"‚úÖ Reviews sauvegard√©es: {filename}\")\n",
    "            return filename\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur sauvegarde: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Ferme le driver\"\"\"\n",
    "        if self.driver:\n",
    "            self.driver.quit()\n",
    "\n",
    "print(\"‚úÖ Classe ProductReviewScraper cr√©√©e\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c106055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Workflow de reviews de produits cr√©√©\n",
      "üìñ Utilisez reviews_workflow_menu() pour commencer\n"
     ]
    }
   ],
   "source": [
    "def product_reviews_workflow(category_search=\"laptop\", site=\"amazon\", max_products=10, reviews_per_rating=50):\n",
    "    \"\"\"\n",
    "    Workflow complet pour r√©cup√©rer les reviews de produits d'une cat√©gorie\n",
    "    \n",
    "    Phase 1: D√©tection automatique des balises\n",
    "    Phase 2: Scraping des reviews avec balises valid√©es\n",
    "    \n",
    "    Args:\n",
    "        category_search: cat√©gorie de produits √† rechercher\n",
    "        site: site √† scraper ('amazon' ou 'ebay')\n",
    "        max_products: nombre de produits √† analyser (d√©faut: 10)\n",
    "        reviews_per_rating: nombre de reviews par note 1-5 √©toiles (d√©faut: 50)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"üéØ WORKFLOW SP√âCIALIS√â - REVIEWS DE PRODUITS\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"üì¶ Cat√©gorie: {category_search}\")\n",
    "    print(f\"üåê Site: {site}\")\n",
    "    print(f\"üìä Produits: {max_products}\")\n",
    "    print(f\"‚≠ê Reviews par note: {reviews_per_rating}\")\n",
    "    print(f\"üìà Total estim√©: {max_products * 5 * reviews_per_rating} reviews max\")\n",
    "    print()\n",
    "    \n",
    "    # URLs des sites\n",
    "    site_urls = {\n",
    "        'amazon': 'https://www.amazon.com',\n",
    "        'ebay': 'https://www.ebay.com'\n",
    "    }\n",
    "    \n",
    "    if site not in site_urls:\n",
    "        print(f\"‚ùå Site non support√©: {site}\")\n",
    "        return None\n",
    "    \n",
    "    site_url = site_urls[site]\n",
    "    \n",
    "    # ============================================================================\n",
    "    # PHASE 1: D√âTECTION DES BALISES\n",
    "    # ============================================================================\n",
    "    print(\"üîç PHASE 1: D√âTECTION AUTOMATIQUE DES BALISES\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    scout = ProductReviewScout()\n",
    "    \n",
    "    try:\n",
    "        # Setup du driver pour la d√©tection\n",
    "        if not scout.setup_driver(headless=True):\n",
    "            print(\"‚ùå √âchec initialisation driver de d√©tection\")\n",
    "            return None\n",
    "        \n",
    "        print(\"‚úÖ Driver de d√©tection initialis√©\")\n",
    "        \n",
    "        # D√©tection des s√©lecteurs de produits\n",
    "        print(f\"üîç D√©tection des s√©lecteurs de produits sur {site}...\")\n",
    "        product_selectors = scout.detect_product_selectors(site_url, category_search)\n",
    "        \n",
    "        if not product_selectors:\n",
    "            print(\"‚ùå √âchec d√©tection s√©lecteurs produits\")\n",
    "            scout.close()\n",
    "            return None\n",
    "        \n",
    "        print(\"‚úÖ S√©lecteurs produits d√©tect√©s:\")\n",
    "        for key, value in product_selectors.items():\n",
    "            print(f\"   ‚Ä¢ {key}: {value}\")\n",
    "        \n",
    "        # Test sur un produit pour d√©tecter les s√©lecteurs de reviews\n",
    "        print(\"üîç Test d√©tection s√©lecteurs de reviews...\")\n",
    "        \n",
    "        # Simuler la r√©cup√©ration d'un produit test\n",
    "        scout.driver.get(scout._build_search_url(site_url, category_search))\n",
    "        time.sleep(3)\n",
    "        \n",
    "        test_product_url = None\n",
    "        try:\n",
    "            # Trouver le premier produit\n",
    "            product_links = scout.driver.find_elements(By.CSS_SELECTOR, 'h2 a, .s-item__link')\n",
    "            if product_links:\n",
    "                test_product_url = product_links[0].get_attribute('href')\n",
    "                print(f\"üì¶ Produit test: {test_product_url[:80]}...\")\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        review_selectors = None\n",
    "        if test_product_url:\n",
    "            review_selectors = scout.detect_review_selectors(test_product_url)\n",
    "        \n",
    "        if review_selectors:\n",
    "            print(\"‚úÖ S√©lecteurs reviews d√©tect√©s:\")\n",
    "            for key, value in review_selectors.items():\n",
    "                print(f\"   ‚Ä¢ {key}: {value}\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è S√©lecteurs reviews non d√©tect√©s, utilisation des s√©lecteurs par d√©faut\")\n",
    "            review_selectors = scout.selectors.get(site, {}).get('reviews', {})\n",
    "        \n",
    "        # Sauvegarder les s√©lecteurs valid√©s\n",
    "        validated_selectors = {\n",
    "            site: {\n",
    "                'products': product_selectors,\n",
    "                'reviews': review_selectors\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        selectors_file = f\"../config/product_review_selectors_{site}.json\"\n",
    "        if scout.save_selectors(validated_selectors, selectors_file):\n",
    "            print(f\"‚úÖ S√©lecteurs sauvegard√©s: {selectors_file}\")\n",
    "        \n",
    "        scout.close()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur phase d√©tection: {e}\")\n",
    "        scout.close()\n",
    "        return None\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    \n",
    "    # ============================================================================\n",
    "    # PHASE 2: SCRAPING DES REVIEWS\n",
    "    # ============================================================================\n",
    "    print(\"üìä PHASE 2: SCRAPING DES REVIEWS\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    scraper = ProductReviewScraper(selectors_file)\n",
    "    \n",
    "    try:\n",
    "        # Setup du driver pour le scraping\n",
    "        if not scraper.setup_driver(headless=False):  # Visible pour monitoring\n",
    "            print(\"‚ùå √âchec initialisation driver de scraping\")\n",
    "            return None\n",
    "        \n",
    "        print(\"‚úÖ Driver de scraping initialis√©\")\n",
    "        print(f\"üé≠ User-Agent: {scraper.driver.execute_script('return navigator.userAgent;')[:80]}...\")\n",
    "        \n",
    "        # Avertissement utilisateur\n",
    "        print(\"\\n\" + \"üö® AVERTISSEMENT: Scraping en cours sur site r√©el!\")\n",
    "        print(\"‚è∞ Estimation dur√©e: {} minutes\".format(max_products * 5))  # ~5min par produit\n",
    "        print(\"üìù Respect des ToS et limitations de d√©bit\")\n",
    "        \n",
    "        input(\"Appuyer sur Entr√©e pour continuer ou Ctrl+C pour annuler...\")\n",
    "        \n",
    "        # Lancement du scraping\n",
    "        print(f\"\\nüöÄ D√©but du scraping pour '{category_search}'...\")\n",
    "        \n",
    "        df_reviews = scraper.scrape_category_product_reviews(\n",
    "            site_url=site_url,\n",
    "            category_search=category_search,\n",
    "            max_products=max_products,\n",
    "            reviews_per_rating=reviews_per_rating\n",
    "        )\n",
    "        \n",
    "        if df_reviews.empty:\n",
    "            print(\"‚ùå Aucune review r√©cup√©r√©e\")\n",
    "            scraper.close()\n",
    "            return None\n",
    "        \n",
    "        # Analyse des r√©sultats\n",
    "        print(\"\\n\" + \"üìä R√âSULTATS DU SCRAPING\")\n",
    "        print(\"-\" * 30)\n",
    "        print(f\"‚úÖ Total reviews: {len(df_reviews)}\")\n",
    "        print(f\"üì¶ Produits uniques: {df_reviews['product_name'].nunique()}\")\n",
    "        print(f\"‚≠ê Distribution des notes:\")\n",
    "        \n",
    "        rating_dist = df_reviews['user_rating'].value_counts().sort_index()\n",
    "        for rating, count in rating_dist.items():\n",
    "            print(f\"   {rating} √©toiles: {count} reviews\")\n",
    "        \n",
    "        print(f\"üìù Longueur moyenne: {df_reviews['review_length'].mean():.0f} caract√®res\")\n",
    "        \n",
    "        # Sauvegarde\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        output_file = f\"../data/raw/{site}_{category_search}_reviews_{timestamp}.csv\"\n",
    "        \n",
    "        saved_file = scraper.save_reviews(df_reviews, output_file)\n",
    "        \n",
    "        if saved_file:\n",
    "            print(f\"‚úÖ Donn√©es sauvegard√©es: {saved_file}\")\n",
    "            \n",
    "            # Aper√ßu des donn√©es\n",
    "            print(\"\\nüìã APER√áU DES DONN√âES:\")\n",
    "            print(df_reviews[['product_name', 'user_rating', 'review_text']].head(3).to_string())\n",
    "            \n",
    "        scraper.close()\n",
    "        \n",
    "        print(\"\\n\" + \"üéâ WORKFLOW TERMIN√â AVEC SUCC√àS!\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        return df_reviews\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n‚èπÔ∏è Arr√™t demand√© par l'utilisateur\")\n",
    "        scraper.close()\n",
    "        return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur phase scraping: {e}\")\n",
    "        scraper.close()\n",
    "        return None\n",
    "\n",
    "def quick_review_test(product_url, max_reviews=20):\n",
    "    \"\"\"\n",
    "    Test rapide pour scraper les reviews d'un produit sp√©cifique\n",
    "    \"\"\"\n",
    "    print(f\"üß™ TEST RAPIDE - Reviews d'un produit\")\n",
    "    print(f\"üîó URL: {product_url}\")\n",
    "    print(f\"üìä Reviews max: {max_reviews}\")\n",
    "    \n",
    "    scraper = ProductReviewScraper()\n",
    "    \n",
    "    try:\n",
    "        if not scraper.setup_driver(headless=False):\n",
    "            print(\"‚ùå √âchec setup driver\")\n",
    "            return None\n",
    "        \n",
    "        # Simuler un produit\n",
    "        fake_product = {\n",
    "            'title': 'Produit Test',\n",
    "            'url': product_url,\n",
    "            'category': 'test'\n",
    "        }\n",
    "        \n",
    "        # Scraper les reviews\n",
    "        reviews = scraper._scrape_product_reviews(fake_product, max_reviews)\n",
    "        \n",
    "        if reviews:\n",
    "            df = pd.DataFrame(reviews)\n",
    "            print(f\"‚úÖ {len(reviews)} reviews r√©cup√©r√©es\")\n",
    "            print(\"\\nüìã Aper√ßu:\")\n",
    "            for i, review in enumerate(reviews[:3], 1):\n",
    "                print(f\"\\nReview {i}:\")\n",
    "                print(f\"  Note: {review.get('user_rating', 'N/A')}\")\n",
    "                print(f\"  Titre: {review.get('review_title', 'N/A')[:50]}...\")\n",
    "                print(f\"  Texte: {review.get('review_text', 'N/A')[:100]}...\")\n",
    "            \n",
    "            return df\n",
    "        else:\n",
    "            print(\"‚ùå Aucune review trouv√©e\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur test: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        scraper.close()\n",
    "\n",
    "def reviews_workflow_menu():\n",
    "    \"\"\"Menu principal pour le workflow de reviews\"\"\"\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"üéØ WORKFLOW REVIEWS DE PRODUITS - MENU PRINCIPAL\")\n",
    "    print(\"=\"*80)\n",
    "    print()\n",
    "    print(\"1Ô∏è‚É£ Workflow complet (d√©tection + scraping)\")\n",
    "    print(\"2Ô∏è‚É£ Test rapide sur un produit\")\n",
    "    print(\"3Ô∏è‚É£ Configuration personnalis√©e\")\n",
    "    print(\"4Ô∏è‚É£ Voir les s√©lecteurs sauvegard√©s\")\n",
    "    print(\"5Ô∏è‚É£ Quitter\")\n",
    "    print()\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            choice = input(\"üëâ Votre choix (1-5): \").strip()\n",
    "            \n",
    "            if choice == '1':\n",
    "                # Workflow complet\n",
    "                print(\"\\nüìã Configuration du workflow complet:\")\n",
    "                category = input(\"üè∑Ô∏è Cat√©gorie de produits (ex: 'laptop', 'smartphone'): \").strip() or \"laptop\"\n",
    "                site = input(\"üåê Site (amazon/ebay): \").strip() or \"amazon\"\n",
    "                \n",
    "                try:\n",
    "                    max_products = int(input(\"üì¶ Nombre de produits (d√©faut: 10): \") or \"10\")\n",
    "                    reviews_per_rating = int(input(\"‚≠ê Reviews par note (d√©faut: 50): \") or \"50\")\n",
    "                except ValueError:\n",
    "                    max_products, reviews_per_rating = 10, 50\n",
    "                \n",
    "                return product_reviews_workflow(category, site, max_products, reviews_per_rating)\n",
    "                \n",
    "            elif choice == '2':\n",
    "                # Test rapide\n",
    "                product_url = input(\"üîó URL du produit √† tester: \").strip()\n",
    "                if product_url:\n",
    "                    try:\n",
    "                        max_reviews = int(input(\"üìä Nombre max de reviews (d√©faut: 20): \") or \"20\")\n",
    "                    except ValueError:\n",
    "                        max_reviews = 20\n",
    "                    return quick_review_test(product_url, max_reviews)\n",
    "                else:\n",
    "                    print(\"‚ùå URL requise\")\n",
    "                    \n",
    "            elif choice == '3':\n",
    "                # Configuration avanc√©e\n",
    "                print(\"\\n‚öôÔ∏è Configuration personnalis√©e disponible dans product_reviews_workflow()\")\n",
    "                print(\"üìñ Consultez la documentation de la fonction\")\n",
    "                \n",
    "            elif choice == '4':\n",
    "                # Voir s√©lecteurs\n",
    "                print(\"\\nüìã S√©lecteurs sauvegard√©s:\")\n",
    "                for filename in ['../config/product_review_selectors_amazon.json', '../config/product_review_selectors_ebay.json']:\n",
    "                    if os.path.exists(filename):\n",
    "                        print(f\"‚úÖ {filename}\")\n",
    "                        try:\n",
    "                            with open(filename, 'r') as f:\n",
    "                                data = json.load(f)\n",
    "                                print(f\"   Sites: {list(data.keys())}\")\n",
    "                        except:\n",
    "                            pass\n",
    "                    else:\n",
    "                        print(f\"‚ùå {filename} (non trouv√©)\")\n",
    "                        \n",
    "            elif choice == '5':\n",
    "                print(\"üëã √Ä bient√¥t!\")\n",
    "                break\n",
    "                \n",
    "            else:\n",
    "                print(\"‚ùå Choix invalide, veuillez r√©essayer\")\n",
    "                \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nüëã √Ä bient√¥t!\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur: {e}\")\n",
    "\n",
    "print(\"‚úÖ Workflow de reviews de produits cr√©√©\")\n",
    "print(\"üìñ Utilisez reviews_workflow_menu() pour commencer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4218b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ WORKFLOW REVIEWS DE PRODUITS - PR√äT √Ä UTILISER\n",
      "================================================================================\n",
      "\n",
      "üìã EXEMPLES D'UTILISATION:\n",
      "\n",
      "1Ô∏è‚É£ Workflow complet automatique:\n",
      "   df = product_reviews_workflow('laptop', 'amazon', 10, 50)\n",
      "   # R√©cup√®re 50 reviews par note (1-5) pour 10 laptops sur Amazon\n",
      "\n",
      "2Ô∏è‚É£ Menu interactif:\n",
      "   reviews_workflow_menu()\n",
      "   # Interface guid√©e pour configurer le scraping\n",
      "\n",
      "3Ô∏è‚É£ Test rapide d'un produit:\n",
      "   df = quick_review_test('https://amazon.com/dp/XXXXXXXXXX', 20)\n",
      "   # Test sur un produit sp√©cifique\n",
      "\n",
      "4Ô∏è‚É£ Configurations personnalis√©es:\n",
      "   # Smartphones sur eBay\n",
      "   df = product_reviews_workflow('smartphone', 'ebay', 5, 30)\n",
      "\n",
      "   # Casques audio sur Amazon\n",
      "   df = product_reviews_workflow('headphones', 'amazon', 15, 40)\n",
      "\n",
      "üìä DONN√âES R√âCUP√âR√âES:\n",
      "   ‚Ä¢ product_name: nom du produit\n",
      "   ‚Ä¢ product_category: cat√©gorie recherch√©e\n",
      "   ‚Ä¢ review_title: titre de la review\n",
      "   ‚Ä¢ review_text: texte complet de la review\n",
      "   ‚Ä¢ user_rating: note donn√©e (1-5)\n",
      "   ‚Ä¢ reviewer_name: nom du reviewer\n",
      "   ‚Ä¢ review_date: date de la review\n",
      "   ‚Ä¢ scraped_at: timestamp du scraping\n",
      "\n",
      "üíæ SAUVEGARDE AUTOMATIQUE:\n",
      "   ‚Ä¢ Format CSV dans ../data/raw/\n",
      "   ‚Ä¢ Nom: {site}_{categorie}_reviews_{timestamp}.csv\n",
      "\n",
      "üõ°Ô∏è S√âCURIT√â:\n",
      "   ‚Ä¢ Anti-d√©tection avec user-agents al√©atoires\n",
      "   ‚Ä¢ D√©lais humains entre requ√™tes\n",
      "   ‚Ä¢ Respect des limitations de d√©bit\n",
      "   ‚Ä¢ Options Chrome optimis√©es\n",
      "\n",
      "‚ö†Ô∏è IMPORTANT:\n",
      "   ‚Ä¢ Respecter les ToS des sites\n",
      "   ‚Ä¢ Utiliser avec mod√©ration\n",
      "   ‚Ä¢ V√©rifier robots.txt\n",
      "   ‚Ä¢ Ne pas surcharger les serveurs\n",
      "\n",
      "üöÄ Pour commencer, utilisez:\n",
      "   reviews_workflow_menu()\n",
      "\n",
      "‚úÖ Workflow pr√™t! Tapez reviews_workflow_menu() pour commencer\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# EXEMPLES D'UTILISATION DU WORKFLOW REVIEWS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"üéØ WORKFLOW REVIEWS DE PRODUITS - PR√äT √Ä UTILISER\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "print(\"üìã EXEMPLES D'UTILISATION:\")\n",
    "print()\n",
    "print(\"1Ô∏è‚É£ Workflow complet automatique:\")\n",
    "print(\"   df = product_reviews_workflow('laptop', 'amazon', 10, 50)\")\n",
    "print(\"   # R√©cup√®re 50 reviews par note (1-5) pour 10 laptops sur Amazon\")\n",
    "print()\n",
    "print(\"2Ô∏è‚É£ Menu interactif:\")\n",
    "print(\"   reviews_workflow_menu()\")\n",
    "print(\"   # Interface guid√©e pour configurer le scraping\")\n",
    "print()\n",
    "print(\"3Ô∏è‚É£ Test rapide d'un produit:\")\n",
    "print(\"   df = quick_review_test('https://amazon.com/dp/XXXXXXXXXX', 20)\")\n",
    "print(\"   # Test sur un produit sp√©cifique\")\n",
    "print()\n",
    "print(\"4Ô∏è‚É£ Configurations personnalis√©es:\")\n",
    "print(\"   # Smartphones sur eBay\")\n",
    "print(\"   df = product_reviews_workflow('smartphone', 'ebay', 5, 30)\")\n",
    "print()\n",
    "print(\"   # Casques audio sur Amazon\")\n",
    "print(\"   df = product_reviews_workflow('headphones', 'amazon', 15, 40)\")\n",
    "print()\n",
    "print(\"üìä DONN√âES R√âCUP√âR√âES:\")\n",
    "print(\"   ‚Ä¢ product_name: nom du produit\")\n",
    "print(\"   ‚Ä¢ product_category: cat√©gorie recherch√©e\")\n",
    "print(\"   ‚Ä¢ review_title: titre de la review\")\n",
    "print(\"   ‚Ä¢ review_text: texte complet de la review\")\n",
    "print(\"   ‚Ä¢ user_rating: note donn√©e (1-5)\")\n",
    "print(\"   ‚Ä¢ reviewer_name: nom du reviewer\")\n",
    "print(\"   ‚Ä¢ review_date: date de la review\")\n",
    "print(\"   ‚Ä¢ scraped_at: timestamp du scraping\")\n",
    "print()\n",
    "print(\"üíæ SAUVEGARDE AUTOMATIQUE:\")\n",
    "print(\"   ‚Ä¢ Format CSV dans ../data/raw/\")\n",
    "print(\"   ‚Ä¢ Nom: {site}_{categorie}_reviews_{timestamp}.csv\")\n",
    "print()\n",
    "print(\"üõ°Ô∏è S√âCURIT√â:\")\n",
    "print(\"   ‚Ä¢ Anti-d√©tection avec user-agents al√©atoires\")\n",
    "print(\"   ‚Ä¢ D√©lais humains entre requ√™tes\")\n",
    "print(\"   ‚Ä¢ Respect des limitations de d√©bit\")\n",
    "print(\"   ‚Ä¢ Options Chrome optimis√©es\")\n",
    "print()\n",
    "print(\"‚ö†Ô∏è IMPORTANT:\")\n",
    "print(\"   ‚Ä¢ Respecter les ToS des sites\")\n",
    "print(\"   ‚Ä¢ Utiliser avec mod√©ration\")\n",
    "print(\"   ‚Ä¢ V√©rifier robots.txt\")\n",
    "print(\"   ‚Ä¢ Ne pas surcharger les serveurs\")\n",
    "print()\n",
    "print(\"üöÄ Pour commencer, utilisez:\")\n",
    "print(\"   reviews_workflow_menu()\")\n",
    "\n",
    "# Exemple de configuration pr√™te √† l'emploi\n",
    "EXAMPLE_CONFIGS = {\n",
    "    'laptops_amazon': {\n",
    "        'category_search': 'laptop gaming',\n",
    "        'site': 'amazon',\n",
    "        'max_products': 8,\n",
    "        'reviews_per_rating': 40,\n",
    "        'description': 'Reviews de laptops gaming sur Amazon'\n",
    "    },\n",
    "    'smartphones_ebay': {\n",
    "        'category_search': 'smartphone iphone',\n",
    "        'site': 'ebay', \n",
    "        'max_products': 5,\n",
    "        'reviews_per_rating': 30,\n",
    "        'description': 'Reviews d\\'iPhones sur eBay'\n",
    "    },\n",
    "    'headphones_amazon': {\n",
    "        'category_search': 'wireless headphones',\n",
    "        'site': 'amazon',\n",
    "        'max_products': 12,\n",
    "        'reviews_per_rating': 35,\n",
    "        'description': 'Reviews de casques sans-fil sur Amazon'\n",
    "    }\n",
    "}\n",
    "\n",
    "def run_example_config(config_name):\n",
    "    \"\"\"Ex√©cute une configuration d'exemple\"\"\"\n",
    "    if config_name in EXAMPLE_CONFIGS:\n",
    "        config = EXAMPLE_CONFIGS[config_name]\n",
    "        print(f\"üöÄ Lancement: {config['description']}\")\n",
    "        return product_reviews_workflow(**{k:v for k,v in config.items() if k != 'description'})\n",
    "    else:\n",
    "        print(f\"‚ùå Configuration '{config_name}' non trouv√©e\")\n",
    "        print(f\"üìã Disponibles: {list(EXAMPLE_CONFIGS.keys())}\")\n",
    "        return None\n",
    "\n",
    "print(\"\\n‚úÖ Workflow pr√™t! Tapez reviews_workflow_menu() pour commencer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6933b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üéØ WORKFLOW REVIEWS DE PRODUITS - MENU PRINCIPAL\n",
      "================================================================================\n",
      "\n",
      "1Ô∏è‚É£ Workflow complet (d√©tection + scraping)\n",
      "2Ô∏è‚É£ Test rapide sur un produit\n",
      "3Ô∏è‚É£ Configuration personnalis√©e\n",
      "4Ô∏è‚É£ Voir les s√©lecteurs sauvegard√©s\n",
      "5Ô∏è‚É£ Quitter\n",
      "\n",
      "\n",
      "üìã Configuration du workflow complet:\n",
      "\n",
      "üìã Configuration du workflow complet:\n",
      "================================================================================\n",
      "üéØ WORKFLOW SP√âCIALIS√â - REVIEWS DE PRODUITS\n",
      "================================================================================\n",
      "üì¶ Cat√©gorie: laptop\n",
      "üåê Site: amazon\n",
      "üìä Produits: 2\n",
      "‚≠ê Reviews par note: 10\n",
      "üìà Total estim√©: 100 reviews max\n",
      "\n",
      "üîç PHASE 1: D√âTECTION AUTOMATIQUE DES BALISES\n",
      "--------------------------------------------------\n",
      "================================================================================\n",
      "üéØ WORKFLOW SP√âCIALIS√â - REVIEWS DE PRODUITS\n",
      "================================================================================\n",
      "üì¶ Cat√©gorie: laptop\n",
      "üåê Site: amazon\n",
      "üìä Produits: 2\n",
      "‚≠ê Reviews par note: 10\n",
      "üìà Total estim√©: 100 reviews max\n",
      "\n",
      "üîç PHASE 1: D√âTECTION AUTOMATIQUE DES BALISES\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 16:24:40,828 - INFO - patching driver executable C:\\Users\\Yann\\appdata\\roaming\\undetected_chromedriver\\undetected_chromedriver.exe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Driver de d√©tection initialis√©\n",
      "üîç D√©tection des s√©lecteurs de produits sur amazon...\n",
      "‚úÖ S√©lecteurs produits d√©tect√©s:\n",
      "   ‚Ä¢ product_container: [data-component-type=\"s-search-result\"]\n",
      "   ‚Ä¢ product_title: h2 span\n",
      "   ‚Ä¢ product_url: h2 a, .a-link-normal\n",
      "   ‚Ä¢ product_price: .a-price .a-offscreen, .a-price-whole\n",
      "   ‚Ä¢ product_rating: .a-icon-alt\n",
      "üîç Test d√©tection s√©lecteurs de reviews...\n",
      "‚úÖ S√©lecteurs produits d√©tect√©s:\n",
      "   ‚Ä¢ product_container: [data-component-type=\"s-search-result\"]\n",
      "   ‚Ä¢ product_title: h2 span\n",
      "   ‚Ä¢ product_url: h2 a, .a-link-normal\n",
      "   ‚Ä¢ product_price: .a-price .a-offscreen, .a-price-whole\n",
      "   ‚Ä¢ product_rating: .a-icon-alt\n",
      "üîç Test d√©tection s√©lecteurs de reviews...\n",
      "‚ö†Ô∏è S√©lecteurs reviews non d√©tect√©s, utilisation des s√©lecteurs par d√©faut\n",
      "‚úÖ S√©lecteurs sauvegard√©s: ../config/product_review_selectors_amazon.json\n",
      "‚úÖ S√©lecteurs sauvegard√©s: ../config/product_review_selectors_amazon.json\n",
      "‚ö†Ô∏è S√©lecteurs reviews non d√©tect√©s, utilisation des s√©lecteurs par d√©faut\n",
      "‚úÖ S√©lecteurs sauvegard√©s: ../config/product_review_selectors_amazon.json\n",
      "‚úÖ S√©lecteurs sauvegard√©s: ../config/product_review_selectors_amazon.json\n",
      "\n",
      "==================================================\n",
      "üìä PHASE 2: SCRAPING DES REVIEWS\n",
      "--------------------------------------------------\n",
      "\n",
      "==================================================\n",
      "üìä PHASE 2: SCRAPING DES REVIEWS\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 16:24:55,093 - INFO - patching driver executable C:\\Users\\Yann\\appdata\\roaming\\undetected_chromedriver\\undetected_chromedriver.exe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Erreur setup driver: Message: invalid argument: cannot parse capability: goog:chromeOptions\n",
      "from invalid argument: unrecognized chrome option: useAutomationExtension\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x0xf54493+62419]\n",
      "\tGetHandleVerifier [0x0xf544d4+62484]\n",
      "\t(No symbol) [0x0xd92133]\n",
      "\t(No symbol) [0x0xdb9723]\n",
      "\t(No symbol) [0x0xdbaeb0]\n",
      "\t(No symbol) [0x0xdb5fea]\n",
      "\t(No symbol) [0x0xe09832]\n",
      "\t(No symbol) [0x0xe0931c]\n",
      "\t(No symbol) [0x0xe0aa20]\n",
      "\t(No symbol) [0x0xe0a82a]\n",
      "\t(No symbol) [0x0xdff266]\n",
      "\t(No symbol) [0x0xdce852]\n",
      "\t(No symbol) [0x0xdcf6f4]\n",
      "\tGetHandleVerifier [0x0x11c4773+2619059]\n",
      "\tGetHandleVerifier [0x0x11bfb8a+2599626]\n",
      "\tGetHandleVerifier [0x0xf7b03a+221050]\n",
      "\tGetHandleVerifier [0x0xf6b2b8+156152]\n",
      "\tGetHandleVerifier [0x0xf71c6d+183213]\n",
      "\tGetHandleVerifier [0x0xf5c378+94904]\n",
      "\tGetHandleVerifier [0x0xf5c502+95298]\n",
      "\tGetHandleVerifier [0x0xf4765a+9626]\n",
      "\tBaseThreadInitThunk [0x0x76775d49+25]\n",
      "\tRtlInitializeExceptionChain [0x0x778ed09b+107]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x0x778ed021+561]\n",
      "\n",
      "‚ùå √âchec initialisation driver de scraping\n"
     ]
    }
   ],
   "source": [
    "reviews_workflow_menu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66698f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CORRECTION ROBUSTE DES OPTIONS CHROME\n",
    "# ============================================================================\n",
    "\n",
    "def create_robust_chrome_options(headless=True):\n",
    "    \"\"\"\n",
    "    Cr√©e des options Chrome 100% compatibles avec toutes les versions\n",
    "    √âvite toutes les options probl√©matiques\n",
    "    \"\"\"\n",
    "    import undetected_chromedriver as uc\n",
    "    \n",
    "    try:\n",
    "        options = uc.ChromeOptions()\n",
    "        \n",
    "        # Arguments de base s√ªrs et test√©s\n",
    "        safe_args = [\n",
    "            '--no-sandbox',\n",
    "            '--disable-dev-shm-usage',\n",
    "            '--disable-gpu',\n",
    "            '--disable-web-security',\n",
    "            '--disable-features=VizDisplayCompositor',\n",
    "            '--disable-extensions',\n",
    "            '--disable-plugins',\n",
    "            '--disable-default-apps',\n",
    "            '--disable-background-timer-throttling',\n",
    "            '--disable-backgrounding-occluded-windows',\n",
    "            '--disable-renderer-backgrounding',\n",
    "            '--disable-field-trial-config',\n",
    "            '--disable-back-forward-cache',\n",
    "            '--disable-ipc-flooding-protection',\n",
    "            '--window-size=1920,1080',\n",
    "            '--remote-debugging-port=9222'\n",
    "        ]\n",
    "        \n",
    "        # Ajouter les arguments s√ªrs\n",
    "        for arg in safe_args:\n",
    "            options.add_argument(arg)\n",
    "        \n",
    "        # Mode headless si demand√©\n",
    "        if headless:\n",
    "            options.add_argument('--headless=new')  # Nouveau mode headless\n",
    "        \n",
    "        # User agent al√©atoire\n",
    "        try:\n",
    "            user_agent = random.choice(REALISTIC_USER_AGENTS)\n",
    "            options.add_argument(f'--user-agent={user_agent}')\n",
    "        except:\n",
    "            # Fallback si REALISTIC_USER_AGENTS n'existe pas\n",
    "            options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')\n",
    "        \n",
    "        # Pr√©f√©rences s√ªres SEULEMENT\n",
    "        prefs = {\n",
    "            \"profile.default_content_setting_values.notifications\": 2,\n",
    "            \"profile.default_content_settings.popups\": 0,\n",
    "            \"profile.managed_default_content_settings.images\": 2,\n",
    "            \"profile.default_content_setting_values.media_stream_mic\": 2,\n",
    "            \"profile.default_content_setting_values.media_stream_camera\": 2,\n",
    "            \"profile.default_content_setting_values.geolocation\": 2\n",
    "        }\n",
    "        \n",
    "        options.add_experimental_option(\"prefs\", prefs)\n",
    "        \n",
    "        # NE PAS AJOUTER: excludeSwitches, useAutomationExtension\n",
    "        # Ces options causent des erreurs dans les nouvelles versions\n",
    "        \n",
    "        return options\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur cr√©ation options Chrome: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_robust_driver(headless=True, max_retries=3):\n",
    "    \"\"\"\n",
    "    Cr√©e un driver robuste avec plusieurs tentatives et fallbacks\n",
    "    \"\"\"\n",
    "    import undetected_chromedriver as uc\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            print(f\"üîß Tentative {attempt + 1}/{max_retries} - Cr√©ation driver...\")\n",
    "            \n",
    "            # Options robustes\n",
    "            options = create_robust_chrome_options(headless)\n",
    "            if not options:\n",
    "                continue\n",
    "            \n",
    "            # Cr√©ation du driver avec param√®tres optimaux\n",
    "            driver = uc.Chrome(\n",
    "                options=options,\n",
    "                version_main=None,  # Auto-d√©tection\n",
    "                headless=headless,\n",
    "                use_subprocess=False,\n",
    "                log_level=3  # R√©duire les logs\n",
    "            )\n",
    "            \n",
    "            # Test rapide\n",
    "            driver.get(\"data:text/html,<html><body><h1>Test</h1></body></html>\")\n",
    "            \n",
    "            print(\"‚úÖ Driver robuste cr√©√© avec succ√®s!\")\n",
    "            return driver\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Tentative {attempt + 1} √©chou√©e: {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                print(\"üîÑ Nouvelle tentative...\")\n",
    "                time.sleep(2)\n",
    "            else:\n",
    "                print(\"‚ùå Toutes les tentatives ont √©chou√©\")\n",
    "                return create_selenium_fallback_driver()\n",
    "    \n",
    "    return None\n",
    "\n",
    "def create_selenium_fallback_driver():\n",
    "    \"\"\"\n",
    "    Driver de secours avec Selenium classique\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"üîÑ Fallback vers Selenium classique...\")\n",
    "        \n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.chrome.service import Service\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        from webdriver_manager.chrome import ChromeDriverManager\n",
    "        \n",
    "        # Options Selenium classiques\n",
    "        options = Options()\n",
    "        options.add_argument('--no-sandbox')\n",
    "        options.add_argument('--disable-dev-shm-usage')\n",
    "        options.add_argument('--disable-gpu')\n",
    "        options.add_argument('--window-size=1920,1080')\n",
    "        \n",
    "        # Service g√©r√© automatiquement\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        \n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        \n",
    "        print(\"‚úÖ Driver Selenium classique cr√©√©!\")\n",
    "        return driver\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Fallback Selenium √©chou√©: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test du syst√®me robuste\n",
    "print(\"üß™ Test du syst√®me de cr√©ation de driver robuste...\")\n",
    "\n",
    "test_driver = create_robust_driver(headless=True)\n",
    "if test_driver:\n",
    "    try:\n",
    "        test_driver.get(\"https://httpbin.org/user-agent\")\n",
    "        print(\"‚úÖ Navigation test r√©ussie!\")\n",
    "        test_driver.quit()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Test navigation: {e}\")\n",
    "        test_driver.quit()\n",
    "else:\n",
    "    print(\"‚ùå Impossible de cr√©er un driver robuste\")\n",
    "\n",
    "print(\"‚úÖ Syst√®me de driver robuste pr√™t!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scraper_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
