{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef35bbc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imports termin√©s - Syst√®me anti-d√©tection pr√™t ! üïµÔ∏è\n"
     ]
    }
   ],
   "source": [
    "import undetected_chromedriver as uc\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium_stealth import stealth\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from fake_useragent import UserAgent\n",
    "import logging\n",
    "from typing import List, Dict, Optional\n",
    "import os\n",
    "\n",
    "# Configuration du logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Configuration globale\n",
    "ua = UserAgent()\n",
    "\n",
    "# Liste de proxies gratuits (remplacer par des proxies premium en production)\n",
    "PROXY_LIST = [\n",
    "    # \"http://proxy1:port\",\n",
    "    # \"http://proxy2:port\", \n",
    "    # Ajoutez vos proxies ici\n",
    "]\n",
    "\n",
    "# User agents r√©alistes\n",
    "REALISTIC_USER_AGENTS = [\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15'\n",
    "]\n",
    "\n",
    "print(\"‚úÖ Imports termin√©s - Syst√®me anti-d√©tection pr√™t ! üïµÔ∏è\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a7dc9cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Scout robuste cr√©√©!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SCOUT ROBUSTE POUR D√âTECTION DE BALISES\n",
    "# ============================================================================\n",
    "\n",
    "class RobustProductReviewScout:\n",
    "    \"\"\"\n",
    "    Scout ultra-robuste pour d√©tecter automatiquement les balises de produits et reviews\n",
    "    Avec gestion d'erreurs compl√®te et fallbacks multiples\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.driver = None\n",
    "        self.detected_selectors = {}\n",
    "        self.base_selectors = {\n",
    "            'amazon': {\n",
    "                'products': {\n",
    "                    'containers': [\n",
    "                        '[data-component-type=\"s-search-result\"]',\n",
    "                        '.s-result-item',\n",
    "                        '.s-widget-container .s-card-container',\n",
    "                        '[data-asin]:not([data-asin=\"\"])'\n",
    "                    ],\n",
    "                    'titles': [\n",
    "                        'h2 span',\n",
    "                        'h2 a span',\n",
    "                        '.s-size-mini span',\n",
    "                        '.a-size-base-plus',\n",
    "                        '[data-cy=\"title-recipe-title\"]'\n",
    "                    ],\n",
    "                    'urls': [\n",
    "                        'h2 a',\n",
    "                        '.a-link-normal',\n",
    "                        'a[href*=\"/dp/\"]',\n",
    "                        'a[href*=\"/gp/product/\"]'\n",
    "                    ],\n",
    "                    'prices': [\n",
    "                        '.a-price .a-offscreen',\n",
    "                        '.a-price-whole',\n",
    "                        '.a-price-range .a-offscreen',\n",
    "                        '.a-price-symbol + .a-price-whole'\n",
    "                    ],\n",
    "                    'ratings': [\n",
    "                        '.a-icon-alt',\n",
    "                        '.a-star-mini .a-icon-alt',\n",
    "                        'span[aria-label*=\"stars\"]'\n",
    "                    ]\n",
    "                },\n",
    "                'reviews': {\n",
    "                    'containers': [\n",
    "                        '[data-hook=\"review\"]',\n",
    "                        '.review',\n",
    "                        '.cr-original-review-content',\n",
    "                        '.reviewText'\n",
    "                    ],\n",
    "                    'titles': [\n",
    "                        '[data-hook=\"review-title\"] span',\n",
    "                        '.review-title',\n",
    "                        '.cr-original-review-content .review-title'\n",
    "                    ],\n",
    "                    'texts': [\n",
    "                        '[data-hook=\"review-body\"] span',\n",
    "                        '.review-text',\n",
    "                        '.cr-original-review-content .review-text',\n",
    "                        '.reviewText'\n",
    "                    ],\n",
    "                    'ratings': [\n",
    "                        '[data-hook=\"review-star-rating\"] .a-icon-alt',\n",
    "                        '.review-rating .a-icon-alt',\n",
    "                        '.cr-original-review-content .a-icon-alt'\n",
    "                    ],\n",
    "                    'authors': [\n",
    "                        '.a-profile-name',\n",
    "                        '.review-author',\n",
    "                        '.cr-original-review-content .author'\n",
    "                    ],\n",
    "                    'dates': [\n",
    "                        '[data-hook=\"review-date\"]',\n",
    "                        '.review-date',\n",
    "                        '.cr-original-review-content .review-date'\n",
    "                    ]\n",
    "                }\n",
    "            },\n",
    "            'ebay': {\n",
    "                'products': {\n",
    "                    'containers': [\n",
    "                        '.s-item',\n",
    "                        '.srp-results .s-item',\n",
    "                        '.b-listing__wrap'\n",
    "                    ],\n",
    "                    'titles': [\n",
    "                        '.s-item__title',\n",
    "                        '.it-ttl',\n",
    "                        '.b-listing__title'\n",
    "                    ],\n",
    "                    'urls': [\n",
    "                        '.s-item__link',\n",
    "                        '.it-ttl a',\n",
    "                        '.b-listing__title a'\n",
    "                    ],\n",
    "                    'prices': [\n",
    "                        '.s-item__price',\n",
    "                        '.notranslate',\n",
    "                        '.b-listing__price'\n",
    "                    ]\n",
    "                },\n",
    "                'reviews': {\n",
    "                    'containers': [\n",
    "                        '.review-item',\n",
    "                        '.ebay-review',\n",
    "                        '.reviews .review'\n",
    "                    ],\n",
    "                    'texts': [\n",
    "                        '.review-item-content',\n",
    "                        '.ebay-review-text',\n",
    "                        '.review-content'\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def setup_robust_driver(self, headless=True, timeout=30):\n",
    "        \"\"\"Configuration driver ultra-robuste avec fallbacks\"\"\"\n",
    "        \n",
    "        max_attempts = 3\n",
    "        \n",
    "        for attempt in range(max_attempts):\n",
    "            try:\n",
    "                print(f\"üîß Tentative {attempt + 1}/{max_attempts} - Setup driver scout...\")\n",
    "                \n",
    "                # Fermer le driver existant si n√©cessaire\n",
    "                if self.driver:\n",
    "                    try:\n",
    "                        self.driver.quit()\n",
    "                    except:\n",
    "                        pass\n",
    "                    self.driver = None\n",
    "                \n",
    "                # Options ultra-s√ªres\n",
    "                options = self._create_safe_options(headless)\n",
    "                \n",
    "                # Cr√©ation driver avec timeout\n",
    "                import undetected_chromedriver as uc\n",
    "                \n",
    "                self.driver = uc.Chrome(\n",
    "                    options=options,\n",
    "                    version_main=None,\n",
    "                    headless=headless,\n",
    "                    use_subprocess=False,\n",
    "                    log_level=3\n",
    "                )\n",
    "                \n",
    "                # Configuration des timeouts\n",
    "                self.driver.set_page_load_timeout(timeout)\n",
    "                self.driver.implicitly_wait(10)\n",
    "                \n",
    "                # Test de fonctionnement\n",
    "                self.driver.get(\"data:text/html,<html><body><h1>Test Scout</h1></body></html>\")\n",
    "                \n",
    "                print(\"‚úÖ Driver scout initialis√© avec succ√®s!\")\n",
    "                return True\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Tentative {attempt + 1} √©chou√©e: {str(e)[:100]}...\")\n",
    "                if attempt < max_attempts - 1:\n",
    "                    time.sleep(3)\n",
    "                else:\n",
    "                    print(\"‚ùå Impossible de cr√©er le driver scout\")\n",
    "                    return False\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def _create_safe_options(self, headless=True):\n",
    "        \"\"\"Cr√©e des options Chrome ultra-s√ªres\"\"\"\n",
    "        \n",
    "        import undetected_chromedriver as uc\n",
    "        \n",
    "        options = uc.ChromeOptions()\n",
    "        \n",
    "        # Arguments de base seulement\n",
    "        safe_args = [\n",
    "            '--no-sandbox',\n",
    "            '--disable-dev-shm-usage',\n",
    "            '--disable-gpu',\n",
    "            '--disable-web-security',\n",
    "            '--disable-features=VizDisplayCompositor',\n",
    "            '--window-size=1920,1080',\n",
    "            '--start-maximized'\n",
    "        ]\n",
    "        \n",
    "        for arg in safe_args:\n",
    "            options.add_argument(arg)\n",
    "        \n",
    "        if headless:\n",
    "            options.add_argument('--headless=new')\n",
    "        \n",
    "        # User agent al√©atoire\n",
    "        try:\n",
    "            user_agent = random.choice(REALISTIC_USER_AGENTS)\n",
    "            options.add_argument(f'--user-agent={user_agent}')\n",
    "        except:\n",
    "            options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')\n",
    "        \n",
    "        # Pr√©f√©rences minimales\n",
    "        prefs = {\n",
    "            \"profile.default_content_setting_values.notifications\": 2,\n",
    "            \"profile.default_content_settings.popups\": 0\n",
    "        }\n",
    "        options.add_experimental_option(\"prefs\", prefs)\n",
    "        \n",
    "        return options\n",
    "    \n",
    "    def detect_site_selectors(self, site_url, search_term=\"laptop\", max_retries=3):\n",
    "        \"\"\"\n",
    "        D√©tecte automatiquement tous les s√©lecteurs pour un site\n",
    "        \"\"\"\n",
    "        \n",
    "        if not self.driver:\n",
    "            print(\"‚ùå Driver non initialis√©\")\n",
    "            return {}\n",
    "        \n",
    "        site_type = 'amazon' if 'amazon' in site_url else 'ebay' if 'ebay' in site_url else 'unknown'\n",
    "        \n",
    "        if site_type == 'unknown':\n",
    "            print(f\"‚ùå Site non support√©: {site_url}\")\n",
    "            return {}\n",
    "        \n",
    "        print(f\"üîç D√©tection des s√©lecteurs pour {site_type}...\")\n",
    "        \n",
    "        detected = {\n",
    "            'site': site_type,\n",
    "            'products': {},\n",
    "            'reviews': {}\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Phase 1: D√©tection s√©lecteurs produits\n",
    "            product_selectors = self._detect_product_selectors(site_url, search_term, site_type)\n",
    "            detected['products'] = product_selectors\n",
    "            \n",
    "            if product_selectors:\n",
    "                print(f\"‚úÖ S√©lecteurs produits d√©tect√©s: {len(product_selectors)}\")\n",
    "                \n",
    "                # Phase 2: D√©tection s√©lecteurs reviews\n",
    "                review_selectors = self._detect_review_selectors(site_type, product_selectors)\n",
    "                detected['reviews'] = review_selectors\n",
    "                \n",
    "                if review_selectors:\n",
    "                    print(f\"‚úÖ S√©lecteurs reviews d√©tect√©s: {len(review_selectors)}\")\n",
    "                else:\n",
    "                    print(\"‚ö†Ô∏è S√©lecteurs reviews non d√©tect√©s, utilisation des d√©fauts\")\n",
    "                    detected['reviews'] = self.base_selectors[site_type]['reviews']\n",
    "            else:\n",
    "                print(\"‚ùå Impossible de d√©tecter les s√©lecteurs produits\")\n",
    "                return {}\n",
    "            \n",
    "            # Sauvegarde des s√©lecteurs\n",
    "            self._save_detected_selectors(detected, site_type)\n",
    "            \n",
    "            return detected\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur d√©tection: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def _detect_product_selectors(self, site_url, search_term, site_type):\n",
    "        \"\"\"D√©tecte les s√©lecteurs de produits avec tests multiples\"\"\"\n",
    "        \n",
    "        try:\n",
    "            # Construire URL de recherche\n",
    "            if site_type == 'amazon':\n",
    "                search_url = f\"https://www.amazon.com/s?k={search_term.replace(' ', '+')}\"\n",
    "            elif site_type == 'ebay':\n",
    "                search_url = f\"https://www.ebay.com/sch/i.html?_nkw={search_term.replace(' ', '+')}\"\n",
    "            else:\n",
    "                return {}\n",
    "            \n",
    "            print(f\"üåê Navigation vers: {search_url}\")\n",
    "            self.driver.get(search_url)\n",
    "            \n",
    "            # Attendre le chargement\n",
    "            time.sleep(5)\n",
    "            \n",
    "            # Tester les s√©lecteurs de conteneurs\n",
    "            selectors = {}\n",
    "            base_selectors = self.base_selectors[site_type]['products']\n",
    "            \n",
    "            # Test conteneurs de produits\n",
    "            for selector in base_selectors['containers']:\n",
    "                try:\n",
    "                    elements = self.driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                    if len(elements) >= 3:  # Au moins 3 produits\n",
    "                        selectors['container'] = selector\n",
    "                        print(f\"‚úÖ Conteneur: {selector} ({len(elements)} √©l√©ments)\")\n",
    "                        break\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            if not selectors.get('container'):\n",
    "                print(\"‚ùå Aucun conteneur de produit trouv√©\")\n",
    "                return {}\n",
    "            \n",
    "            # Test autres s√©lecteurs dans le contexte du conteneur\n",
    "            container_elements = self.driver.find_elements(By.CSS_SELECTOR, selectors['container'])\n",
    "            \n",
    "            if container_elements:\n",
    "                first_container = container_elements[0]\n",
    "                \n",
    "                # Test titres\n",
    "                for title_selector in base_selectors['titles']:\n",
    "                    try:\n",
    "                        title_elem = first_container.find_element(By.CSS_SELECTOR, title_selector)\n",
    "                        if title_elem.text.strip():\n",
    "                            selectors['title'] = title_selector\n",
    "                            print(f\"‚úÖ Titre: {title_selector}\")\n",
    "                            break\n",
    "                    except:\n",
    "                        continue\n",
    "                \n",
    "                # Test URLs\n",
    "                for url_selector in base_selectors['urls']:\n",
    "                    try:\n",
    "                        url_elem = first_container.find_element(By.CSS_SELECTOR, url_selector)\n",
    "                        href = url_elem.get_attribute('href')\n",
    "                        if href and ('amazon.com' in href or 'ebay.com' in href):\n",
    "                            selectors['url'] = url_selector\n",
    "                            print(f\"‚úÖ URL: {url_selector}\")\n",
    "                            break\n",
    "                    except:\n",
    "                        continue\n",
    "                \n",
    "                # Test prix\n",
    "                for price_selector in base_selectors['prices']:\n",
    "                    try:\n",
    "                        price_elem = first_container.find_element(By.CSS_SELECTOR, price_selector)\n",
    "                        if price_elem.text.strip() and ('$' in price_elem.text or '‚Ç¨' in price_elem.text):\n",
    "                            selectors['price'] = price_selector\n",
    "                            print(f\"‚úÖ Prix: {price_selector}\")\n",
    "                            break\n",
    "                    except:\n",
    "                        continue\n",
    "                \n",
    "                # Test ratings\n",
    "                for rating_selector in base_selectors['ratings']:\n",
    "                    try:\n",
    "                        rating_elem = first_container.find_element(By.CSS_SELECTOR, rating_selector)\n",
    "                        rating_text = rating_elem.get_attribute('textContent') or rating_elem.text\n",
    "                        if rating_text and ('star' in rating_text.lower() or '√©toile' in rating_text.lower()):\n",
    "                            selectors['rating'] = rating_selector\n",
    "                            print(f\"‚úÖ Rating: {rating_selector}\")\n",
    "                            break\n",
    "                    except:\n",
    "                        continue\n",
    "            \n",
    "            return selectors\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur d√©tection produits: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def _detect_review_selectors(self, site_type, product_selectors):\n",
    "        \"\"\"D√©tecte les s√©lecteurs de reviews en naviguant vers un produit\"\"\"\n",
    "        \n",
    "        try:\n",
    "            # Trouver un lien produit\n",
    "            if not product_selectors.get('url'):\n",
    "                print(\"‚ùå Pas de s√©lecteur URL disponible\")\n",
    "                return {}\n",
    "            \n",
    "            # R√©cup√©rer le premier lien produit\n",
    "            product_links = self.driver.find_elements(By.CSS_SELECTOR, product_selectors['url'])\n",
    "            \n",
    "            if not product_links:\n",
    "                print(\"‚ùå Aucun lien produit trouv√©\")\n",
    "                return {}\n",
    "            \n",
    "            product_url = product_links[0].get_attribute('href')\n",
    "            print(f\"üîó Test reviews sur: {product_url[:80]}...\")\n",
    "            \n",
    "            # Naviguer vers le produit\n",
    "            self.driver.get(product_url)\n",
    "            time.sleep(3)\n",
    "            \n",
    "            # Chercher les reviews sur la page produit ou naviguer vers la page reviews\n",
    "            review_selectors = {}\n",
    "            base_selectors = self.base_selectors[site_type]['reviews']\n",
    "            \n",
    "            # D'abord, chercher un lien vers les reviews\n",
    "            review_link_selectors = [\n",
    "                'a[href*=\"customer-reviews\"]',\n",
    "                'a[href*=\"reviews\"]',\n",
    "                'a[href*=\"review\"]',\n",
    "                '.cr-widget-ACR a'\n",
    "            ]\n",
    "            \n",
    "            review_page_found = False\n",
    "            for link_selector in review_link_selectors:\n",
    "                try:\n",
    "                    review_links = self.driver.find_elements(By.CSS_SELECTOR, link_selector)\n",
    "                    for link in review_links:\n",
    "                        href = link.get_attribute('href')\n",
    "                        if href and 'review' in href:\n",
    "                            print(f\"üîó Navigation vers page reviews: {href[:80]}...\")\n",
    "                            self.driver.get(href)\n",
    "                            time.sleep(3)\n",
    "                            review_page_found = True\n",
    "                            break\n",
    "                    if review_page_found:\n",
    "                        break\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            # Tester les s√©lecteurs de reviews\n",
    "            # Test conteneurs\n",
    "            for container_selector in base_selectors['containers']:\n",
    "                try:\n",
    "                    review_elements = self.driver.find_elements(By.CSS_SELECTOR, container_selector)\n",
    "                    if len(review_elements) >= 2:  # Au moins 2 reviews\n",
    "                        review_selectors['container'] = container_selector\n",
    "                        print(f\"‚úÖ Conteneur reviews: {container_selector} ({len(review_elements)} reviews)\")\n",
    "                        break\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            if review_selectors.get('container'):\n",
    "                # Tester les autres s√©lecteurs dans le contexte\n",
    "                review_containers = self.driver.find_elements(By.CSS_SELECTOR, review_selectors['container'])\n",
    "                \n",
    "                if review_containers:\n",
    "                    first_review = review_containers[0]\n",
    "                    \n",
    "                    # Test texte de review\n",
    "                    for text_selector in base_selectors['texts']:\n",
    "                        try:\n",
    "                            text_elem = first_review.find_element(By.CSS_SELECTOR, text_selector)\n",
    "                            if text_elem.text.strip() and len(text_elem.text.strip()) > 20:\n",
    "                                review_selectors['text'] = text_selector\n",
    "                                print(f\"‚úÖ Texte review: {text_selector}\")\n",
    "                                break\n",
    "                        except:\n",
    "                            continue\n",
    "                    \n",
    "                    # Test titre de review\n",
    "                    for title_selector in base_selectors['titles']:\n",
    "                        try:\n",
    "                            title_elem = first_review.find_element(By.CSS_SELECTOR, title_selector)\n",
    "                            if title_elem.text.strip():\n",
    "                                review_selectors['title'] = title_selector\n",
    "                                print(f\"‚úÖ Titre review: {title_selector}\")\n",
    "                                break\n",
    "                        except:\n",
    "                            continue\n",
    "                    \n",
    "                    # Test rating\n",
    "                    for rating_selector in base_selectors['ratings']:\n",
    "                        try:\n",
    "                            rating_elem = first_review.find_element(By.CSS_SELECTOR, rating_selector)\n",
    "                            rating_text = rating_elem.get_attribute('textContent') or rating_elem.text\n",
    "                            if rating_text and ('star' in rating_text.lower() or '√©toile' in rating_text.lower()):\n",
    "                                review_selectors['rating'] = rating_selector\n",
    "                                print(f\"‚úÖ Rating review: {rating_selector}\")\n",
    "                                break\n",
    "                        except:\n",
    "                            continue\n",
    "                    \n",
    "                    # Test auteur\n",
    "                    for author_selector in base_selectors['authors']:\n",
    "                        try:\n",
    "                            author_elem = first_review.find_element(By.CSS_SELECTOR, author_selector)\n",
    "                            if author_elem.text.strip():\n",
    "                                review_selectors['author'] = author_selector\n",
    "                                print(f\"‚úÖ Auteur review: {author_selector}\")\n",
    "                                break\n",
    "                        except:\n",
    "                            continue\n",
    "                    \n",
    "                    # Test date\n",
    "                    for date_selector in base_selectors['dates']:\n",
    "                        try:\n",
    "                            date_elem = first_review.find_element(By.CSS_SELECTOR, date_selector)\n",
    "                            if date_elem.text.strip():\n",
    "                                review_selectors['date'] = date_selector\n",
    "                                print(f\"‚úÖ Date review: {date_selector}\")\n",
    "                                break\n",
    "                        except:\n",
    "                            continue\n",
    "            \n",
    "            return review_selectors\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur d√©tection reviews: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def _save_detected_selectors(self, selectors, site_type):\n",
    "        \"\"\"Sauvegarde les s√©lecteurs d√©tect√©s\"\"\"\n",
    "        \n",
    "        try:\n",
    "            import os\n",
    "            import json\n",
    "            \n",
    "            config_dir = \"../config\"\n",
    "            os.makedirs(config_dir, exist_ok=True)\n",
    "            \n",
    "            filename = f\"{config_dir}/detected_selectors_{site_type}.json\"\n",
    "            \n",
    "            with open(filename, 'w', encoding='utf-8') as f:\n",
    "                json.dump(selectors, f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "            print(f\"‚úÖ S√©lecteurs sauvegard√©s: {filename}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur sauvegarde: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Ferme proprement le driver\"\"\"\n",
    "        if self.driver:\n",
    "            try:\n",
    "                self.driver.quit()\n",
    "                print(\"‚úÖ Driver scout ferm√©\")\n",
    "            except:\n",
    "                pass\n",
    "            self.driver = None\n",
    "\n",
    "print(\"‚úÖ Scout robuste cr√©√©!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b25cd51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Scraper robuste cr√©√©!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SCRAPER ROBUSTE POUR REVIEWS DE PRODUITS\n",
    "# ============================================================================\n",
    "\n",
    "class RobustProductReviewScraper:\n",
    "    \"\"\"\n",
    "    Scraper ultra-robuste pour r√©cup√©rer les reviews de produits\n",
    "    Utilise les s√©lecteurs d√©tect√©s par le scout\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, selectors_file=None):\n",
    "        self.driver = None\n",
    "        self.selectors = {}\n",
    "        self.scraped_data = []\n",
    "        self.session_stats = {\n",
    "            'products_processed': 0,\n",
    "            'reviews_collected': 0,\n",
    "            'errors': 0,\n",
    "            'start_time': None\n",
    "        }\n",
    "        \n",
    "        if selectors_file:\n",
    "            self.load_selectors(selectors_file)\n",
    "    \n",
    "    def load_selectors(self, filename):\n",
    "        \"\"\"Charge les s√©lecteurs depuis un fichier JSON\"\"\"\n",
    "        try:\n",
    "            import json\n",
    "            with open(filename, 'r', encoding='utf-8') as f:\n",
    "                self.selectors = json.load(f)\n",
    "            print(f\"‚úÖ S√©lecteurs charg√©s: {filename}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur chargement s√©lecteurs: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def setup_robust_driver(self, headless=False, timeout=60):\n",
    "        \"\"\"Configuration driver ultra-robuste pour scraping\"\"\"\n",
    "        \n",
    "        max_attempts = 3\n",
    "        \n",
    "        for attempt in range(max_attempts):\n",
    "            try:\n",
    "                print(f\"üöÄ Tentative {attempt + 1}/{max_attempts} - Setup driver scraper...\")\n",
    "                \n",
    "                # Fermer le driver existant\n",
    "                if self.driver:\n",
    "                    try:\n",
    "                        self.driver.quit()\n",
    "                    except:\n",
    "                        pass\n",
    "                    self.driver = None\n",
    "                \n",
    "                # Options anti-d√©tection\n",
    "                options = self._create_stealth_options(headless)\n",
    "                \n",
    "                # Cr√©ation du driver\n",
    "                import undetected_chromedriver as uc\n",
    "                \n",
    "                self.driver = uc.Chrome(\n",
    "                    options=options,\n",
    "                    version_main=None,\n",
    "                    use_subprocess=False,\n",
    "                    log_level=3\n",
    "                )\n",
    "                \n",
    "                # Configuration timeouts\n",
    "                self.driver.set_page_load_timeout(timeout)\n",
    "                self.driver.implicitly_wait(15)\n",
    "                \n",
    "                # Scripts anti-d√©tection\n",
    "                self._inject_stealth_scripts()\n",
    "                \n",
    "                # Test fonctionnement\n",
    "                self.driver.get(\"data:text/html,<html><body><h1>Scraper Ready</h1></body></html>\")\n",
    "                \n",
    "                print(\"‚úÖ Driver scraper pr√™t!\")\n",
    "                print(f\"üé≠ User-Agent: {self.driver.execute_script('return navigator.userAgent;')[:80]}...\")\n",
    "                \n",
    "                return True\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Tentative {attempt + 1} √©chou√©e: {str(e)[:100]}...\")\n",
    "                if attempt < max_attempts - 1:\n",
    "                    time.sleep(5)\n",
    "                else:\n",
    "                    print(\"‚ùå Impossible de cr√©er le driver scraper\")\n",
    "                    return False\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def _create_stealth_options(self, headless=True):\n",
    "        \"\"\"Cr√©e des options Chrome furtives\"\"\"\n",
    "        \n",
    "        import undetected_chromedriver as uc\n",
    "        \n",
    "        options = uc.ChromeOptions()\n",
    "        \n",
    "        # Arguments furtifs\n",
    "        stealth_args = [\n",
    "            '--no-sandbox',\n",
    "            '--disable-dev-shm-usage',\n",
    "            '--disable-gpu',\n",
    "            '--disable-web-security',\n",
    "            '--disable-features=VizDisplayCompositor',\n",
    "            '--disable-blink-features=AutomationControlled',\n",
    "            '--disable-extensions',\n",
    "            '--no-first-run',\n",
    "            '--no-default-browser-check',\n",
    "            '--disable-default-apps',\n",
    "            '--disable-background-timer-throttling',\n",
    "            '--disable-backgrounding-occluded-windows',\n",
    "            '--disable-renderer-backgrounding',\n",
    "            '--window-size=1920,1080',\n",
    "            '--start-maximized'\n",
    "        ]\n",
    "        \n",
    "        for arg in stealth_args:\n",
    "            options.add_argument(arg)\n",
    "        \n",
    "        if headless:\n",
    "            options.add_argument('--headless=new')\n",
    "        \n",
    "        # User agent al√©atoire r√©aliste\n",
    "        try:\n",
    "            user_agent = random.choice(REALISTIC_USER_AGENTS)\n",
    "            options.add_argument(f'--user-agent={user_agent}')\n",
    "        except:\n",
    "            options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')\n",
    "        \n",
    "        # Pr√©f√©rences furtives\n",
    "        prefs = {\n",
    "            \"profile.default_content_setting_values.notifications\": 2,\n",
    "            \"profile.default_content_settings.popups\": 0,\n",
    "            \"profile.managed_default_content_settings.images\": 1,  # Charger les images\n",
    "            \"profile.default_content_setting_values.plugins\": 1,\n",
    "            \"profile.content_settings.plugin_whitelist.adobe-flash-player\": 1,\n",
    "            \"profile.content_settings.exceptions.plugins.*,*.per_resource.adobe-flash-player\": 1\n",
    "        }\n",
    "        options.add_experimental_option(\"prefs\", prefs)\n",
    "        \n",
    "        return options\n",
    "    \n",
    "    def _inject_stealth_scripts(self):\n",
    "        \"\"\"Injecte des scripts anti-d√©tection\"\"\"\n",
    "        try:\n",
    "            stealth_script = '''\n",
    "                Object.defineProperty(navigator, 'webdriver', {\n",
    "                    get: () => undefined,\n",
    "                });\n",
    "                \n",
    "                Object.defineProperty(navigator, 'plugins', {\n",
    "                    get: () => [1, 2, 3, 4, 5],\n",
    "                });\n",
    "                \n",
    "                Object.defineProperty(navigator, 'languages', {\n",
    "                    get: () => ['en-US', 'en'],\n",
    "                });\n",
    "                \n",
    "                window.chrome = {\n",
    "                    runtime: {},\n",
    "                };\n",
    "                \n",
    "                Object.defineProperty(navigator, 'permissions', {\n",
    "                    get: () => ({\n",
    "                        query: () => Promise.resolve({ state: 'granted' }),\n",
    "                    }),\n",
    "                });\n",
    "            '''\n",
    "            \n",
    "            self.driver.execute_cdp_cmd('Page.addScriptToEvaluateOnNewDocument', {\n",
    "                'source': stealth_script\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Scripts anti-d√©tection non inject√©s: {e}\")\n",
    "    \n",
    "    def scrape_category_reviews(self, category, site='amazon', max_products=10, reviews_per_rating=50):\n",
    "        \"\"\"\n",
    "        Scrape principal pour r√©cup√©rer les reviews d'une cat√©gorie\n",
    "        \"\"\"\n",
    "        \n",
    "        if not self.driver:\n",
    "            print(\"‚ùå Driver non initialis√©\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        if not self.selectors:\n",
    "            print(\"‚ùå S√©lecteurs non charg√©s\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        print(\"=\"*80)\n",
    "        print(\"üéØ D√âBUT DU SCRAPING ROBUSTE\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"üì¶ Cat√©gorie: {category}\")\n",
    "        print(f\"üåê Site: {site}\")\n",
    "        print(f\"üìä Produits max: {max_products}\")\n",
    "        print(f\"‚≠ê Reviews par note: {reviews_per_rating}\")\n",
    "        print()\n",
    "        \n",
    "        # Initialiser les stats\n",
    "        self.session_stats['start_time'] = time.time()\n",
    "        \n",
    "        try:\n",
    "            # Phase 1: R√©cup√©rer la liste des produits\n",
    "            products = self._get_products_list(category, site, max_products)\n",
    "            \n",
    "            if not products:\n",
    "                print(\"‚ùå Aucun produit trouv√©\")\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            print(f\"‚úÖ {len(products)} produits trouv√©s\")\n",
    "            \n",
    "            # Phase 2: Scraper les reviews de chaque produit\n",
    "            all_reviews = []\n",
    "            \n",
    "            for i, product in enumerate(products, 1):\n",
    "                print(f\"\\nüì¶ PRODUIT {i}/{len(products)}\")\n",
    "                print(f\"üè∑Ô∏è {product['title'][:60]}...\")\n",
    "                \n",
    "                try:\n",
    "                    product_reviews = self._scrape_product_reviews(\n",
    "                        product, \n",
    "                        reviews_per_rating,\n",
    "                        max_pages=5\n",
    "                    )\n",
    "                    \n",
    "                    if product_reviews:\n",
    "                        all_reviews.extend(product_reviews)\n",
    "                        self.session_stats['reviews_collected'] += len(product_reviews)\n",
    "                        print(f\"‚úÖ {len(product_reviews)} reviews r√©cup√©r√©es\")\n",
    "                    else:\n",
    "                        print(\"‚ö†Ô∏è Aucune review trouv√©e\")\n",
    "                    \n",
    "                    self.session_stats['products_processed'] += 1\n",
    "                    \n",
    "                    # D√©lai humain entre produits\n",
    "                    delay = random.uniform(3, 8)\n",
    "                    print(f\"‚è≥ D√©lai: {delay:.1f}s...\")\n",
    "                    time.sleep(delay)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå Erreur produit {i}: {e}\")\n",
    "                    self.session_stats['errors'] += 1\n",
    "                    continue\n",
    "            \n",
    "            # Phase 3: Cr√©er et nettoyer le DataFrame\n",
    "            if all_reviews:\n",
    "                df = pd.DataFrame(all_reviews)\n",
    "                df = self._clean_review_data(df)\n",
    "                \n",
    "                # Stats finales\n",
    "                duration = time.time() - self.session_stats['start_time']\n",
    "                print(\"\\n\" + \"=\"*80)\n",
    "                print(\"üìä SCRAPING TERMIN√â - STATISTIQUES\")\n",
    "                print(\"=\"*80)\n",
    "                print(f\"‚è±Ô∏è Dur√©e: {duration/60:.1f} minutes\")\n",
    "                print(f\"üì¶ Produits trait√©s: {self.session_stats['products_processed']}\")\n",
    "                print(f\"üìù Reviews r√©cup√©r√©es: {self.session_stats['reviews_collected']}\")\n",
    "                print(f\"‚ùå Erreurs: {self.session_stats['errors']}\")\n",
    "                print(f\"üìà Taux de succ√®s: {(1-self.session_stats['errors']/max(1,len(products)))*100:.1f}%\")\n",
    "                \n",
    "                return df\n",
    "            else:\n",
    "                print(\"‚ùå Aucune review r√©cup√©r√©e\")\n",
    "                return pd.DataFrame()\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur scraping global: {e}\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def _get_products_list(self, category, site, max_products):\n",
    "        \"\"\"R√©cup√®re la liste des produits √† scraper\"\"\"\n",
    "        \n",
    "        try:\n",
    "            # URL de recherche\n",
    "            if site == 'amazon':\n",
    "                search_url = f\"https://www.amazon.com/s?k={category.replace(' ', '+')}\"\n",
    "            elif site == 'ebay':\n",
    "                search_url = f\"https://www.ebay.com/sch/i.html?_nkw={category.replace(' ', '+')}\"\n",
    "            else:\n",
    "                print(f\"‚ùå Site non support√©: {site}\")\n",
    "                return []\n",
    "            \n",
    "            print(f\"üîç Recherche: {search_url}\")\n",
    "            self.driver.get(search_url)\n",
    "            \n",
    "            # Attendre le chargement\n",
    "            time.sleep(5)\n",
    "            \n",
    "            # S√©lecteurs pour ce site\n",
    "            site_selectors = self.selectors.get('products', {})\n",
    "            \n",
    "            if not site_selectors:\n",
    "                print(\"‚ùå S√©lecteurs produits non disponibles\")\n",
    "                return []\n",
    "            \n",
    "            # R√©cup√©rer les conteneurs de produits\n",
    "            container_selector = site_selectors.get('container')\n",
    "            if not container_selector:\n",
    "                print(\"‚ùå S√©lecteur conteneur manquant\")\n",
    "                return []\n",
    "            \n",
    "            containers = self.driver.find_elements(By.CSS_SELECTOR, container_selector)\n",
    "            print(f\"üì¶ {len(containers)} conteneurs trouv√©s\")\n",
    "            \n",
    "            products = []\n",
    "            \n",
    "            for i, container in enumerate(containers[:max_products]):\n",
    "                try:\n",
    "                    product_data = self._extract_product_info(container, site_selectors, category)\n",
    "                    \n",
    "                    if product_data and product_data.get('url'):\n",
    "                        products.append(product_data)\n",
    "                        print(f\"‚úÖ Produit {len(products)}: {product_data['title'][:40]}...\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è Produit {i+1} ignor√©: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            return products\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur r√©cup√©ration produits: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def _extract_product_info(self, container, selectors, category):\n",
    "        \"\"\"Extrait les infos d'un produit depuis son conteneur\"\"\"\n",
    "        \n",
    "        product_data = {\n",
    "            'category': category,\n",
    "            'scraped_at': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        # Titre\n",
    "        try:\n",
    "            if selectors.get('title'):\n",
    "                title_elem = container.find_element(By.CSS_SELECTOR, selectors['title'])\n",
    "                product_data['title'] = title_elem.text.strip()\n",
    "        except:\n",
    "            product_data['title'] = 'Titre non trouv√©'\n",
    "        \n",
    "        # URL\n",
    "        try:\n",
    "            if selectors.get('url'):\n",
    "                url_elem = container.find_element(By.CSS_SELECTOR, selectors['url'])\n",
    "                product_data['url'] = url_elem.get_attribute('href')\n",
    "        except:\n",
    "            product_data['url'] = None\n",
    "        \n",
    "        # Prix\n",
    "        try:\n",
    "            if selectors.get('price'):\n",
    "                price_elem = container.find_element(By.CSS_SELECTOR, selectors['price'])\n",
    "                product_data['price'] = price_elem.text.strip()\n",
    "        except:\n",
    "            product_data['price'] = 'N/A'\n",
    "        \n",
    "        # Rating\n",
    "        try:\n",
    "            if selectors.get('rating'):\n",
    "                rating_elem = container.find_element(By.CSS_SELECTOR, selectors['rating'])\n",
    "                rating_text = rating_elem.get_attribute('textContent') or rating_elem.text\n",
    "                product_data['rating'] = rating_text.strip()\n",
    "        except:\n",
    "            product_data['rating'] = 'N/A'\n",
    "        \n",
    "        return product_data\n",
    "    \n",
    "    def _scrape_product_reviews(self, product, reviews_per_rating, max_pages=5):\n",
    "        \"\"\"Scrape les reviews d'un produit sp√©cifique\"\"\"\n",
    "        \n",
    "        try:\n",
    "            # Naviguer vers le produit\n",
    "            self.driver.get(product['url'])\n",
    "            time.sleep(3)\n",
    "            \n",
    "            # Trouver la page des reviews\n",
    "            reviews_url = self._find_reviews_page(product['url'])\n",
    "            \n",
    "            if reviews_url:\n",
    "                self.driver.get(reviews_url)\n",
    "                time.sleep(3)\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è Page reviews non trouv√©e, tentative sur page produit\")\n",
    "            \n",
    "            # R√©cup√©rer les reviews\n",
    "            all_reviews = []\n",
    "            \n",
    "            # S√©lecteurs reviews\n",
    "            review_selectors = self.selectors.get('reviews', {})\n",
    "            \n",
    "            if not review_selectors:\n",
    "                print(\"‚ùå S√©lecteurs reviews non disponibles\")\n",
    "                return []\n",
    "            \n",
    "            # Scraper les reviews page par page\n",
    "            for page in range(max_pages):\n",
    "                try:\n",
    "                    page_reviews = self._extract_reviews_from_page(product, review_selectors)\n",
    "                    \n",
    "                    if page_reviews:\n",
    "                        all_reviews.extend(page_reviews)\n",
    "                        print(f\"üìù Page {page+1}: {len(page_reviews)} reviews\")\n",
    "                        \n",
    "                        # Essayer de passer √† la page suivante\n",
    "                        if not self._go_to_next_page():\n",
    "                            print(\"üìÑ Plus de pages disponibles\")\n",
    "                            break\n",
    "                        \n",
    "                        time.sleep(random.uniform(2, 4))\n",
    "                    else:\n",
    "                        print(f\"üìÑ Page {page+1}: aucune review\")\n",
    "                        break\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå Erreur page {page+1}: {e}\")\n",
    "                    break\n",
    "            \n",
    "            # Limiter le nombre de reviews si n√©cessaire\n",
    "            if len(all_reviews) > reviews_per_rating * 5:  # 5 notes possibles\n",
    "                all_reviews = all_reviews[:reviews_per_rating * 5]\n",
    "            \n",
    "            return all_reviews\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur scraping reviews produit: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def _find_reviews_page(self, product_url):\n",
    "        \"\"\"Trouve l'URL de la page des reviews\"\"\"\n",
    "        \n",
    "        try:\n",
    "            # S√©lecteurs de liens vers reviews\n",
    "            review_link_selectors = [\n",
    "                'a[href*=\"customer-reviews\"]',\n",
    "                'a[href*=\"reviews\"]',\n",
    "                '[data-hook=\"see-all-reviews-link-foot\"]',\n",
    "                '.cr-widget-ACR a'\n",
    "            ]\n",
    "            \n",
    "            for selector in review_link_selectors:\n",
    "                try:\n",
    "                    links = self.driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                    for link in links:\n",
    "                        href = link.get_attribute('href')\n",
    "                        if href and 'review' in href:\n",
    "                            return href\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            # Si aucun lien trouv√©, construire l'URL pour Amazon\n",
    "            if 'amazon.com' in product_url:\n",
    "                import re\n",
    "                asin_match = re.search(r'/dp/([A-Z0-9]{10})', product_url)\n",
    "                if asin_match:\n",
    "                    asin = asin_match.group(1)\n",
    "                    return f\"https://www.amazon.com/product-reviews/{asin}\"\n",
    "            \n",
    "            return None\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Erreur recherche page reviews: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _extract_reviews_from_page(self, product, review_selectors):\n",
    "        \"\"\"Extrait les reviews de la page actuelle\"\"\"\n",
    "        \n",
    "        reviews = []\n",
    "        \n",
    "        try:\n",
    "            # R√©cup√©rer les conteneurs de reviews\n",
    "            container_selector = review_selectors.get('container')\n",
    "            if not container_selector:\n",
    "                return []\n",
    "            \n",
    "            review_containers = self.driver.find_elements(By.CSS_SELECTOR, container_selector)\n",
    "            \n",
    "            for container in review_containers:\n",
    "                try:\n",
    "                    review_data = {\n",
    "                        'product_name': product['title'],\n",
    "                        'product_category': product['category'],\n",
    "                        'product_url': product['url'],\n",
    "                        'product_price': product.get('price', 'N/A'),\n",
    "                        'scraped_at': datetime.now().isoformat()\n",
    "                    }\n",
    "                    \n",
    "                    # Texte de la review\n",
    "                    if review_selectors.get('text'):\n",
    "                        try:\n",
    "                            text_elem = container.find_element(By.CSS_SELECTOR, review_selectors['text'])\n",
    "                            review_data['review_text'] = text_elem.text.strip()\n",
    "                        except:\n",
    "                            review_data['review_text'] = ''\n",
    "                    \n",
    "                    # Titre de la review\n",
    "                    if review_selectors.get('title'):\n",
    "                        try:\n",
    "                            title_elem = container.find_element(By.CSS_SELECTOR, review_selectors['title'])\n",
    "                            review_data['review_title'] = title_elem.text.strip()\n",
    "                        except:\n",
    "                            review_data['review_title'] = ''\n",
    "                    \n",
    "                    # Note de la review\n",
    "                    if review_selectors.get('rating'):\n",
    "                        try:\n",
    "                            rating_elem = container.find_element(By.CSS_SELECTOR, review_selectors['rating'])\n",
    "                            rating_text = rating_elem.get_attribute('textContent') or rating_elem.text\n",
    "                            # Extraire le chiffre de la note\n",
    "                            import re\n",
    "                            rating_match = re.search(r'(\\d+(?:\\.\\d+)?)', rating_text)\n",
    "                            review_data['user_rating'] = float(rating_match.group(1)) if rating_match else None\n",
    "                        except:\n",
    "                            review_data['user_rating'] = None\n",
    "                    \n",
    "                    # Auteur\n",
    "                    if review_selectors.get('author'):\n",
    "                        try:\n",
    "                            author_elem = container.find_element(By.CSS_SELECTOR, review_selectors['author'])\n",
    "                            review_data['reviewer_name'] = author_elem.text.strip()\n",
    "                        except:\n",
    "                            review_data['reviewer_name'] = 'Anonymous'\n",
    "                    \n",
    "                    # Date\n",
    "                    if review_selectors.get('date'):\n",
    "                        try:\n",
    "                            date_elem = container.find_element(By.CSS_SELECTOR, review_selectors['date'])\n",
    "                            review_data['review_date'] = date_elem.text.strip()\n",
    "                        except:\n",
    "                            review_data['review_date'] = 'N/A'\n",
    "                    \n",
    "                    # Ajouter si on a du contenu utile\n",
    "                    if (review_data.get('review_text') and len(review_data['review_text']) > 10) or \\\n",
    "                       (review_data.get('review_title') and len(review_data['review_title']) > 5):\n",
    "                        reviews.append(review_data)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    continue  # Ignorer les reviews probl√©matiques\n",
    "            \n",
    "            return reviews\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur extraction reviews: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def _go_to_next_page(self):\n",
    "        \"\"\"Tente de passer √† la page suivante\"\"\"\n",
    "        \n",
    "        try:\n",
    "            # S√©lecteurs de bouton \"suivant\"\n",
    "            next_selectors = [\n",
    "                '.a-pagination .a-last a',\n",
    "                'a[aria-label=\"Next page\"]',\n",
    "                '.a-pagination li:last-child a',\n",
    "                'a[href*=\"pageNumber\"]'\n",
    "            ]\n",
    "            \n",
    "            for selector in next_selectors:\n",
    "                try:\n",
    "                    next_button = self.driver.find_element(By.CSS_SELECTOR, selector)\n",
    "                    if next_button.is_enabled():\n",
    "                        next_button.click()\n",
    "                        time.sleep(2)\n",
    "                        return True\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            return False\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Erreur navigation page suivante: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def _clean_review_data(self, df):\n",
    "        \"\"\"Nettoie et optimise les donn√©es r√©cup√©r√©es\"\"\"\n",
    "        \n",
    "        try:\n",
    "            print(\"üßπ Nettoyage des donn√©es...\")\n",
    "            \n",
    "            # Supprimer les doublons\n",
    "            initial_count = len(df)\n",
    "            df = df.drop_duplicates(subset=['review_text', 'product_name'], keep='first')\n",
    "            print(f\"üìù Doublons supprim√©s: {initial_count - len(df)}\")\n",
    "            \n",
    "            # Nettoyer les textes\n",
    "            df['review_text'] = df['review_text'].str.strip()\n",
    "            df['review_title'] = df['review_title'].str.strip()\n",
    "            df['product_name'] = df['product_name'].str.strip()\n",
    "            \n",
    "            # Filtrer les reviews trop courtes\n",
    "            df = df[df['review_text'].str.len() > 15]\n",
    "            \n",
    "            # Ajouter des m√©triques\n",
    "            df['review_length'] = df['review_text'].str.len()\n",
    "            df['word_count'] = df['review_text'].str.split().str.len()\n",
    "            \n",
    "            # Nettoyer les ratings\n",
    "            df['user_rating'] = pd.to_numeric(df['user_rating'], errors='coerce')\n",
    "            \n",
    "            print(f\"‚úÖ {len(df)} reviews nettoy√©es\")\n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur nettoyage: {e}\")\n",
    "            return df\n",
    "    \n",
    "    def save_data(self, df, filename=None):\n",
    "        \"\"\"Sauvegarde les donn√©es avec horodatage\"\"\"\n",
    "        \n",
    "        try:\n",
    "            if filename is None:\n",
    "                timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "                filename = f\"../data/raw/reviews_robustes_{timestamp}.csv\"\n",
    "            \n",
    "            import os\n",
    "            os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "            \n",
    "            df.to_csv(filename, index=False, encoding='utf-8')\n",
    "            print(f\"üíæ Donn√©es sauvegard√©es: {filename}\")\n",
    "            \n",
    "            # Sauvegarder aussi en JSON pour backup\n",
    "            json_filename = filename.replace('.csv', '.json')\n",
    "            df.to_json(json_filename, orient='records', force_ascii=False, indent=2)\n",
    "            \n",
    "            return filename\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur sauvegarde: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Ferme proprement le scraper\"\"\"\n",
    "        if self.driver:\n",
    "            try:\n",
    "                self.driver.quit()\n",
    "                print(\"‚úÖ Driver scraper ferm√©\")\n",
    "            except:\n",
    "                pass\n",
    "            self.driver = None\n",
    "\n",
    "print(\"‚úÖ Scraper robuste cr√©√©!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f86c5ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Workflow robuste pr√™t!\n",
      "üìñ Utilisez robust_reviews_menu() pour commencer\n",
      "üöÄ Ou run_example('laptops_gaming') pour un test rapide\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# WORKFLOW PRINCIPAL ROBUSTE\n",
    "# ============================================================================\n",
    "\n",
    "def robust_reviews_workflow(category=\"laptop\", site=\"amazon\", max_products=10, reviews_per_rating=50, headless=False):\n",
    "    \"\"\"\n",
    "    Workflow principal ultra-robuste pour scraper les reviews de produits\n",
    "    \n",
    "    Phase 1: D√©tection automatique des balises (Scout)\n",
    "    Phase 2: Scraping des reviews avec balises valid√©es (Scraper)\n",
    "    \n",
    "    Args:\n",
    "        category: cat√©gorie de produits (ex: \"laptop\", \"smartphone\")\n",
    "        site: site √† scraper (\"amazon\" ou \"ebay\")\n",
    "        max_products: nombre max de produits √† analyser\n",
    "        reviews_per_rating: nombre de reviews √† r√©cup√©rer par note\n",
    "        headless: mode sans interface (True) ou visible (False)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\"*100)\n",
    "    print(\"üöÄ WORKFLOW ROBUSTE - SCRAPING REVIEWS DE PRODUITS\")\n",
    "    print(\"=\"*100)\n",
    "    print(f\"üì¶ Cat√©gorie: {category}\")\n",
    "    print(f\"üåê Site: {site}\")\n",
    "    print(f\"üìä Produits max: {max_products}\")\n",
    "    print(f\"‚≠ê Reviews par note: {reviews_per_rating}\")\n",
    "    print(f\"üëÅÔ∏è Mode: {'Headless' if headless else 'Visible'}\")\n",
    "    print(f\"üìà Total estim√©: {max_products * 5 * reviews_per_rating} reviews max\")\n",
    "    print()\n",
    "    \n",
    "    # Variables pour cleanup\n",
    "    scout = None\n",
    "    scraper = None\n",
    "    \n",
    "    try:\n",
    "        # ====================================================================\n",
    "        # PHASE 1: D√âTECTION DES BALISES (SCOUT)\n",
    "        # ====================================================================\n",
    "        print(\"üîç PHASE 1: D√âTECTION AUTOMATIQUE DES BALISES\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        scout = RobustProductReviewScout()\n",
    "        \n",
    "        # Setup du driver scout\n",
    "        if not scout.setup_robust_driver(headless=True, timeout=30):\n",
    "            print(\"‚ùå Impossible d'initialiser le scout\")\n",
    "            return None\n",
    "        \n",
    "        # D√©tection des s√©lecteurs\n",
    "        site_url = f\"https://www.{site}.com\"\n",
    "        detected_selectors = scout.detect_site_selectors(site_url, category)\n",
    "        \n",
    "        if not detected_selectors or not detected_selectors.get('products'):\n",
    "            print(\"‚ùå √âchec de la d√©tection des s√©lecteurs\")\n",
    "            scout.close()\n",
    "            return None\n",
    "        \n",
    "        print(\"‚úÖ S√©lecteurs d√©tect√©s avec succ√®s!\")\n",
    "        print(f\"üì¶ Produits: {list(detected_selectors['products'].keys())}\")\n",
    "        print(f\"üìù Reviews: {list(detected_selectors['reviews'].keys())}\")\n",
    "        \n",
    "        # Fermer le scout\n",
    "        scout.close()\n",
    "        scout = None\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        \n",
    "        # ====================================================================\n",
    "        # PHASE 2: SCRAPING DES REVIEWS (SCRAPER)\n",
    "        # ====================================================================\n",
    "        print(\"üìä PHASE 2: SCRAPING DES REVIEWS\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        scraper = RobustProductReviewScraper()\n",
    "        scraper.selectors = detected_selectors  # Utiliser les s√©lecteurs d√©tect√©s\n",
    "        \n",
    "        # Setup du driver scraper\n",
    "        if not scraper.setup_robust_driver(headless=headless, timeout=60):\n",
    "            print(\"‚ùå Impossible d'initialiser le scraper\")\n",
    "            return None\n",
    "        \n",
    "        # Avertissement utilisateur\n",
    "        if not headless:\n",
    "            print(\"\\nüö® AVERTISSEMENT: Scraping sur site r√©el en cours!\")\n",
    "            print(\"‚è∞ Dur√©e estim√©e: {:.1f} minutes\".format(max_products * 3))\n",
    "            print(\"üìù Respectez les ToS et les limitations de d√©bit\")\n",
    "            \n",
    "            response = input(\"\\nüîÑ Continuer? (o/n): \").strip().lower()\n",
    "            if response not in ['o', 'oui', 'y', 'yes', '']:\n",
    "                print(\"‚èπÔ∏è Arr√™t demand√© par l'utilisateur\")\n",
    "                scraper.close()\n",
    "                return None\n",
    "        \n",
    "        # Lancement du scraping\n",
    "        print(f\"\\nüéØ D√©but du scraping pour '{category}' sur {site}...\")\n",
    "        \n",
    "        df_reviews = scraper.scrape_category_reviews(\n",
    "            category=category,\n",
    "            site=site,\n",
    "            max_products=max_products,\n",
    "            reviews_per_rating=reviews_per_rating\n",
    "        )\n",
    "        \n",
    "        # ====================================================================\n",
    "        # PHASE 3: R√âSULTATS ET SAUVEGARDE\n",
    "        # ====================================================================\n",
    "        if df_reviews.empty:\n",
    "            print(\"‚ùå Aucune review r√©cup√©r√©e\")\n",
    "            scraper.close()\n",
    "            return None\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"üìä ANALYSE DES R√âSULTATS\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        # Statistiques d√©taill√©es\n",
    "        stats = {\n",
    "            'total_reviews': len(df_reviews),\n",
    "            'unique_products': df_reviews['product_name'].nunique(),\n",
    "            'avg_review_length': df_reviews['review_length'].mean(),\n",
    "            'rating_distribution': df_reviews['user_rating'].value_counts().sort_index(),\n",
    "            'top_products': df_reviews['product_name'].value_counts().head(5)\n",
    "        }\n",
    "        \n",
    "        print(f\"‚úÖ Total reviews: {stats['total_reviews']}\")\n",
    "        print(f\"üì¶ Produits uniques: {stats['unique_products']}\")\n",
    "        print(f\"üìù Longueur moyenne: {stats['avg_review_length']:.0f} caract√®res\")\n",
    "        print(f\"‚≠ê Distribution des notes:\")\n",
    "        for rating, count in stats['rating_distribution'].items():\n",
    "            if pd.notna(rating):\n",
    "                print(f\"   {rating} √©toiles: {count} reviews\")\n",
    "        \n",
    "        print(f\"\\nüèÜ Top produits par nombre de reviews:\")\n",
    "        for product, count in stats['top_products'].items():\n",
    "            print(f\"   ‚Ä¢ {product[:50]}... ({count} reviews)\")\n",
    "        \n",
    "        # Sauvegarde\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"../data/raw/{site}_{category}_{timestamp}.csv\"\n",
    "        \n",
    "        saved_file = scraper.save_data(df_reviews, filename)\n",
    "        \n",
    "        # Aper√ßu des donn√©es\n",
    "        print(f\"\\nüìã APER√áU DES DONN√âES (5 premi√®res reviews):\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        sample_cols = ['product_name', 'user_rating', 'review_text', 'reviewer_name']\n",
    "        available_cols = [col for col in sample_cols if col in df_reviews.columns]\n",
    "        \n",
    "        for i, row in df_reviews[available_cols].head(5).iterrows():\n",
    "            print(f\"\\nReview {i+1}:\")\n",
    "            for col in available_cols:\n",
    "                value = str(row[col])\n",
    "                if col == 'review_text' and len(value) > 100:\n",
    "                    value = value[:100] + \"...\"\n",
    "                elif col == 'product_name' and len(value) > 50:\n",
    "                    value = value[:50] + \"...\"\n",
    "                print(f\"  {col}: {value}\")\n",
    "        \n",
    "        # Fermer le scraper\n",
    "        scraper.close()\n",
    "        scraper = None\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*100)\n",
    "        print(\"üéâ WORKFLOW TERMIN√â AVEC SUCC√àS!\")\n",
    "        print(\"=\"*100)\n",
    "        \n",
    "        return df_reviews\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n‚èπÔ∏è Arr√™t demand√© par l'utilisateur\")\n",
    "        return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Erreur workflow: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "        \n",
    "    finally:\n",
    "        # Cleanup des ressources\n",
    "        if scout:\n",
    "            try:\n",
    "                scout.close()\n",
    "            except:\n",
    "                pass\n",
    "        if scraper:\n",
    "            try:\n",
    "                scraper.close()\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "def quick_scout_test(site=\"amazon\", category=\"laptop\"):\n",
    "    \"\"\"\n",
    "    Test rapide du scout uniquement\n",
    "    \"\"\"\n",
    "    print(f\"üß™ TEST RAPIDE - Scout pour {category} sur {site}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    scout = RobustProductReviewScout()\n",
    "    \n",
    "    try:\n",
    "        if scout.setup_robust_driver(headless=True):\n",
    "            site_url = f\"https://www.{site}.com\"\n",
    "            selectors = scout.detect_site_selectors(site_url, category)\n",
    "            \n",
    "            if selectors:\n",
    "                print(\"‚úÖ Test scout r√©ussi!\")\n",
    "                print(f\"üì¶ S√©lecteurs produits: {list(selectors['products'].keys())}\")\n",
    "                print(f\"üìù S√©lecteurs reviews: {list(selectors['reviews'].keys())}\")\n",
    "                return selectors\n",
    "            else:\n",
    "                print(\"‚ùå Test scout √©chou√©\")\n",
    "                return None\n",
    "        else:\n",
    "            print(\"‚ùå Impossible de cr√©er le driver scout\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur test scout: {e}\")\n",
    "        return None\n",
    "        \n",
    "    finally:\n",
    "        scout.close()\n",
    "\n",
    "def robust_reviews_menu():\n",
    "    \"\"\"\n",
    "    Menu principal pour le workflow robuste\n",
    "    \"\"\"\n",
    "    print(\"=\"*100)\n",
    "    print(\"üéØ WORKFLOW ROBUSTE - REVIEWS DE PRODUITS\")\n",
    "    print(\"=\"*100)\n",
    "    print()\n",
    "    print(\"1Ô∏è‚É£ Workflow complet (scout + scraper)\")\n",
    "    print(\"2Ô∏è‚É£ Test scout uniquement\")\n",
    "    print(\"3Ô∏è‚É£ Configuration avanc√©e\")\n",
    "    print(\"4Ô∏è‚É£ Voir fichiers de donn√©es\")\n",
    "    print(\"5Ô∏è‚É£ Quitter\")\n",
    "    print()\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            choice = input(\"üëâ Votre choix (1-5): \").strip()\n",
    "            \n",
    "            if choice == '1':\n",
    "                print(\"\\nüìã CONFIGURATION DU WORKFLOW COMPLET\")\n",
    "                print(\"-\" * 50)\n",
    "                \n",
    "                category = input(\"üè∑Ô∏è Cat√©gorie (ex: laptop, smartphone): \").strip() or \"laptop\"\n",
    "                site = input(\"üåê Site (amazon/ebay): \").strip() or \"amazon\"\n",
    "                \n",
    "                try:\n",
    "                    max_products = int(input(\"üì¶ Nombre de produits (d√©faut: 5): \") or \"5\")\n",
    "                    reviews_per_rating = int(input(\"‚≠ê Reviews par note (d√©faut: 20): \") or \"20\")\n",
    "                except ValueError:\n",
    "                    max_products, reviews_per_rating = 5, 20\n",
    "                \n",
    "                headless_choice = input(\"üëÅÔ∏è Mode headless? (o/n, d√©faut: n): \").strip().lower()\n",
    "                headless = headless_choice in ['o', 'oui', 'y', 'yes']\n",
    "                \n",
    "                print(f\"\\nüöÄ Lancement du workflow...\")\n",
    "                result = robust_reviews_workflow(category, site, max_products, reviews_per_rating, headless)\n",
    "                \n",
    "                if result is not None:\n",
    "                    print(f\"\\n‚úÖ Workflow termin√© - {len(result)} reviews r√©cup√©r√©es\")\n",
    "                else:\n",
    "                    print(\"\\n‚ùå Workflow √©chou√©\")\n",
    "                \n",
    "                return result\n",
    "                \n",
    "            elif choice == '2':\n",
    "                print(\"\\nüß™ TEST SCOUT\")\n",
    "                print(\"-\" * 30)\n",
    "                \n",
    "                site = input(\"üåê Site (amazon/ebay): \").strip() or \"amazon\"\n",
    "                category = input(\"üè∑Ô∏è Cat√©gorie: \").strip() or \"laptop\"\n",
    "                \n",
    "                result = quick_scout_test(site, category)\n",
    "                if result:\n",
    "                    print(\"‚úÖ Scout fonctionne correctement!\")\n",
    "                else:\n",
    "                    print(\"‚ùå Probl√®me avec le scout\")\n",
    "                \n",
    "            elif choice == '3':\n",
    "                print(\"\\n‚öôÔ∏è CONFIGURATION AVANC√âE\")\n",
    "                print(\"-\" * 40)\n",
    "                print(\"üìñ Param√®tres disponibles dans robust_reviews_workflow():\")\n",
    "                print(\"   ‚Ä¢ category: cat√©gorie de produits\")\n",
    "                print(\"   ‚Ä¢ site: amazon ou ebay\")\n",
    "                print(\"   ‚Ä¢ max_products: nombre de produits max\")\n",
    "                print(\"   ‚Ä¢ reviews_per_rating: reviews par note (1-5)\")\n",
    "                print(\"   ‚Ä¢ headless: mode sans interface\")\n",
    "                print(\"\\nüí° Exemple:\")\n",
    "                print(\"   df = robust_reviews_workflow('gaming laptop', 'amazon', 8, 30, False)\")\n",
    "                \n",
    "            elif choice == '4':\n",
    "                print(\"\\nüìÅ FICHIERS DE DONN√âES\")\n",
    "                print(\"-\" * 30)\n",
    "                \n",
    "                import os\n",
    "                data_dir = \"../data/raw\"\n",
    "                \n",
    "                if os.path.exists(data_dir):\n",
    "                    files = [f for f in os.listdir(data_dir) if f.endswith('.csv')]\n",
    "                    if files:\n",
    "                        print(\"üìÑ Fichiers CSV trouv√©s:\")\n",
    "                        for f in sorted(files, reverse=True)[:10]:  # 10 plus r√©cents\n",
    "                            size = os.path.getsize(os.path.join(data_dir, f)) / 1024  # KB\n",
    "                            print(f\"   ‚Ä¢ {f} ({size:.1f} KB)\")\n",
    "                    else:\n",
    "                        print(\"‚ùå Aucun fichier CSV trouv√©\")\n",
    "                else:\n",
    "                    print(\"‚ùå Dossier data/raw non trouv√©\")\n",
    "                \n",
    "            elif choice == '5':\n",
    "                print(\"üëã √Ä bient√¥t!\")\n",
    "                break\n",
    "                \n",
    "            else:\n",
    "                print(\"‚ùå Choix invalide\")\n",
    "                \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nüëã √Ä bient√¥t!\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur: {e}\")\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Configuration d'exemples pr√™ts √† l'emploi\n",
    "ROBUST_EXAMPLES = {\n",
    "    'laptops_gaming': {\n",
    "        'category': 'gaming laptop',\n",
    "        'site': 'amazon',\n",
    "        'max_products': 8,\n",
    "        'reviews_per_rating': 25,\n",
    "        'headless': False\n",
    "    },\n",
    "    'smartphones': {\n",
    "        'category': 'smartphone',\n",
    "        'site': 'amazon', \n",
    "        'max_products': 6,\n",
    "        'reviews_per_rating': 30,\n",
    "        'headless': False\n",
    "    },\n",
    "    'headphones': {\n",
    "        'category': 'wireless headphones',\n",
    "        'site': 'amazon',\n",
    "        'max_products': 10,\n",
    "        'reviews_per_rating': 20,\n",
    "        'headless': False\n",
    "    }\n",
    "}\n",
    "\n",
    "def run_example(example_name):\n",
    "    \"\"\"Ex√©cute un exemple pr√©d√©fini\"\"\"\n",
    "    if example_name in ROBUST_EXAMPLES:\n",
    "        config = ROBUST_EXAMPLES[example_name]\n",
    "        print(f\"üöÄ Lancement exemple: {example_name}\")\n",
    "        return robust_reviews_workflow(**config)\n",
    "    else:\n",
    "        print(f\"‚ùå Exemple '{example_name}' non trouv√©\")\n",
    "        print(f\"üìã Disponibles: {list(ROBUST_EXAMPLES.keys())}\")\n",
    "        return None\n",
    "\n",
    "print(\"‚úÖ Workflow robuste pr√™t!\")\n",
    "print(\"üìñ Utilisez robust_reviews_menu() pour commencer\")\n",
    "print(\"üöÄ Ou run_example('laptops_gaming') pour un test rapide\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6919b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "üéØ WORKFLOW ROBUSTE - REVIEWS DE PRODUITS\n",
      "====================================================================================================\n",
      "\n",
      "1Ô∏è‚É£ Workflow complet (scout + scraper)\n",
      "2Ô∏è‚É£ Test scout uniquement\n",
      "3Ô∏è‚É£ Configuration avanc√©e\n",
      "4Ô∏è‚É£ Voir fichiers de donn√©es\n",
      "5Ô∏è‚É£ Quitter\n",
      "\n",
      "\n",
      "üìã CONFIGURATION DU WORKFLOW COMPLET\n",
      "--------------------------------------------------\n",
      "\n",
      "üöÄ Lancement du workflow...\n",
      "====================================================================================================\n",
      "üöÄ WORKFLOW ROBUSTE - SCRAPING REVIEWS DE PRODUITS\n",
      "====================================================================================================\n",
      "üì¶ Cat√©gorie: laptop\n",
      "üåê Site: amazon\n",
      "üìä Produits max: 5\n",
      "‚≠ê Reviews par note: 20\n",
      "üëÅÔ∏è Mode: Headless\n",
      "üìà Total estim√©: 500 reviews max\n",
      "\n",
      "üîç PHASE 1: D√âTECTION AUTOMATIQUE DES BALISES\n",
      "----------------------------------------------------------------------\n",
      "üîß Tentative 1/3 - Setup driver scout...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-29 16:24:30,659 - INFO - patching driver executable C:\\Users\\Yann\\appdata\\roaming\\undetected_chromedriver\\undetected_chromedriver.exe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Driver scout initialis√© avec succ√®s!\n",
      "üîç D√©tection des s√©lecteurs pour amazon...\n",
      "üåê Navigation vers: https://www.amazon.com/s?k=laptop\n",
      "‚ùå Aucun conteneur de produit trouv√©\n",
      "‚ùå Impossible de d√©tecter les s√©lecteurs produits\n",
      "‚ùå √âchec de la d√©tection des s√©lecteurs\n",
      "‚úÖ Driver scout ferm√©\n",
      "\n",
      "‚ùå Workflow √©chou√©\n"
     ]
    }
   ],
   "source": [
    "robust_reviews_menu()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scraper_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
