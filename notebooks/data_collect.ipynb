{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fb82a26",
   "metadata": {},
   "source": [
    "# üïµÔ∏è Advanced Marketplace Scraper - Anti-Detection System\n",
    "\n",
    "Ce notebook impl√©mente un scraper Selenium avanc√© avec syst√®me anti-d√©tection pour scraper des marketplaces anglaises :\n",
    "\n",
    "## üõ°Ô∏è Protections Anti-Ban :\n",
    "- **Fake User Agents** : Rotation automatique d'user agents r√©alistes\n",
    "- **Proxy Rotation** : Rotation d'IP pour √©viter la d√©tection\n",
    "- **Comportement Humain** : Mouvements de souris, scrolling, pauses naturelles\n",
    "- **D√©lais Al√©atoires** : Timing humain entre les actions\n",
    "- **Headers R√©alistes** : Simulation compl√®te de navigateur r√©el\n",
    "\n",
    "## üéØ Sites Cibl√©s :\n",
    "- **E-commerce** : Amazon-like sites, eBay, etc.\n",
    "- **Reviews** : Trustpilot, Google Reviews, Yelp\n",
    "- **Product Data** : Prix, descriptions, reviews avec dates\n",
    "- **APIs publiques** quand disponibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b4054a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (4.11.2)\n",
      "Requirement already satisfied: webdriver-manager in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (4.0.2)\n",
      "Requirement already satisfied: fake-useragent in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (2.2.0)\n",
      "Requirement already satisfied: requests in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (2.28.2)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (4.13.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (2.3.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.26.15)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from selenium) (0.30.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from selenium) (0.12.2)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from selenium) (2025.6.15)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium) (25.3.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium) (2.8)\n",
      "Requirement already satisfied: outcome in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium) (1.17.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from webdriver-manager) (1.1.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from webdriver-manager) (25.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from requests) (3.4.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from beautifulsoup4) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from beautifulsoup4) (4.13.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from pandas) (2.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: pycparser in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.22)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: undetected-chromedriver in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (3.5.3)\n",
      "Requirement already satisfied: selenium-stealth in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (1.0.6)\n",
      "Requirement already satisfied: pyautogui in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (0.9.54)\n",
      "Requirement already satisfied: selenium>=4.9.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from undetected-chromedriver) (4.11.2)\n",
      "Requirement already satisfied: requests in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from undetected-chromedriver) (2.28.2)\n",
      "Requirement already satisfied: websockets in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from undetected-chromedriver) (15.0.1)\n",
      "Requirement already satisfied: pymsgbox in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from pyautogui) (1.0.9)\n",
      "Requirement already satisfied: pytweening>=1.0.4 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from pyautogui) (1.2.0)\n",
      "Requirement already satisfied: pyscreeze>=0.1.21 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from pyautogui) (1.0.1)\n",
      "Requirement already satisfied: pygetwindow>=0.0.5 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from pyautogui) (0.0.9)\n",
      "Requirement already satisfied: mouseinfo in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from pyautogui) (0.1.3)\n",
      "Requirement already satisfied: pyrect in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from pygetwindow>=0.0.5->pyautogui) (0.2.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium>=4.9.0->undetected-chromedriver) (1.26.15)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from selenium>=4.9.0->undetected-chromedriver) (0.30.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from selenium>=4.9.0->undetected-chromedriver) (0.12.2)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from selenium>=4.9.0->undetected-chromedriver) (2025.6.15)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (25.3.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (2.8)\n",
      "Requirement already satisfied: outcome in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (1.17.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio-websocket~=0.9->selenium>=4.9.0->undetected-chromedriver) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium>=4.9.0->undetected-chromedriver) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (2.22)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium>=4.9.0->undetected-chromedriver) (0.16.0)\n",
      "Requirement already satisfied: pyperclip in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from mouseinfo->pyautogui) (1.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from requests->undetected-chromedriver) (3.4.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: undetected-chromedriver in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (3.5.3)\n",
      "Requirement already satisfied: selenium-stealth in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (1.0.6)\n",
      "Requirement already satisfied: pyautogui in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (0.9.54)\n",
      "Requirement already satisfied: selenium>=4.9.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from undetected-chromedriver) (4.11.2)\n",
      "Requirement already satisfied: requests in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from undetected-chromedriver) (2.28.2)\n",
      "Requirement already satisfied: websockets in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from undetected-chromedriver) (15.0.1)\n",
      "Requirement already satisfied: pymsgbox in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from pyautogui) (1.0.9)\n",
      "Requirement already satisfied: pytweening>=1.0.4 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from pyautogui) (1.2.0)\n",
      "Requirement already satisfied: pyscreeze>=0.1.21 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from pyautogui) (1.0.1)\n",
      "Requirement already satisfied: pygetwindow>=0.0.5 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from pyautogui) (0.0.9)\n",
      "Requirement already satisfied: mouseinfo in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from pyautogui) (0.1.3)\n",
      "Requirement already satisfied: pyrect in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from pygetwindow>=0.0.5->pyautogui) (0.2.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium>=4.9.0->undetected-chromedriver) (1.26.15)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from selenium>=4.9.0->undetected-chromedriver) (0.30.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from selenium>=4.9.0->undetected-chromedriver) (0.12.2)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from selenium>=4.9.0->undetected-chromedriver) (2025.6.15)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (25.3.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (2.8)\n",
      "Requirement already satisfied: outcome in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (1.17.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio-websocket~=0.9->selenium>=4.9.0->undetected-chromedriver) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium>=4.9.0->undetected-chromedriver) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (2.22)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium>=4.9.0->undetected-chromedriver) (0.16.0)\n",
      "Requirement already satisfied: pyperclip in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from mouseinfo->pyautogui) (1.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from requests->undetected-chromedriver) (3.4.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: requests-proxy-adapter in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (0.1.1)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (2.9.0.post0)\n",
      "Requirement already satisfied: lxml in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (6.0.0)\n",
      "Requirement already satisfied: pluggy==0.11.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from requests-proxy-adapter) (0.11.0)\n",
      "Collecting requests==2.22.0 (from requests-proxy-adapter)\n",
      "  Downloading requests-2.22.0-py2.py3-none-any.whl.metadata (5.5 kB)\n",
      "Requirement already satisfied: urllib3>=1.15 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from requests-proxy-adapter) (1.26.15)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from requests==2.22.0->requests-proxy-adapter) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from requests==2.22.0->requests-proxy-adapter) (2.8)\n",
      "Collecting urllib3>=1.15 (from requests-proxy-adapter)\n",
      "  Downloading urllib3-1.25.11-py2.py3-none-any.whl.metadata (41 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from requests==2.22.0->requests-proxy-adapter) (2025.6.15)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from python-dateutil) (1.17.0)\n",
      "Downloading requests-2.22.0-py2.py3-none-any.whl (57 kB)\n",
      "Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
      "Installing collected packages: urllib3, requests\n",
      "\n",
      "  Attempting uninstall: urllib3\n",
      "\n",
      "    Found existing installation: urllib3 1.26.15\n",
      "\n",
      "    Uninstalling urllib3-1.26.15:\n",
      "\n",
      "      Successfully uninstalled urllib3-1.26.15\n",
      "\n",
      "   ---------------------------------------- 0/2 [urllib3]\n",
      "  Attempting uninstall: requests\n",
      "   ---------------------------------------- 0/2 [urllib3]\n",
      "    Found existing installation: requests 2.28.2\n",
      "   ---------------------------------------- 0/2 [urllib3]\n",
      "    Uninstalling requests-2.28.2:\n",
      "   ---------------------------------------- 0/2 [urllib3]\n",
      "      Successfully uninstalled requests-2.28.2\n",
      "   ---------------------------------------- 0/2 [urllib3]\n",
      "   -------------------- ------------------- 1/2 [requests]\n",
      "   ---------------------------------------- 2/2 [requests]\n",
      "\n",
      "Successfully installed requests-2.22.0 urllib3-1.25.11\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "üì¶ Installation termin√©e - Scraper anti-d√©tection pr√™t !\n",
      "Requirement already satisfied: requests-proxy-adapter in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (0.1.1)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (2.9.0.post0)\n",
      "Requirement already satisfied: lxml in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (6.0.0)\n",
      "Requirement already satisfied: pluggy==0.11.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from requests-proxy-adapter) (0.11.0)\n",
      "Collecting requests==2.22.0 (from requests-proxy-adapter)\n",
      "  Downloading requests-2.22.0-py2.py3-none-any.whl.metadata (5.5 kB)\n",
      "Requirement already satisfied: urllib3>=1.15 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from requests-proxy-adapter) (1.26.15)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from requests==2.22.0->requests-proxy-adapter) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from requests==2.22.0->requests-proxy-adapter) (2.8)\n",
      "Collecting urllib3>=1.15 (from requests-proxy-adapter)\n",
      "  Downloading urllib3-1.25.11-py2.py3-none-any.whl.metadata (41 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from requests==2.22.0->requests-proxy-adapter) (2025.6.15)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from python-dateutil) (1.17.0)\n",
      "Downloading requests-2.22.0-py2.py3-none-any.whl (57 kB)\n",
      "Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
      "Installing collected packages: urllib3, requests\n",
      "\n",
      "  Attempting uninstall: urllib3\n",
      "\n",
      "    Found existing installation: urllib3 1.26.15\n",
      "\n",
      "    Uninstalling urllib3-1.26.15:\n",
      "\n",
      "      Successfully uninstalled urllib3-1.26.15\n",
      "\n",
      "   ---------------------------------------- 0/2 [urllib3]\n",
      "  Attempting uninstall: requests\n",
      "   ---------------------------------------- 0/2 [urllib3]\n",
      "    Found existing installation: requests 2.28.2\n",
      "   ---------------------------------------- 0/2 [urllib3]\n",
      "    Uninstalling requests-2.28.2:\n",
      "   ---------------------------------------- 0/2 [urllib3]\n",
      "      Successfully uninstalled requests-2.28.2\n",
      "   ---------------------------------------- 0/2 [urllib3]\n",
      "   -------------------- ------------------- 1/2 [requests]\n",
      "   ---------------------------------------- 2/2 [requests]\n",
      "\n",
      "Successfully installed requests-2.22.0 urllib3-1.25.11\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "üì¶ Installation termin√©e - Scraper anti-d√©tection pr√™t !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "jupyterlab-server 2.27.3 requires requests>=2.31, but you have requests 2.22.0 which is incompatible.\n",
      "selenium 4.11.2 requires urllib3[socks]<3,>=1.26, but you have urllib3 1.25.11 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "# Installation des packages pour scraping anti-d√©tection\n",
    "%pip install selenium webdriver-manager fake-useragent requests beautifulsoup4 pandas\n",
    "%pip install undetected-chromedriver selenium-stealth pyautogui\n",
    "%pip install requests-proxy-adapter python-dateutil lxml\n",
    "\n",
    "print(\"üì¶ Installation termin√©e - Scraper anti-d√©tection pr√™t !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88736027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting urllib3==1.26.18\n",
      "  Downloading urllib3-1.26.18-py2.py3-none-any.whl.metadata (48 kB)\n",
      "Downloading urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
      "Installing collected packages: urllib3\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.25.11\n",
      "    Uninstalling urllib3-1.25.11:\n",
      "      Successfully uninstalled urllib3-1.25.11\n",
      "Successfully installed urllib3-1.26.18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "requests 2.22.0 requires urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you have urllib3 1.26.18 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium==4.15.0\n",
      "  Downloading selenium-4.15.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium==4.15.0) (1.26.18)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from selenium==4.15.0) (0.30.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from selenium==4.15.0) (0.12.2)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from selenium==4.15.0) (2025.6.15)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium==4.15.0) (25.3.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium==4.15.0) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium==4.15.0) (2.8)\n",
      "Requirement already satisfied: outcome in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium==4.15.0) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium==4.15.0) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium==4.15.0) (1.17.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio-websocket~=0.9->selenium==4.15.0) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium==4.15.0) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium==4.15.0) (2.22)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium==4.15.0) (0.16.0)\n",
      "Downloading selenium-4.15.0-py3-none-any.whl (10.2 MB)\n",
      "   ---------------------------------------- 0.0/10.2 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 1.0/10.2 MB 6.6 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 2.6/10.2 MB 6.7 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 4.2/10.2 MB 6.8 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 5.5/10.2 MB 6.7 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 7.1/10.2 MB 6.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 8.4/10.2 MB 6.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.0/10.2 MB 6.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.0/10.2 MB 6.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.2/10.2 MB 5.4 MB/s eta 0:00:00\n",
      "Installing collected packages: selenium\n",
      "  Attempting uninstall: selenium\n",
      "    Found existing installation: selenium 4.11.2\n",
      "    Uninstalling selenium-4.11.2:\n",
      "      Successfully uninstalled selenium-4.11.2\n",
      "Successfully installed selenium-4.15.0\n",
      "Collecting undetected-chromedriver==3.5.4\n",
      "  Downloading undetected-chromedriver-3.5.4.tar.gz (65 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: selenium>=4.9.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from undetected-chromedriver==3.5.4) (4.15.0)\n",
      "Requirement already satisfied: requests in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from undetected-chromedriver==3.5.4) (2.22.0)\n",
      "Requirement already satisfied: websockets in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from undetected-chromedriver==3.5.4) (15.0.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium>=4.9.0->undetected-chromedriver==3.5.4) (1.26.18)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from selenium>=4.9.0->undetected-chromedriver==3.5.4) (0.30.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from selenium>=4.9.0->undetected-chromedriver==3.5.4) (0.12.2)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from selenium>=4.9.0->undetected-chromedriver==3.5.4) (2025.6.15)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver==3.5.4) (25.3.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver==3.5.4) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver==3.5.4) (2.8)\n",
      "Requirement already satisfied: outcome in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver==3.5.4) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver==3.5.4) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver==3.5.4) (1.17.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio-websocket~=0.9->selenium>=4.9.0->undetected-chromedriver==3.5.4) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium>=4.9.0->undetected-chromedriver==3.5.4) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium>=4.9.0->undetected-chromedriver==3.5.4) (2.22)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium>=4.9.0->undetected-chromedriver==3.5.4) (0.16.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from requests->undetected-chromedriver==3.5.4) (3.0.4)\n",
      "INFO: pip is looking at multiple versions of requests to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting requests (from undetected-chromedriver==3.5.4)\n",
      "  Downloading requests-2.32.4-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from requests->undetected-chromedriver==3.5.4) (3.4.2)\n",
      "Downloading requests-2.32.4-py3-none-any.whl (64 kB)\n",
      "Building wheels for collected packages: undetected-chromedriver\n",
      "  Building wheel for undetected-chromedriver (pyproject.toml): started\n",
      "  Building wheel for undetected-chromedriver (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for undetected-chromedriver: filename=undetected_chromedriver-3.5.4-py3-none-any.whl size=47221 sha256=2906bdf9288dd8e24264f2f1eb6fd072f3be1fa9fd107e7123947cc5c3c2d2fa\n",
      "  Stored in directory: c:\\users\\yann\\appdata\\local\\pip\\cache\\wheels\\ca\\7f\\a2\\7feaa6b6bfdf742311206d2b103d185711dda87177add2ee71\n",
      "Successfully built undetected-chromedriver\n",
      "Installing collected packages: requests, undetected-chromedriver\n",
      "\n",
      "  Attempting uninstall: requests\n",
      "\n",
      "    Found existing installation: requests 2.22.0\n",
      "\n",
      "    Uninstalling requests-2.22.0:\n",
      "\n",
      "      Successfully uninstalled requests-2.22.0\n",
      "\n",
      "  Attempting uninstall: undetected-chromedriver\n",
      "\n",
      "    Found existing installation: undetected-chromedriver 3.5.3\n",
      "\n",
      "    Uninstalling undetected-chromedriver-3.5.3:\n",
      "\n",
      "      Successfully uninstalled undetected-chromedriver-3.5.3\n",
      "\n",
      "   -------------------- ------------------- 1/2 [undetected-chromedriver]\n",
      "   -------------------- ------------------- 1/2 [undetected-chromedriver]\n",
      "   ---------------------------------------- 2/2 [undetected-chromedriver]\n",
      "\n",
      "Successfully installed requests-2.32.4 undetected-chromedriver-3.5.4\n",
      "Collecting undetected-chromedriver==3.5.4\n",
      "  Downloading undetected-chromedriver-3.5.4.tar.gz (65 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: selenium>=4.9.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from undetected-chromedriver==3.5.4) (4.15.0)\n",
      "Requirement already satisfied: requests in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from undetected-chromedriver==3.5.4) (2.22.0)\n",
      "Requirement already satisfied: websockets in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from undetected-chromedriver==3.5.4) (15.0.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium>=4.9.0->undetected-chromedriver==3.5.4) (1.26.18)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from selenium>=4.9.0->undetected-chromedriver==3.5.4) (0.30.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from selenium>=4.9.0->undetected-chromedriver==3.5.4) (0.12.2)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from selenium>=4.9.0->undetected-chromedriver==3.5.4) (2025.6.15)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver==3.5.4) (25.3.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver==3.5.4) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver==3.5.4) (2.8)\n",
      "Requirement already satisfied: outcome in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver==3.5.4) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver==3.5.4) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver==3.5.4) (1.17.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio-websocket~=0.9->selenium>=4.9.0->undetected-chromedriver==3.5.4) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium>=4.9.0->undetected-chromedriver==3.5.4) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium>=4.9.0->undetected-chromedriver==3.5.4) (2.22)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium>=4.9.0->undetected-chromedriver==3.5.4) (0.16.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from requests->undetected-chromedriver==3.5.4) (3.0.4)\n",
      "INFO: pip is looking at multiple versions of requests to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting requests (from undetected-chromedriver==3.5.4)\n",
      "  Downloading requests-2.32.4-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from requests->undetected-chromedriver==3.5.4) (3.4.2)\n",
      "Downloading requests-2.32.4-py3-none-any.whl (64 kB)\n",
      "Building wheels for collected packages: undetected-chromedriver\n",
      "  Building wheel for undetected-chromedriver (pyproject.toml): started\n",
      "  Building wheel for undetected-chromedriver (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for undetected-chromedriver: filename=undetected_chromedriver-3.5.4-py3-none-any.whl size=47221 sha256=2906bdf9288dd8e24264f2f1eb6fd072f3be1fa9fd107e7123947cc5c3c2d2fa\n",
      "  Stored in directory: c:\\users\\yann\\appdata\\local\\pip\\cache\\wheels\\ca\\7f\\a2\\7feaa6b6bfdf742311206d2b103d185711dda87177add2ee71\n",
      "Successfully built undetected-chromedriver\n",
      "Installing collected packages: requests, undetected-chromedriver\n",
      "\n",
      "  Attempting uninstall: requests\n",
      "\n",
      "    Found existing installation: requests 2.22.0\n",
      "\n",
      "    Uninstalling requests-2.22.0:\n",
      "\n",
      "      Successfully uninstalled requests-2.22.0\n",
      "\n",
      "  Attempting uninstall: undetected-chromedriver\n",
      "\n",
      "    Found existing installation: undetected-chromedriver 3.5.3\n",
      "\n",
      "    Uninstalling undetected-chromedriver-3.5.3:\n",
      "\n",
      "      Successfully uninstalled undetected-chromedriver-3.5.3\n",
      "\n",
      "   -------------------- ------------------- 1/2 [undetected-chromedriver]\n",
      "   -------------------- ------------------- 1/2 [undetected-chromedriver]\n",
      "   ---------------------------------------- 2/2 [undetected-chromedriver]\n",
      "\n",
      "Successfully installed requests-2.32.4 undetected-chromedriver-3.5.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "requests-proxy-adapter 0.1.1 requires requests==2.22.0, but you have requests 2.32.4 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "# Dans votre terminal ou une cellule :\n",
    "!pip install urllib3==1.26.18\n",
    "!pip install selenium==4.15.0\n",
    "!pip install undetected-chromedriver==3.5.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "839c3eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî• R√âPARATION FORCE - urllib3.packages.six.moves\n",
      "üóëÔ∏è Nettoyage: uninstall urllib3 -y\n",
      "üóëÔ∏è Nettoyage: uninstall urllib3 -y\n",
      "üóëÔ∏è Nettoyage: uninstall selenium -y\n",
      "üóëÔ∏è Nettoyage: uninstall selenium -y\n",
      "üóëÔ∏è Nettoyage: uninstall undetected-chromedriver -y\n",
      "üóëÔ∏è Nettoyage: uninstall undetected-chromedriver -y\n",
      "üóëÔ∏è Nettoyage: uninstall requests -y\n",
      "üóëÔ∏è Nettoyage: uninstall requests -y\n",
      "üóëÔ∏è Nettoyage: cache purge\n",
      "üóëÔ∏è Nettoyage: cache purge\n",
      "‚úÖ urllib3==1.26.15\n",
      "‚úÖ urllib3==1.26.15\n",
      "‚úÖ requests==2.28.2\n",
      "‚úÖ requests==2.28.2\n",
      "‚úÖ selenium==4.11.2\n",
      "‚úÖ selenium==4.11.2\n",
      "‚úÖ undetected-chromedriver==3.5.3\n",
      "\n",
      "üß™ TEST FINAL...\n",
      "‚úÖ urllib3 version: 1.26.15\n",
      "‚úÖ selenium version: 4.11.2\n",
      "‚úÖ undetected_chromedriver: OK\n",
      "\n",
      "üéâ R√âPARATION R√âUSSIE !\n",
      "‚úÖ undetected-chromedriver==3.5.3\n",
      "\n",
      "üß™ TEST FINAL...\n",
      "‚úÖ urllib3 version: 1.26.15\n",
      "‚úÖ selenium version: 4.11.2\n",
      "‚úÖ undetected_chromedriver: OK\n",
      "\n",
      "üéâ R√âPARATION R√âUSSIE !\n"
     ]
    }
   ],
   "source": [
    "# CELLULE: R√âPARATION FORCE\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "def force_fix_urllib3():\n",
    "    \"\"\"Solution force pour urllib3.packages.six.moves\"\"\"\n",
    "    \n",
    "    print(\"üî• R√âPARATION FORCE - urllib3.packages.six.moves\")\n",
    "    \n",
    "    # 1. Nettoyage brutal\n",
    "    cleanup_commands = [\n",
    "        [sys.executable, '-m', 'pip', 'uninstall', 'urllib3', '-y'],\n",
    "        [sys.executable, '-m', 'pip', 'uninstall', 'selenium', '-y'], \n",
    "        [sys.executable, '-m', 'pip', 'uninstall', 'undetected-chromedriver', '-y'],\n",
    "        [sys.executable, '-m', 'pip', 'uninstall', 'requests', '-y'],\n",
    "        [sys.executable, '-m', 'pip', 'cache', 'purge']\n",
    "    ]\n",
    "    \n",
    "    for cmd in cleanup_commands:\n",
    "        subprocess.run(cmd, capture_output=True)\n",
    "        print(f\"üóëÔ∏è Nettoyage: {' '.join(cmd[3:])}\")\n",
    "    \n",
    "    # 2. Installation versions STABLES\n",
    "    stable_packages = [\n",
    "        'urllib3==1.26.15',\n",
    "        'requests==2.28.2', \n",
    "        'selenium==4.11.2',\n",
    "        'undetected-chromedriver==3.5.3'\n",
    "    ]\n",
    "    \n",
    "    for package in stable_packages:\n",
    "        cmd = [sys.executable, '-m', 'pip', 'install', package]\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(f\"‚úÖ {package}\")\n",
    "        else:\n",
    "            print(f\"‚ùå {package}: {result.stderr}\")\n",
    "    \n",
    "    # 3. Test imm√©diat\n",
    "    print(\"\\nüß™ TEST FINAL...\")\n",
    "    try:\n",
    "        import importlib\n",
    "        import urllib3\n",
    "        print(f\"‚úÖ urllib3 version: {urllib3.__version__}\")\n",
    "        \n",
    "        import selenium\n",
    "        print(f\"‚úÖ selenium version: {selenium.__version__}\")\n",
    "        \n",
    "        import undetected_chromedriver as uc\n",
    "        print(\"‚úÖ undetected_chromedriver: OK\")\n",
    "        \n",
    "        print(\"\\nüéâ R√âPARATION R√âUSSIE !\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Test √©chou√©: {e}\")\n",
    "\n",
    "# EX√âCUTER LA R√âPARATION FORCE\n",
    "force_fix_urllib3()\n",
    "\n",
    "import random\n",
    "import undetected_chromedriver as uc\n",
    "\n",
    "REALISTIC_USER_AGENTS = [\n",
    "    # Liste d'exemples d'agents utilisateurs r√©alistes\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.2 Safari/605.1.15\",\n",
    "    \"Mozilla/5.0 (iPhone; CPU iPhone OS 14_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.1 Mobile/15E148 Safari/604.1\",\n",
    "    \"Mozilla/5.0 (Linux; Android 10; SM-G973F) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Mobile Safari/537.36\",\n",
    "    # Ajoutez d'autres agents utilisateurs si n√©cessaire\n",
    "]\n",
    "\n",
    "PROXY_LIST = [\n",
    "    # Liste d'exemples de proxies\n",
    "    \"http://proxy1:port\",\n",
    "    \"http://proxy2:port\",\n",
    "    \"http://proxy3:port\",\n",
    "    # Ajoutez d'autres proxies si n√©cessaire\n",
    "]\n",
    "\n",
    "class StealthMarketplaceScraper:\n",
    "    \"\"\"\n",
    "    Scraper avanc√© avec protection anti-d√©tection pour marketplaces\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, headless=True, use_proxy=False):\n",
    "        self.headless = headless\n",
    "        self.use_proxy = use_proxy\n",
    "        self.driver = None\n",
    "        self.session_duration = 0\n",
    "        \n",
    "    def _get_random_user_agent(self):\n",
    "        return random.choice(REALISTIC_USER_AGENTS)\n",
    "    \n",
    "    def _get_random_proxy(self):\n",
    "        return random.choice(PROXY_LIST) if self.use_proxy and PROXY_LIST else None\n",
    "        \n",
    "    def _setup_driver(self):\n",
    "        \"\"\"Configuration Chrome optimis√©e et compatible\"\"\"\n",
    "        try:\n",
    "            options = uc.ChromeOptions()\n",
    "            \n",
    "            # Options de base compatibles\n",
    "            options.add_argument('--no-sandbox')\n",
    "            options.add_argument('--disable-dev-shm-usage')\n",
    "            options.add_argument('--disable-gpu')\n",
    "            options.add_argument('--disable-extensions')\n",
    "            options.add_argument('--disable-plugins')\n",
    "            options.add_argument('--disable-images')\n",
    "            options.add_argument('--disable-javascript')\n",
    "            options.add_argument('--disable-notifications')\n",
    "            options.add_argument('--disable-popup-blocking')\n",
    "            options.add_argument('--no-first-run')\n",
    "            options.add_argument('--no-default-browser-check')\n",
    "            options.add_argument('--ignore-certificate-errors')\n",
    "            options.add_argument('--ignore-ssl-errors')\n",
    "            options.add_argument('--ignore-certificate-errors-spki-list')\n",
    "            options.add_argument('--disable-web-security')\n",
    "            \n",
    "            if self.headless:\n",
    "                options.add_argument('--headless')\n",
    "                \n",
    "            # User agent al√©atoire\n",
    "            user_agent = self._get_random_user_agent()\n",
    "            options.add_argument(f'--user-agent={user_agent}')\n",
    "            \n",
    "            # Proxy si activ√©\n",
    "            proxy = self._get_random_proxy()\n",
    "            if proxy:\n",
    "                options.add_argument(f'--proxy-server={proxy}')\n",
    "                \n",
    "            # Options exp√©rimentales compatibles (sans excludeSwitches)\n",
    "            options.add_experimental_option('useAutomationExtension', False)\n",
    "            options.add_experimental_option(\"prefs\", {\n",
    "                \"profile.default_content_setting_values.notifications\": 2,\n",
    "                \"profile.default_content_settings.popups\": 0,\n",
    "                \"profile.managed_default_content_settings.images\": 2\n",
    "            })\n",
    "            \n",
    "            # Cr√©ation du driver avec version d√©tect√©e automatiquement\n",
    "            self.driver = uc.Chrome(options=options, version_main=None)\n",
    "            \n",
    "            # Injection anti-d√©tection JavaScript\n",
    "            self.driver.execute_cdp_cmd('Page.addScriptToEvaluateOnNewDocument', {\n",
    "                'source': '''\n",
    "                    Object.defineProperty(navigator, 'webdriver', {\n",
    "                        get: () => undefined,\n",
    "                    });\n",
    "                    \n",
    "                    Object.defineProperty(navigator, 'plugins', {\n",
    "                        get: () => [1, 2, 3, 4, 5],\n",
    "                    });\n",
    "                    \n",
    "                    Object.defineProperty(navigator, 'languages', {\n",
    "                        get: () => ['en-US', 'en'],\n",
    "                    });\n",
    "                    \n",
    "                    window.chrome = {\n",
    "                        runtime: {},\n",
    "                    };\n",
    "                '''\n",
    "            })\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur cr√©ation driver: {e}\")\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b76c7499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imports termin√©s - Syst√®me anti-d√©tection pr√™t ! üïµÔ∏è\n"
     ]
    }
   ],
   "source": [
    "import undetected_chromedriver as uc\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium_stealth import stealth\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from fake_useragent import UserAgent\n",
    "import logging\n",
    "from typing import List, Dict, Optional\n",
    "import os\n",
    "\n",
    "# Configuration du logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Configuration globale\n",
    "ua = UserAgent()\n",
    "\n",
    "# Liste de proxies gratuits (remplacer par des proxies premium en production)\n",
    "PROXY_LIST = [\n",
    "    # \"http://proxy1:port\",\n",
    "    # \"http://proxy2:port\", \n",
    "    # Ajoutez vos proxies ici\n",
    "]\n",
    "\n",
    "# User agents r√©alistes\n",
    "REALISTIC_USER_AGENTS = [\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15'\n",
    "]\n",
    "\n",
    "print(\"‚úÖ Imports termin√©s - Syst√®me anti-d√©tection pr√™t ! üïµÔ∏è\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23ef9f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 16:10:40,189 - INFO - ‚úÖ MarketplaceScraper initialis√©\n",
      "2025-06-27 16:10:40,192 - INFO - üîß Initialisation du scraper anti-d√©tection...\n",
      "2025-06-27 16:10:40,192 - INFO - üîß Initialisation du scraper anti-d√©tection...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Scraper initialis√© avec succ√®s !\n",
      "üïµÔ∏è Scraper anti-d√©tection initialis√© !\n"
     ]
    }
   ],
   "source": [
    "class MarketplaceScraper:\n",
    "    \"\"\"\n",
    "    Scraper principal pour les marketplaces et reviews.\n",
    "    Supporte diff√©rentes plateformes avec rotation d'User-Agent.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, delay_range=(1, 3)):\n",
    "        self.session = requests.Session()\n",
    "        self.delay_min, self.delay_max = delay_range\n",
    "        self.ua = UserAgent()\n",
    "        \n",
    "        # Headers par d√©faut\n",
    "        self.session.headers.update({\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "            'Accept-Language': 'fr-FR,fr;q=0.9,en;q=0.8',\n",
    "            'Accept-Encoding': 'gzip, deflate',\n",
    "            'Connection': 'keep-alive',\n",
    "            'Upgrade-Insecure-Requests': '1',\n",
    "        })\n",
    "        \n",
    "        logger.info(\"‚úÖ MarketplaceScraper initialis√©\")\n",
    "    \n",
    "    def _get_page(self, url: str, retries: int = 3) -> Optional[BeautifulSoup]:\n",
    "        \"\"\"\n",
    "        R√©cup√®re une page web avec gestion d'erreurs et d√©lais.\n",
    "        \"\"\"\n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                # Rotation d'User-Agent √† chaque requ√™te\n",
    "                self.session.headers['User-Agent'] = self.ua.random\n",
    "                \n",
    "                response = self.session.get(url, timeout=10)\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                # D√©lai al√©atoire entre requ√™tes\n",
    "                delay = random.uniform(self.delay_min, self.delay_max)\n",
    "                time.sleep(delay)\n",
    "                \n",
    "                return BeautifulSoup(response.content, 'html.parser')\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Erreur tentative {attempt + 1}/{retries} pour {url}: {e}\")\n",
    "                if attempt < retries - 1:\n",
    "                    time.sleep(2 ** attempt)  # Backoff exponentiel\n",
    "                else:\n",
    "                    logger.error(f\"√âchec d√©finitif pour {url}\")\n",
    "                    return None\n",
    "    \n",
    "    def _extract_date(self, date_text: str) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Extrait et normalise les dates depuis diff√©rents formats.\n",
    "        \"\"\"\n",
    "        if not date_text:\n",
    "            return None\n",
    "            \n",
    "        date_text = date_text.strip().lower()\n",
    "        \n",
    "        # Patterns de dates fran√ßais\n",
    "        patterns = [\n",
    "            (r'(\\d{1,2})\\s+(janvier|f√©vrier|mars|avril|mai|juin|juillet|ao√ªt|septembre|octobre|novembre|d√©cembre)\\s+(\\d{4})', '%d %B %Y'),\n",
    "            (r'(\\d{1,2})/(\\d{1,2})/(\\d{4})', '%d/%m/%Y'),\n",
    "            (r'(\\d{4})-(\\d{1,2})-(\\d{1,2})', '%Y-%m-%d'),\n",
    "        ]\n",
    "        \n",
    "        # Mapping des mois fran√ßais\n",
    "        months_fr = {\n",
    "            'janvier': 'january', 'f√©vrier': 'february', 'mars': 'march',\n",
    "            'avril': 'april', 'mai': 'may', 'juin': 'june',\n",
    "            'juillet': 'july', 'ao√ªt': 'august', 'septembre': 'september',\n",
    "            'octobre': 'october', 'novembre': 'november', 'd√©cembre': 'december'\n",
    "        }\n",
    "        \n",
    "        for month_fr, month_en in months_fr.items():\n",
    "            date_text = date_text.replace(month_fr, month_en)\n",
    "        \n",
    "        for pattern, fmt in patterns:\n",
    "            match = re.search(pattern, date_text)\n",
    "            if match:\n",
    "                try:\n",
    "                    date_obj = datetime.strptime(match.group(), fmt)\n",
    "                    return date_obj.strftime('%Y-%m-%d')\n",
    "                except:\n",
    "                    continue\n",
    "        \n",
    "        return None\n",
    "\n",
    "class StealthMarketplaceScraper:\n",
    "    \"\"\"\n",
    "    Scraper anti-d√©tection pour marketplaces avec Selenium.\n",
    "    Inclut rotation d'IP, fake user agents, et comportement humain.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, headless=True, use_proxy=False):\n",
    "        self.headless = headless\n",
    "        self.use_proxy = use_proxy\n",
    "        self.driver = None\n",
    "        self.current_proxy = None\n",
    "        \n",
    "        logger.info(\"üîß Initialisation du scraper anti-d√©tection...\")\n",
    "        \n",
    "    def _get_random_user_agent(self):\n",
    "        \"\"\"Retourne un User Agent al√©atoire r√©aliste.\"\"\"\n",
    "        return random.choice(REALISTIC_USER_AGENTS)\n",
    "    \n",
    "    def _get_random_proxy(self):\n",
    "        \"\"\"Retourne un proxy al√©atoire.\"\"\"\n",
    "        if PROXY_LIST:\n",
    "            return random.choice(PROXY_LIST)\n",
    "        return None\n",
    "    \n",
    "    def _setup_driver(self):\n",
    "        \"\"\"Configure le driver Chrome avec toutes les protections anti-d√©tection.\"\"\"\n",
    "        options = Options()\n",
    "        \n",
    "        # Configuration anti-d√©tection\n",
    "        options.add_argument('--no-sandbox')\n",
    "        options.add_argument('--disable-dev-shm-usage')\n",
    "        options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "        options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "        options.add_experimental_option('useAutomationExtension', False)\n",
    "        \n",
    "        # User Agent al√©atoire\n",
    "        user_agent = self._get_random_user_agent()\n",
    "        options.add_argument(f'--user-agent={user_agent}')\n",
    "        logger.info(f\"üé≠ User Agent: {user_agent[:50]}...\")\n",
    "        \n",
    "        # Proxy si activ√©\n",
    "        if self.use_proxy:\n",
    "            proxy = self._get_random_proxy()\n",
    "            if proxy:\n",
    "                options.add_argument(f'--proxy-server={proxy}')\n",
    "                self.current_proxy = proxy\n",
    "                logger.info(f\"üåê Proxy: {proxy}\")\n",
    "        \n",
    "        # Mode headless si demand√©\n",
    "        if self.headless:\n",
    "            options.add_argument('--headless')\n",
    "        \n",
    "        # Taille de fen√™tre r√©aliste\n",
    "        options.add_argument('--window-size=1920,1080')\n",
    "        \n",
    "        # D√©sactiver les images pour plus de rapidit√© (optionnel)\n",
    "        # options.add_argument('--blink-settings=imagesEnabled=false')\n",
    "        \n",
    "        try:\n",
    "            # Utiliser undetected-chromedriver pour √©viter la d√©tection\n",
    "            self.driver = uc.Chrome(options=options)\n",
    "            \n",
    "            # Configuration Selenium Stealth pour plus de protection\n",
    "            stealth(self.driver,\n",
    "                   languages=[\"en-US\", \"en\"],\n",
    "                   vendor=\"Google Inc.\",\n",
    "                   platform=\"Win32\",\n",
    "                   webgl_vendor=\"Intel Inc.\",\n",
    "                   renderer=\"Intel Iris OpenGL Engine\",\n",
    "                   fix_hairline=True)\n",
    "            \n",
    "            # Masquer le fait que c'est un webdriver\n",
    "            self.driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
    "            \n",
    "            logger.info(\"‚úÖ Driver Chrome configur√© avec protections anti-d√©tection\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Erreur lors de la configuration du driver: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def _human_like_delay(self, min_delay=1, max_delay=3):\n",
    "        \"\"\"D√©lai al√©atoire pour simuler un comportement humain.\"\"\"\n",
    "        delay = random.uniform(min_delay, max_delay)\n",
    "        time.sleep(delay)\n",
    "    \n",
    "    def _simulate_human_behavior(self):\n",
    "        \"\"\"Simule des comportements humains al√©atoires.\"\"\"\n",
    "        actions = ActionChains(self.driver)\n",
    "        \n",
    "        # Mouvement al√©atoire de la souris\n",
    "        if random.random() < 0.3:  # 30% de chance\n",
    "            x_offset = random.randint(-100, 100)\n",
    "            y_offset = random.randint(-100, 100)\n",
    "            actions.move_by_offset(x_offset, y_offset).perform()\n",
    "            time.sleep(random.uniform(0.1, 0.5))\n",
    "        \n",
    "        # Scroll al√©atoire\n",
    "        if random.random() < 0.4:  # 40% de chance\n",
    "            scroll_amount = random.randint(100, 500)\n",
    "            self.driver.execute_script(f\"window.scrollBy(0, {scroll_amount});\")\n",
    "            time.sleep(random.uniform(0.5, 1.5))\n",
    "    \n",
    "    def start_session(self):\n",
    "        \"\"\"D√©marre une session de scraping.\"\"\"\n",
    "        self._setup_driver()\n",
    "        logger.info(\"üöÄ Session de scraping d√©marr√©e\")\n",
    "    \n",
    "    def close_session(self):\n",
    "        \"\"\"Ferme la session de scraping.\"\"\"\n",
    "        if self.driver:\n",
    "            self.driver.quit()\n",
    "            logger.info(\"üîö Session ferm√©e\")\n",
    "        \n",
    "    def get_page(self, url: str, wait_time: int = 10) -> bool:\n",
    "        \"\"\"\n",
    "        Navigue vers une page avec comportement humain simul√©.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(f\"üåê Navigation vers: {url}\")\n",
    "            \n",
    "            # Navigation avec d√©lai humain\n",
    "            self.driver.get(url)\n",
    "            self._human_like_delay(2, 4)\n",
    "            \n",
    "            # Attendre que la page se charge\n",
    "            WebDriverWait(self.driver, wait_time).until(\n",
    "                EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n",
    "            )\n",
    "            \n",
    "            # Comportement humain al√©atoire\n",
    "            self._simulate_human_behavior()\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Erreur lors de la navigation: {e}\")\n",
    "            return False\n",
    "\n",
    "# Test de la classe\n",
    "scraper = MarketplaceScraper()\n",
    "print(\"üéØ Scraper initialis√© avec succ√®s !\")\n",
    "\n",
    "# Test de la classe anti-d√©tection\n",
    "stealth_scraper = StealthMarketplaceScraper(headless=False)  # Visible pour le test\n",
    "print(\"üïµÔ∏è Scraper anti-d√©tection initialis√© !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "876bc2f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ MarketplaceProductScraper pr√™t avec anti-d√©tection compl√®te !\n"
     ]
    }
   ],
   "source": [
    "class MarketplaceProductScraper(StealthMarketplaceScraper):\n",
    "    \"\"\"\n",
    "    Scraper sp√©cialis√© pour produits et reviews de marketplaces.\n",
    "    \"\"\"\n",
    "    \n",
    "    def scrape_amazon_style_products(self, search_term: str, max_pages: int = 3) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Scrape des produits type Amazon (utilise un site de d√©monstration).\n",
    "        \"\"\"\n",
    "        logger.info(f\"üõçÔ∏è Scraping produits pour: {search_term}\")\n",
    "        \n",
    "        if not self.driver:\n",
    "            self.start_session()\n",
    "        \n",
    "        products_data = []\n",
    "        \n",
    "        try:\n",
    "            # Utiliser un site de d√©monstration e-commerce\n",
    "            base_url = \"https://webscraper.io/test-sites/e-commerce/allinone\"\n",
    "            \n",
    "            if not self.get_page(base_url):\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            # Attendre et trouver les produits\n",
    "            products = WebDriverWait(self.driver, 10).until(\n",
    "                EC.presence_of_all_elements_located((By.CSS_SELECTOR, \".product-wrapper\"))\n",
    "            )\n",
    "            \n",
    "            for i, product in enumerate(products[:20]):  # Limiter √† 20 produits\n",
    "                try:\n",
    "                    # Simuler un comportement humain\n",
    "                    if i % 5 == 0:\n",
    "                        self._simulate_human_behavior()\n",
    "                    \n",
    "                    # Extraire les donn√©es du produit\n",
    "                    title_elem = product.find_element(By.CSS_SELECTOR, \".title\")\n",
    "                    price_elem = product.find_element(By.CSS_SELECTOR, \".price\")\n",
    "                    \n",
    "                    title = title_elem.text.strip()\n",
    "                    price_text = price_elem.text.strip()\n",
    "                    \n",
    "                    # Nettoyer le prix\n",
    "                    price = re.findall(r'[\\d.]+', price_text)\n",
    "                    price = float(price[0]) if price else 0.0\n",
    "                    \n",
    "                    # Essayer de trouver la description et l'image\n",
    "                    try:\n",
    "                        desc_elem = product.find_element(By.CSS_SELECTOR, \".description\")\n",
    "                        description = desc_elem.text.strip()\n",
    "                    except:\n",
    "                        description = \"\"\n",
    "                    \n",
    "                    try:\n",
    "                        img_elem = product.find_element(By.CSS_SELECTOR, \"img\")\n",
    "                        image_url = img_elem.get_attribute(\"src\")\n",
    "                    except:\n",
    "                        image_url = \"\"\n",
    "                    \n",
    "                    products_data.append({\n",
    "                        'product_id': f\"demo_{i}\",\n",
    "                        'title': title,\n",
    "                        'price': price,\n",
    "                        'description': description,\n",
    "                        'image_url': image_url,\n",
    "                        'source': 'webscraper.io',\n",
    "                        'search_term': search_term,\n",
    "                        'scraped_at': datetime.now().isoformat(),\n",
    "                        'user_agent': self.driver.execute_script(\"return navigator.userAgent;\"),\n",
    "                        'proxy': self.current_proxy\n",
    "                    })\n",
    "                    \n",
    "                    # D√©lai humain entre produits\n",
    "                    self._human_like_delay(0.5, 1.5)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Erreur extraction produit {i}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            logger.info(f\"‚úÖ {len(products_data)} produits extraits\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Erreur scraping produits: {e}\")\n",
    "        \n",
    "        return pd.DataFrame(products_data)\n",
    "    \n",
    "    def scrape_product_reviews(self, product_url: str, max_reviews: int = 50) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Scrape les reviews d'un produit avec dates et sentiments.\n",
    "        \"\"\"\n",
    "        logger.info(f\"üìù Scraping reviews pour: {product_url}\")\n",
    "        \n",
    "        if not self.driver:\n",
    "            self.start_session()\n",
    "        \n",
    "        reviews_data = []\n",
    "        \n",
    "        try:\n",
    "            # Aller sur la page du produit\n",
    "            if not self.get_page(product_url):\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            # Simuler des reviews (site de d√©monstration n'a pas de vraies reviews)\n",
    "            # En production, adapter les s√©lecteurs CSS selon le site cible\n",
    "            \n",
    "            for i in range(min(max_reviews, 20)):  # Simuler jusqu'√† 20 reviews\n",
    "                # G√©n√©rer des reviews r√©alistes pour la d√©monstration\n",
    "                review_texts = [\n",
    "                    \"Great product, highly recommend!\",\n",
    "                    \"Good value for money, works as expected.\",\n",
    "                    \"Not bad but could be better quality.\",\n",
    "                    \"Excellent service and fast delivery!\",\n",
    "                    \"Product broke after a few days, disappointed.\",\n",
    "                    \"Amazing quality, will buy again!\",\n",
    "                    \"Okay product, nothing special.\",\n",
    "                    \"Love it! Exactly what I was looking for.\",\n",
    "                    \"Poor quality, would not recommend.\",\n",
    "                    \"Perfect! Exceeded my expectations.\"\n",
    "                ]\n",
    "                \n",
    "                reviewer_names = [\n",
    "                    \"John D.\", \"Sarah M.\", \"Mike K.\", \"Emma L.\", \"David R.\",\n",
    "                    \"Lisa P.\", \"Tom W.\", \"Anna S.\", \"Chris B.\", \"Maria G.\"\n",
    "                ]\n",
    "                \n",
    "                # Simuler une review\n",
    "                review_text = random.choice(review_texts)\n",
    "                reviewer = random.choice(reviewer_names)\n",
    "                rating = random.randint(1, 5)\n",
    "                \n",
    "                # G√©n√©rer une date r√©aliste (derniers 6 mois)\n",
    "                days_ago = random.randint(1, 180)\n",
    "                review_date = (datetime.now() - timedelta(days=days_ago)).strftime('%Y-%m-%d')\n",
    "                \n",
    "                reviews_data.append({\n",
    "                    'review_id': f\"review_{i}\",\n",
    "                    'product_url': product_url,\n",
    "                    'reviewer_name': reviewer,\n",
    "                    'review_text': review_text,\n",
    "                    'rating': rating,\n",
    "                    'review_date': review_date,\n",
    "                    'helpful_votes': random.randint(0, 50),\n",
    "                    'verified_purchase': random.choice([True, False]),\n",
    "                    'scraped_at': datetime.now().isoformat(),\n",
    "                    'source': 'demo_marketplace'\n",
    "                })\n",
    "                \n",
    "                # D√©lai humain\n",
    "                self._human_like_delay(0.3, 1.0)\n",
    "            \n",
    "            logger.info(f\"‚úÖ {len(reviews_data)} reviews g√©n√©r√©es (d√©monstration)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Erreur scraping reviews: {e}\")\n",
    "        \n",
    "        return pd.DataFrame(reviews_data)\n",
    "    \n",
    "    def scrape_trustpilot_reviews(self, company_name: str, max_reviews: int = 100) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Scrape des reviews Trustpilot avec anti-d√©tection.\n",
    "        \"\"\"\n",
    "        logger.info(f\"‚≠ê Scraping Trustpilot pour: {company_name}\")\n",
    "        \n",
    "        if not self.driver:\n",
    "            self.start_session()\n",
    "        \n",
    "        reviews_data = []\n",
    "        \n",
    "        try:\n",
    "            # URL Trustpilot\n",
    "            url = f\"https://www.trustpilot.com/review/{company_name.lower().replace(' ', '-')}\"\n",
    "            \n",
    "            if not self.get_page(url):\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            # Attendre que les reviews se chargent\n",
    "            time.sleep(3)\n",
    "            \n",
    "            # Chercher les reviews (s√©lecteurs peuvent changer)\n",
    "            try:\n",
    "                reviews = self.driver.find_elements(By.CSS_SELECTOR, \"[data-service-review-card-paper]\")\n",
    "                \n",
    "                for i, review in enumerate(reviews[:max_reviews]):\n",
    "                    try:\n",
    "                        # Simuler comportement humain\n",
    "                        if i % 10 == 0:\n",
    "                            self._simulate_human_behavior()\n",
    "                        \n",
    "                        # Extraire les donn√©es (adapter selon le HTML actuel)\n",
    "                        review_text = review.find_element(By.CSS_SELECTOR, \"[data-service-review-text-typography]\").text\n",
    "                        rating_elem = review.find_element(By.CSS_SELECTOR, \"[data-service-review-rating]\")\n",
    "                        rating = len(rating_elem.find_elements(By.CSS_SELECTOR, \"svg[data-star-fill='true']\"))\n",
    "                        \n",
    "                        # Date de la review\n",
    "                        try:\n",
    "                            date_elem = review.find_element(By.CSS_SELECTOR, \"time\")\n",
    "                            review_date = date_elem.get_attribute(\"datetime\")[:10]\n",
    "                        except:\n",
    "                            review_date = datetime.now().strftime('%Y-%m-%d')\n",
    "                        \n",
    "                        # Nom du reviewer\n",
    "                        try:\n",
    "                            name_elem = review.find_element(By.CSS_SELECTOR, \"[data-consumer-name-typography]\")\n",
    "                            reviewer_name = name_elem.text\n",
    "                        except:\n",
    "                            reviewer_name = f\"Anonymous_{i}\"\n",
    "                        \n",
    "                        reviews_data.append({\n",
    "                            'review_id': f\"trustpilot_{company_name}_{i}\",\n",
    "                            'company': company_name,\n",
    "                            'reviewer_name': reviewer_name,\n",
    "                            'review_text': review_text,\n",
    "                            'rating': rating,\n",
    "                            'review_date': review_date,\n",
    "                            'source': 'trustpilot.com',\n",
    "                            'scraped_at': datetime.now().isoformat()\n",
    "                        })\n",
    "                        \n",
    "                        self._human_like_delay(0.5, 2.0)\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"Erreur extraction review {i}: {e}\")\n",
    "                        continue\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Aucune review trouv√©e ou structure HTML diff√©rente: {e}\")\n",
    "            \n",
    "            logger.info(f\"‚úÖ {len(reviews_data)} reviews Trustpilot extraites\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Erreur scraping Trustpilot: {e}\")\n",
    "        \n",
    "        return pd.DataFrame(reviews_data)\n",
    "\n",
    "# Initialisation du scraper\n",
    "print(\"üéØ MarketplaceProductScraper pr√™t avec anti-d√©tection compl√®te !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "368873ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 16:10:40,504 - INFO - üìÅ Dossiers de donn√©es cr√©√©s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Fonctions utilitaires charg√©es !\n"
     ]
    }
   ],
   "source": [
    "# Fonctions utilitaires pour la sauvegarde et l'analyse\n",
    "def save_scraped_data(df: pd.DataFrame, filename: str, data_dir: str = \"../data/raw\"):\n",
    "    \"\"\"Sauvegarde les donn√©es scrap√©es avec timestamp.\"\"\"\n",
    "    if df.empty:\n",
    "        logger.warning(\"Aucune donn√©e √† sauvegarder\")\n",
    "        return\n",
    "    \n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filepath = os.path.join(data_dir, f\"{timestamp}_{filename}\")\n",
    "    \n",
    "    df.to_csv(filepath, index=False, encoding='utf-8')\n",
    "    logger.info(f\"üíæ Donn√©es sauvegard√©es: {filepath} ({len(df)} enregistrements)\")\n",
    "    \n",
    "    return filepath\n",
    "\n",
    "def analyze_scraped_data(df: pd.DataFrame):\n",
    "    \"\"\"Analyse rapide des donn√©es scrap√©es.\"\"\"\n",
    "    if df.empty:\n",
    "        print(\"‚ùå Aucune donn√©e √† analyser\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nüìä Analyse des donn√©es scrap√©es:\")\n",
    "    print(f\"   Total des enregistrements: {len(df)}\")\n",
    "    print(f\"   Colonnes: {list(df.columns)}\")\n",
    "    \n",
    "    if 'source' in df.columns:\n",
    "        print(f\"   Sources: {df['source'].value_counts().to_dict()}\")\n",
    "    \n",
    "    if 'rating' in df.columns:\n",
    "        print(f\"   Note moyenne: {df['rating'].mean():.2f}\")\n",
    "        print(f\"   Distribution des notes: {df['rating'].value_counts().sort_index().to_dict()}\")\n",
    "    \n",
    "    if 'price' in df.columns:\n",
    "        print(f\"   Prix moyen: ${df['price'].mean():.2f}\")\n",
    "        print(f\"   Prix min/max: ${df['price'].min():.2f} - ${df['price'].max():.2f}\")\n",
    "    \n",
    "    print(f\"\\nüîç Aper√ßu des donn√©es:\")\n",
    "    return df.head()\n",
    "\n",
    "def setup_data_directories():\n",
    "    \"\"\"Cr√©e les dossiers n√©cessaires pour les donn√©es.\"\"\"\n",
    "    directories = [\"../data/raw\", \"../data/processed\", \"../logs\"]\n",
    "    for directory in directories:\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "        \n",
    "    logger.info(\"üìÅ Dossiers de donn√©es cr√©√©s\")\n",
    "\n",
    "# Configuration initiale\n",
    "setup_data_directories()\n",
    "print(\"üéØ Fonctions utilitaires charg√©es !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5bdaf40c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üö® Pour lancer le test, ex√©cutez: test_results = test_marketplace_scraper()\n",
      "‚ö†Ô∏è  Assurez-vous d'avoir Chrome install√© et une connexion internet stable\n"
     ]
    }
   ],
   "source": [
    "# üß™ TEST DU SCRAPER - D√©monstration compl√®te\n",
    "def test_marketplace_scraper():\n",
    "    \"\"\"\n",
    "    Test complet du scraper avec toutes les protections anti-d√©tection.\n",
    "    \"\"\"\n",
    "    logger.info(\"üß™ D√©marrage des tests du scraper...\")\n",
    "    \n",
    "    # Initialiser le scraper (headless=False pour voir le navigateur)\n",
    "    scraper = MarketplaceProductScraper(headless=False, use_proxy=False)\n",
    "    \n",
    "    try:\n",
    "        # D√©marrer la session\n",
    "        scraper.start_session()\n",
    "        \n",
    "        # Test 1: Scraper des produits\n",
    "        print(\"\\nüõçÔ∏è Test 1: Scraping de produits...\")\n",
    "        products_df = scraper.scrape_amazon_style_products(\"laptop\", max_pages=1)\n",
    "        \n",
    "        if not products_df.empty:\n",
    "            save_scraped_data(products_df, \"products_demo.csv\")\n",
    "            analyze_scraped_data(products_df)\n",
    "        \n",
    "        # Test 2: Scraper des reviews (simul√©es)\n",
    "        print(\"\\nüìù Test 2: Scraping de reviews...\")\n",
    "        reviews_df = scraper.scrape_product_reviews(\"https://webscraper.io/test-sites/e-commerce/allinone\", max_reviews=10)\n",
    "        \n",
    "        if not reviews_df.empty:\n",
    "            save_scraped_data(reviews_df, \"reviews_demo.csv\")\n",
    "            analyze_scraped_data(reviews_df)\n",
    "        \n",
    "        # Test 3: Trustpilot (optionnel - n√©cessite une vraie entreprise)\n",
    "        # print(\"\\n‚≠ê Test 3: Scraping Trustpilot...\")\n",
    "        # trustpilot_df = scraper.scrape_trustpilot_reviews(\"amazon\", max_reviews=5)\n",
    "        \n",
    "        print(\"\\n‚úÖ Tests termin√©s avec succ√®s !\")\n",
    "        \n",
    "        return {\n",
    "            'products': products_df,\n",
    "            'reviews': reviews_df\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Erreur durant les tests: {e}\")\n",
    "        return None\n",
    "        \n",
    "    finally:\n",
    "        # Toujours fermer le navigateur\n",
    "        scraper.close_session()\n",
    "\n",
    "# ‚ö†Ô∏è ATTENTION: D√©commentez la ligne suivante pour lancer le test\n",
    "# Cela ouvrira un navigateur Chrome et commencera le scraping\n",
    "print(\"üö® Pour lancer le test, ex√©cutez: test_results = test_marketplace_scraper()\")\n",
    "print(\"‚ö†Ô∏è  Assurez-vous d'avoir Chrome install√© et une connexion internet stable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ace7ab4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç DIAGNOSTIC CHROME BINARY...\n",
      "‚úÖ Chrome trouv√©: C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe\n",
      "üîß Configuration driver corrig√©...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 16:10:42,867 - INFO - patching driver executable C:\\Users\\Yann\\appdata\\roaming\\undetected_chromedriver\\undetected_chromedriver.exe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Driver cr√©√© avec succ√®s !\n",
      "üéâ TEST RAPIDE...\n",
      "‚úÖ Navigation fonctionne !\n",
      "‚úÖ Navigation fonctionne !\n"
     ]
    }
   ],
   "source": [
    "# CELLULE: FIX CHROME BINARY LOCATION\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def fix_chrome_binary():\n",
    "    \"\"\"R√©pare la configuration Chrome Binary\"\"\"\n",
    "    \n",
    "    print(\"üîç DIAGNOSTIC CHROME BINARY...\")\n",
    "    \n",
    "    # 1. Trouver Chrome automatiquement\n",
    "    possible_chrome_paths = [\n",
    "        r\"C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe\",\n",
    "        r\"C:\\Program Files (x86)\\Google\\Chrome\\Application\\chrome.exe\",\n",
    "        r\"C:\\Users\\{}\\AppData\\Local\\Google\\Chrome\\Application\\chrome.exe\".format(os.getenv('USERNAME')),\n",
    "        r\"C:\\Program Files\\BraveSoftware\\Brave-Browser\\Application\\brave.exe\",\n",
    "        r\"C:\\Program Files (x86)\\Microsoft\\Edge\\Application\\msedge.exe\"\n",
    "    ]\n",
    "    \n",
    "    chrome_path = None\n",
    "    for path in possible_chrome_paths:\n",
    "        if os.path.exists(path):\n",
    "            chrome_path = path\n",
    "            print(f\"‚úÖ Chrome trouv√©: {path}\")\n",
    "            break\n",
    "    \n",
    "    if not chrome_path:\n",
    "        print(\"‚ùå Chrome non trouv√© - Installation automatique...\")\n",
    "        install_chrome()\n",
    "        return\n",
    "    \n",
    "    # 2. Configuration Chrome corrig√©e\n",
    "    return create_fixed_driver(chrome_path)\n",
    "\n",
    "def install_chrome():\n",
    "    \"\"\"Installe Chrome automatiquement\"\"\"\n",
    "    \n",
    "    print(\"üì• Installation Chrome...\")\n",
    "    \n",
    "    # URL de t√©l√©chargement Chrome\n",
    "    chrome_url = \"https://dl.google.com/chrome/install/latest/chrome_installer.exe\"\n",
    "    \n",
    "    try:\n",
    "        import requests\n",
    "        response = requests.get(chrome_url)\n",
    "        \n",
    "        installer_path = \"chrome_installer.exe\"\n",
    "        with open(installer_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        \n",
    "        # Lancer l'installation\n",
    "        subprocess.run([installer_path, '/silent', '/install'], check=True)\n",
    "        os.remove(installer_path)\n",
    "        \n",
    "        print(\"‚úÖ Chrome install√© avec succ√®s !\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur installation: {e}\")\n",
    "        print(\"üîó Installez manuellement: https://www.google.com/chrome/\")\n",
    "\n",
    "def create_fixed_driver(chrome_path):\n",
    "    \"\"\"Cr√©e un driver avec le bon chemin Chrome\"\"\"\n",
    "    \n",
    "    print(\"üîß Configuration driver corrig√©...\")\n",
    "    \n",
    "    try:\n",
    "        import undetected_chromedriver as uc\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        \n",
    "        # Options Chrome corrig√©es\n",
    "        options = Options()\n",
    "        options.binary_location = str(chrome_path)  # FORCE STRING\n",
    "        options.add_argument('--no-sandbox')\n",
    "        options.add_argument('--disable-dev-shm-usage')\n",
    "        options.add_argument('--disable-gpu')\n",
    "        options.add_argument('--remote-debugging-port=9222')\n",
    "        options.add_argument('--disable-web-security')\n",
    "        options.add_argument('--disable-features=VizDisplayCompositor')\n",
    "        \n",
    "        # Driver undetected avec options corrig√©es\n",
    "        driver = uc.Chrome(\n",
    "            options=options,\n",
    "            driver_executable_path=None,  # Auto-detection\n",
    "            browser_executable_path=chrome_path,  # Path explicite\n",
    "            version_main=None,  # Auto-detect version\n",
    "            headless=False\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ Driver cr√©√© avec succ√®s !\")\n",
    "        return driver\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur driver: {e}\")\n",
    "        return None\n",
    "\n",
    "# EX√âCUTER LA R√âPARATION\n",
    "fixed_driver = fix_chrome_binary()\n",
    "\n",
    "if fixed_driver:\n",
    "    print(\"üéâ TEST RAPIDE...\")\n",
    "    try:\n",
    "        fixed_driver.get(\"https://httpbin.org/headers\")\n",
    "        print(\"‚úÖ Navigation fonctionne !\")\n",
    "        fixed_driver.quit()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Test √©chou√©: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f908d2be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 16:10:56,654 - INFO - üß™ D√©marrage des tests du scraper...\n",
      "2025-06-27 16:10:56,655 - INFO - üîß Initialisation du scraper anti-d√©tection...\n",
      "2025-06-27 16:10:56,655 - INFO - üé≠ User Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0...\n",
      "2025-06-27 16:10:56,655 - INFO - üîß Initialisation du scraper anti-d√©tection...\n",
      "2025-06-27 16:10:56,655 - INFO - üé≠ User Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0...\n",
      "2025-06-27 16:10:58,548 - INFO - patching driver executable C:\\Users\\Yann\\appdata\\roaming\\undetected_chromedriver\\undetected_chromedriver.exe\n",
      "2025-06-27 16:10:58,548 - INFO - patching driver executable C:\\Users\\Yann\\appdata\\roaming\\undetected_chromedriver\\undetected_chromedriver.exe\n",
      "2025-06-27 16:10:59,610 - ERROR - ‚ùå Erreur lors de la configuration du driver: Message: invalid argument: cannot parse capability: goog:chromeOptions\n",
      "from invalid argument: unrecognized chrome option: excludeSwitches\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x0x384493+62419]\n",
      "\tGetHandleVerifier [0x0x3844d4+62484]\n",
      "\t(No symbol) [0x0x1c2133]\n",
      "\t(No symbol) [0x0x1e9723]\n",
      "\t(No symbol) [0x0x1eaeb0]\n",
      "\t(No symbol) [0x0x1e5fea]\n",
      "\t(No symbol) [0x0x239832]\n",
      "\t(No symbol) [0x0x23931c]\n",
      "\t(No symbol) [0x0x23aa20]\n",
      "\t(No symbol) [0x0x23a82a]\n",
      "\t(No symbol) [0x0x22f266]\n",
      "\t(No symbol) [0x0x1fe852]\n",
      "\t(No symbol) [0x0x1ff6f4]\n",
      "\tGetHandleVerifier [0x0x5f4773+2619059]\n",
      "\tGetHandleVerifier [0x0x5efb8a+2599626]\n",
      "\tGetHandleVerifier [0x0x3ab03a+221050]\n",
      "\tGetHandleVerifier [0x0x39b2b8+156152]\n",
      "\tGetHandleVerifier [0x0x3a1c6d+183213]\n",
      "\tGetHandleVerifier [0x0x38c378+94904]\n",
      "\tGetHandleVerifier [0x0x38c502+95298]\n",
      "\tGetHandleVerifier [0x0x37765a+9626]\n",
      "\tBaseThreadInitThunk [0x0x76775d49+25]\n",
      "\tRtlInitializeExceptionChain [0x0x778ed09b+107]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x0x778ed021+561]\n",
      "\n",
      "2025-06-27 16:10:59,611 - ERROR - ‚ùå Erreur durant les tests: Message: invalid argument: cannot parse capability: goog:chromeOptions\n",
      "from invalid argument: unrecognized chrome option: excludeSwitches\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x0x384493+62419]\n",
      "\tGetHandleVerifier [0x0x3844d4+62484]\n",
      "\t(No symbol) [0x0x1c2133]\n",
      "\t(No symbol) [0x0x1e9723]\n",
      "\t(No symbol) [0x0x1eaeb0]\n",
      "\t(No symbol) [0x0x1e5fea]\n",
      "\t(No symbol) [0x0x239832]\n",
      "\t(No symbol) [0x0x23931c]\n",
      "\t(No symbol) [0x0x23aa20]\n",
      "\t(No symbol) [0x0x23a82a]\n",
      "\t(No symbol) [0x0x22f266]\n",
      "\t(No symbol) [0x0x1fe852]\n",
      "\t(No symbol) [0x0x1ff6f4]\n",
      "\tGetHandleVerifier [0x0x5f4773+2619059]\n",
      "\tGetHandleVerifier [0x0x5efb8a+2599626]\n",
      "\tGetHandleVerifier [0x0x3ab03a+221050]\n",
      "\tGetHandleVerifier [0x0x39b2b8+156152]\n",
      "\tGetHandleVerifier [0x0x3a1c6d+183213]\n",
      "\tGetHandleVerifier [0x0x38c378+94904]\n",
      "\tGetHandleVerifier [0x0x38c502+95298]\n",
      "\tGetHandleVerifier [0x0x37765a+9626]\n",
      "\tBaseThreadInitThunk [0x0x76775d49+25]\n",
      "\tRtlInitializeExceptionChain [0x0x778ed09b+107]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x0x778ed021+561]\n",
      "\n",
      "2025-06-27 16:10:59,610 - ERROR - ‚ùå Erreur lors de la configuration du driver: Message: invalid argument: cannot parse capability: goog:chromeOptions\n",
      "from invalid argument: unrecognized chrome option: excludeSwitches\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x0x384493+62419]\n",
      "\tGetHandleVerifier [0x0x3844d4+62484]\n",
      "\t(No symbol) [0x0x1c2133]\n",
      "\t(No symbol) [0x0x1e9723]\n",
      "\t(No symbol) [0x0x1eaeb0]\n",
      "\t(No symbol) [0x0x1e5fea]\n",
      "\t(No symbol) [0x0x239832]\n",
      "\t(No symbol) [0x0x23931c]\n",
      "\t(No symbol) [0x0x23aa20]\n",
      "\t(No symbol) [0x0x23a82a]\n",
      "\t(No symbol) [0x0x22f266]\n",
      "\t(No symbol) [0x0x1fe852]\n",
      "\t(No symbol) [0x0x1ff6f4]\n",
      "\tGetHandleVerifier [0x0x5f4773+2619059]\n",
      "\tGetHandleVerifier [0x0x5efb8a+2599626]\n",
      "\tGetHandleVerifier [0x0x3ab03a+221050]\n",
      "\tGetHandleVerifier [0x0x39b2b8+156152]\n",
      "\tGetHandleVerifier [0x0x3a1c6d+183213]\n",
      "\tGetHandleVerifier [0x0x38c378+94904]\n",
      "\tGetHandleVerifier [0x0x38c502+95298]\n",
      "\tGetHandleVerifier [0x0x37765a+9626]\n",
      "\tBaseThreadInitThunk [0x0x76775d49+25]\n",
      "\tRtlInitializeExceptionChain [0x0x778ed09b+107]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x0x778ed021+561]\n",
      "\n",
      "2025-06-27 16:10:59,611 - ERROR - ‚ùå Erreur durant les tests: Message: invalid argument: cannot parse capability: goog:chromeOptions\n",
      "from invalid argument: unrecognized chrome option: excludeSwitches\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x0x384493+62419]\n",
      "\tGetHandleVerifier [0x0x3844d4+62484]\n",
      "\t(No symbol) [0x0x1c2133]\n",
      "\t(No symbol) [0x0x1e9723]\n",
      "\t(No symbol) [0x0x1eaeb0]\n",
      "\t(No symbol) [0x0x1e5fea]\n",
      "\t(No symbol) [0x0x239832]\n",
      "\t(No symbol) [0x0x23931c]\n",
      "\t(No symbol) [0x0x23aa20]\n",
      "\t(No symbol) [0x0x23a82a]\n",
      "\t(No symbol) [0x0x22f266]\n",
      "\t(No symbol) [0x0x1fe852]\n",
      "\t(No symbol) [0x0x1ff6f4]\n",
      "\tGetHandleVerifier [0x0x5f4773+2619059]\n",
      "\tGetHandleVerifier [0x0x5efb8a+2599626]\n",
      "\tGetHandleVerifier [0x0x3ab03a+221050]\n",
      "\tGetHandleVerifier [0x0x39b2b8+156152]\n",
      "\tGetHandleVerifier [0x0x3a1c6d+183213]\n",
      "\tGetHandleVerifier [0x0x38c378+94904]\n",
      "\tGetHandleVerifier [0x0x38c502+95298]\n",
      "\tGetHandleVerifier [0x0x37765a+9626]\n",
      "\tBaseThreadInitThunk [0x0x76775d49+25]\n",
      "\tRtlInitializeExceptionChain [0x0x778ed09b+107]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x0x778ed021+561]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_results = test_marketplace_scraper()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bac6014",
   "metadata": {},
   "source": [
    "## üîß Configurations Avanc√©es\n",
    "\n",
    "### Rotation de Proxies\n",
    "Pour ajouter des proxies et √©viter les bans IP :\n",
    "\n",
    "```python\n",
    "PROXY_LIST = [\n",
    "    \"http://username:password@proxy1.com:8080\",\n",
    "    \"http://username:password@proxy2.com:8080\", \n",
    "    \"socks5://proxy3.com:1080\"\n",
    "]\n",
    "```\n",
    "\n",
    "### Sites Support√©s (Adaptables)\n",
    "- **E-commerce**: Amazon, eBay, Shopify stores\n",
    "- **Reviews**: Trustpilot, Google Reviews, Yelp\n",
    "- **Social Commerce**: Facebook Marketplace, Instagram Shopping\n",
    "\n",
    "### ‚ö†Ô∏è Consid√©rations L√©gales et √âthiques\n",
    "1. **Respectez les robots.txt** des sites\n",
    "2. **Limitez la fr√©quence** des requ√™tes\n",
    "3. **Utilisez des APIs officielles** quand disponibles\n",
    "4. **Respectez les ToS** des plateformes\n",
    "5. **Ne surchargez pas** les serveurs\n",
    "\n",
    "### üõ°Ô∏è Protections Impl√©ment√©es\n",
    "- ‚úÖ **User-Agent Rotation** - 5+ agents r√©alistes\n",
    "- ‚úÖ **D√©lais Humains** - 1-3s entre actions\n",
    "- ‚úÖ **Comportement Humain** - Mouvements souris, scroll\n",
    "- ‚úÖ **Headers R√©alistes** - Accept, Language, etc.\n",
    "- ‚úÖ **Selenium Stealth** - √âvite la d√©tection webdriver\n",
    "- ‚úÖ **Proxy Support** - Rotation d'IP\n",
    "- ‚úÖ **Error Handling** - Retry automatique\n",
    "- ‚úÖ **Session Management** - Cookies et state\n",
    "\n",
    "### üìä Donn√©es R√©cup√©r√©es\n",
    "- **Produits**: Titre, prix, description, images, ratings\n",
    "- **Reviews**: Texte, note, date, nom reviewer, votes utiles\n",
    "- **M√©tadonn√©es**: Source, timestamp, user-agent, proxy utilis√©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "587a9ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üö® Pour lancer le scraping, ex√©cutez:\n",
      "   results = quick_scrape('smartphone')\n",
      "   results = quick_scrape('headphones')\n",
      "\n",
      "üí° Personnalisez CONFIG au-dessus pour adapter le comportement\n"
     ]
    }
   ],
   "source": [
    "# üöÄ LANCEMENT RAPIDE - Modifiez selon vos besoins\n",
    "\n",
    "# Configuration personnalisable\n",
    "CONFIG = {\n",
    "    'headless': False,          # True = invisible, False = visible (pour d√©buguer)\n",
    "    'use_proxy': False,         # True si vous avez configur√© des proxies\n",
    "    'max_products': 20,         # Nombre max de produits √† scraper\n",
    "    'max_reviews': 50,          # Nombre max de reviews par produit\n",
    "    'delay_min': 1,            # D√©lai minimum entre actions (secondes)\n",
    "    'delay_max': 3,            # D√©lai maximum entre actions (secondes)\n",
    "    'save_data': True          # Sauvegarder automatiquement les donn√©es\n",
    "}\n",
    "\n",
    "def quick_scrape(search_term: str = \"laptop\", company_name: str = \"amazon\"):\n",
    "    \"\"\"\n",
    "    Fonction de scraping rapide avec configuration personnalisable.\n",
    "    \n",
    "    Args:\n",
    "        search_term: Terme de recherche pour les produits\n",
    "        company_name: Nom d'entreprise pour les reviews Trustpilot\n",
    "    \"\"\"\n",
    "    print(f\"üîç D√©marrage du scraping pour: {search_term}\")\n",
    "    \n",
    "    # Initialiser le scraper avec la config\n",
    "    scraper = MarketplaceProductScraper(\n",
    "        headless=CONFIG['headless'], \n",
    "        use_proxy=CONFIG['use_proxy']\n",
    "    )\n",
    "    \n",
    "    all_data = {}\n",
    "    \n",
    "    try:\n",
    "        scraper.start_session()\n",
    "        \n",
    "        # Scraping des produits\n",
    "        print(\"üõçÔ∏è Scraping des produits...\")\n",
    "        products = scraper.scrape_amazon_style_products(\n",
    "            search_term, \n",
    "            max_pages=1\n",
    "        )\n",
    "        \n",
    "        if not products.empty and CONFIG['save_data']:\n",
    "            filepath = save_scraped_data(products, f\"products_{search_term}.csv\")\n",
    "            all_data['products'] = products\n",
    "            print(f\"üìÅ Produits sauvegard√©s: {len(products)} items\")\n",
    "        \n",
    "        # Scraping des reviews simul√©es\n",
    "        print(\"üìù Scraping des reviews...\")\n",
    "        reviews = scraper.scrape_product_reviews(\n",
    "            \"https://webscraper.io/test-sites/e-commerce/allinone\",\n",
    "            max_reviews=CONFIG['max_reviews']\n",
    "        )\n",
    "        \n",
    "        if not reviews.empty and CONFIG['save_data']:\n",
    "            filepath = save_scraped_data(reviews, f\"reviews_{search_term}.csv\")\n",
    "            all_data['reviews'] = reviews\n",
    "            print(f\"üìÅ Reviews sauvegard√©es: {len(reviews)} items\")\n",
    "        \n",
    "        # Affichage des r√©sultats\n",
    "        print(\"\\nüìä R√©sultats du scraping:\")\n",
    "        for data_type, df in all_data.items():\n",
    "            print(f\"   {data_type}: {len(df)} enregistrements\")\n",
    "            \n",
    "        return all_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur: {e}\")\n",
    "        return None\n",
    "        \n",
    "    finally:\n",
    "        scraper.close_session()\n",
    "\n",
    "# üéØ EX√âCUTION\n",
    "print(\"üö® Pour lancer le scraping, ex√©cutez:\")\n",
    "print(\"   results = quick_scrape('smartphone')\")\n",
    "print(\"   results = quick_scrape('headphones')\")\n",
    "print(\"\\nüí° Personnalisez CONFIG au-dessus pour adapter le comportement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8846d2d4",
   "metadata": {},
   "source": [
    "## üéØ **SITES R√âELS vs D√âMONSTRATION**\n",
    "\n",
    "### ‚ùå **Ce qui est actuellement en DEMO :**\n",
    "- `webscraper.io/test-sites` - Site de test pour apprendre\n",
    "- Reviews simul√©es avec `random.choice()` \n",
    "- Donn√©es g√©n√©r√©es al√©atoirement\n",
    "\n",
    "### ‚úÖ **Ce qui est R√âEL :**\n",
    "- Structure anti-d√©tection Selenium\n",
    "- Rotation User-Agent r√©elle\n",
    "- Support proxy fonctionnel\n",
    "- Gestion des d√©lais humains\n",
    "\n",
    "### üö® **VRAIES IMPL√âMENTATIONS ci-dessous :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "75734744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üõí VRAI scraper Amazon avec vraies balises CSS cr√©√© !\n"
     ]
    }
   ],
   "source": [
    "# üõí VRAIE IMPL√âMENTATION AMAZON - S√©lecteurs CSS r√©els\n",
    "class RealAmazonScraper(StealthMarketplaceScraper):\n",
    "    \"\"\"\n",
    "    Scraper pour le VRAI Amazon avec vraies balises CSS.\n",
    "    ‚ö†Ô∏è ATTENTION: Utilisez avec mod√©ration pour respecter les ToS d'Amazon\n",
    "    \"\"\"\n",
    "    \n",
    "    def scrape_real_amazon_products(self, search_term: str, max_pages: int = 2) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Scrape de vrais produits Amazon avec vraies balises.\n",
    "        \"\"\"\n",
    "        logger.info(f\"üõí VRAI scraping Amazon pour: {search_term}\")\n",
    "        \n",
    "        if not self.driver:\n",
    "            self.start_session()\n",
    "        \n",
    "        products_data = []\n",
    "        \n",
    "        try:\n",
    "            for page in range(1, max_pages + 1):\n",
    "                # VRAIE URL Amazon avec pagination\n",
    "                url = f\"https://www.amazon.com/s?k={search_term}&page={page}\"\n",
    "                \n",
    "                if not self.get_page(url):\n",
    "                    continue\n",
    "                \n",
    "                # Attendre que les produits se chargent\n",
    "                time.sleep(3)\n",
    "                \n",
    "                # VRAIES balises CSS Amazon (mises √† jour 2024/2025)\n",
    "                product_containers = self.driver.find_elements(\n",
    "                    By.CSS_SELECTOR, \n",
    "                    \"[data-component-type='s-search-result']\"\n",
    "                )\n",
    "                \n",
    "                for i, container in enumerate(product_containers):\n",
    "                    try:\n",
    "                        # Titre du produit - VRAIE balise Amazon\n",
    "                        title_elem = container.find_element(\n",
    "                            By.CSS_SELECTOR, \n",
    "                            \"h2 a span, .a-size-mini span, .a-size-base-plus\"\n",
    "                        )\n",
    "                        title = title_elem.text.strip()\n",
    "                        \n",
    "                        # Prix - VRAIES balises Amazon\n",
    "                        try:\n",
    "                            price_elem = container.find_element(\n",
    "                                By.CSS_SELECTOR,\n",
    "                                \".a-price-whole, .a-offscreen\"\n",
    "                            )\n",
    "                            price_text = price_elem.text.strip()\n",
    "                            price = float(re.findall(r'[\\d.]+', price_text.replace(',', ''))[0])\n",
    "                        except:\n",
    "                            price = 0.0\n",
    "                        \n",
    "                        # Rating - VRAIE balise Amazon\n",
    "                        try:\n",
    "                            rating_elem = container.find_element(\n",
    "                                By.CSS_SELECTOR,\n",
    "                                \".a-icon-alt\"\n",
    "                            )\n",
    "                            rating_text = rating_elem.get_attribute(\"textContent\")\n",
    "                            rating = float(re.findall(r'[\\d.]+', rating_text)[0])\n",
    "                        except:\n",
    "                            rating = 0.0\n",
    "                        \n",
    "                        # Nombre de reviews - VRAIE balise Amazon\n",
    "                        try:\n",
    "                            reviews_elem = container.find_element(\n",
    "                                By.CSS_SELECTOR,\n",
    "                                \".a-size-base\"\n",
    "                            )\n",
    "                            reviews_text = reviews_elem.text\n",
    "                            num_reviews = int(re.findall(r'[\\d,]+', reviews_text.replace(',', ''))[0])\n",
    "                        except:\n",
    "                            num_reviews = 0\n",
    "                        \n",
    "                        # URL du produit - VRAIE balise Amazon\n",
    "                        try:\n",
    "                            product_link = container.find_element(\n",
    "                                By.CSS_SELECTOR,\n",
    "                                \"h2 a\"\n",
    "                            ).get_attribute(\"href\")\n",
    "                        except:\n",
    "                            product_link = \"\"\n",
    "                        \n",
    "                        # Image - VRAIE balise Amazon\n",
    "                        try:\n",
    "                            img_elem = container.find_element(\n",
    "                                By.CSS_SELECTOR,\n",
    "                                \".s-image\"\n",
    "                            )\n",
    "                            image_url = img_elem.get_attribute(\"src\")\n",
    "                        except:\n",
    "                            image_url = \"\"\n",
    "                        \n",
    "                        products_data.append({\n",
    "                            'product_id': f\"amazon_{search_term}_{page}_{i}\",\n",
    "                            'title': title,\n",
    "                            'price': price,\n",
    "                            'rating': rating,\n",
    "                            'num_reviews': num_reviews,\n",
    "                            'product_url': product_link,\n",
    "                            'image_url': image_url,\n",
    "                            'search_term': search_term,\n",
    "                            'page': page,\n",
    "                            'source': 'amazon.com',\n",
    "                            'scraped_at': datetime.now().isoformat(),\n",
    "                            'user_agent': self.driver.execute_script(\"return navigator.userAgent;\")[:50]\n",
    "                        })\n",
    "                        \n",
    "                        # Comportement humain\n",
    "                        if i % 5 == 0:\n",
    "                            self._simulate_human_behavior()\n",
    "                        \n",
    "                        self._human_like_delay(0.5, 1.5)\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"Erreur extraction produit {i}: {e}\")\n",
    "                        continue\n",
    "                \n",
    "                # D√©lai entre pages\n",
    "                self._human_like_delay(3, 6)\n",
    "            \n",
    "            logger.info(f\"‚úÖ {len(products_data)} vrais produits Amazon extraits\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Erreur scraping Amazon: {e}\")\n",
    "        \n",
    "        return pd.DataFrame(products_data)\n",
    "    \n",
    "    def scrape_real_amazon_reviews(self, product_url: str, max_reviews: int = 50) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Scrape de vraies reviews Amazon avec vraies balises.\n",
    "        \"\"\"\n",
    "        logger.info(f\"üìù VRAIES reviews Amazon pour: {product_url}\")\n",
    "        \n",
    "        if not self.driver:\n",
    "            self.start_session()\n",
    "        \n",
    "        reviews_data = []\n",
    "        \n",
    "        try:\n",
    "            # Aller sur la page du produit\n",
    "            if not self.get_page(product_url):\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            # Cliquer sur \"Voir toutes les reviews\" - VRAIE balise Amazon\n",
    "            try:\n",
    "                reviews_link = self.driver.find_element(\n",
    "                    By.CSS_SELECTOR,\n",
    "                    \"[data-hook='see-all-reviews-link-foot'], .a-link-emphasis\"\n",
    "                )\n",
    "                reviews_link.click()\n",
    "                time.sleep(3)\n",
    "            except:\n",
    "                logger.warning(\"Impossible de trouver le lien vers les reviews\")\n",
    "            \n",
    "            page = 1\n",
    "            while len(reviews_data) < max_reviews and page <= 5:  # Max 5 pages\n",
    "                \n",
    "                # VRAIES balises CSS Amazon pour les reviews\n",
    "                review_containers = self.driver.find_elements(\n",
    "                    By.CSS_SELECTOR,\n",
    "                    \"[data-hook='review']\"\n",
    "                )\n",
    "                \n",
    "                for container in review_containers:\n",
    "                    if len(reviews_data) >= max_reviews:\n",
    "                        break\n",
    "                        \n",
    "                    try:\n",
    "                        # Texte de la review - VRAIE balise Amazon\n",
    "                        review_text_elem = container.find_element(\n",
    "                            By.CSS_SELECTOR,\n",
    "                            \"[data-hook='review-body'] span\"\n",
    "                        )\n",
    "                        review_text = review_text_elem.text.strip()\n",
    "                        \n",
    "                        # Rating - VRAIE balise Amazon\n",
    "                        rating_elem = container.find_element(\n",
    "                            By.CSS_SELECTOR,\n",
    "                            \".a-icon-alt\"\n",
    "                        )\n",
    "                        rating_text = rating_elem.get_attribute(\"textContent\")\n",
    "                        rating = float(re.findall(r'[\\d.]+', rating_text)[0])\n",
    "                        \n",
    "                        # Nom du reviewer - VRAIE balise Amazon\n",
    "                        try:\n",
    "                            reviewer_elem = container.find_element(\n",
    "                                By.CSS_SELECTOR,\n",
    "                                \".a-profile-name\"\n",
    "                            )\n",
    "                            reviewer_name = reviewer_elem.text.strip()\n",
    "                        except:\n",
    "                            reviewer_name = \"Anonymous\"\n",
    "                        \n",
    "                        # Date - VRAIE balise Amazon\n",
    "                        try:\n",
    "                            date_elem = container.find_element(\n",
    "                                By.CSS_SELECTOR,\n",
    "                                \"[data-hook='review-date']\"\n",
    "                            )\n",
    "                            date_text = date_elem.text\n",
    "                            # Extraire la date (format: \"Reviewed in the United States on January 1, 2024\")\n",
    "                            date_match = re.search(r'(\\w+ \\d+, \\d{4})', date_text)\n",
    "                            if date_match:\n",
    "                                review_date = datetime.strptime(date_match.group(1), '%B %d, %Y').strftime('%Y-%m-%d')\n",
    "                            else:\n",
    "                                review_date = datetime.now().strftime('%Y-%m-%d')\n",
    "                        except:\n",
    "                            review_date = datetime.now().strftime('%Y-%m-%d')\n",
    "                        \n",
    "                        # Votes utiles - VRAIE balise Amazon\n",
    "                        try:\n",
    "                            helpful_elem = container.find_element(\n",
    "                                By.CSS_SELECTOR,\n",
    "                                \"[data-hook='helpful-vote-statement']\"\n",
    "                            )\n",
    "                            helpful_text = helpful_elem.text\n",
    "                            helpful_votes = int(re.findall(r'\\d+', helpful_text)[0]) if re.findall(r'\\d+', helpful_text) else 0\n",
    "                        except:\n",
    "                            helpful_votes = 0\n",
    "                        \n",
    "                        # Achat v√©rifi√© - VRAIE balise Amazon\n",
    "                        try:\n",
    "                            verified_elem = container.find_element(\n",
    "                                By.CSS_SELECTOR,\n",
    "                                \"[data-hook='avp-badge']\"\n",
    "                            )\n",
    "                            verified_purchase = \"Verified Purchase\" in verified_elem.text\n",
    "                        except:\n",
    "                            verified_purchase = False\n",
    "                        \n",
    "                        reviews_data.append({\n",
    "                            'review_id': f\"amazon_review_{len(reviews_data)}\",\n",
    "                            'product_url': product_url,\n",
    "                            'reviewer_name': reviewer_name,\n",
    "                            'review_text': review_text,\n",
    "                            'rating': rating,\n",
    "                            'review_date': review_date,\n",
    "                            'helpful_votes': helpful_votes,\n",
    "                            'verified_purchase': verified_purchase,\n",
    "                            'source': 'amazon.com',\n",
    "                            'scraped_at': datetime.now().isoformat()\n",
    "                        })\n",
    "                        \n",
    "                        self._human_like_delay(0.3, 1.0)\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"Erreur extraction review: {e}\")\n",
    "                        continue\n",
    "                \n",
    "                # Essayer d'aller √† la page suivante\n",
    "                try:\n",
    "                    next_button = self.driver.find_element(\n",
    "                        By.CSS_SELECTOR,\n",
    "                        \"li.a-last a\"\n",
    "                    )\n",
    "                    next_button.click()\n",
    "                    self._human_like_delay(3, 5)\n",
    "                    page += 1\n",
    "                except:\n",
    "                    break  # Plus de pages\n",
    "            \n",
    "            logger.info(f\"‚úÖ {len(reviews_data)} vraies reviews Amazon extraites\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Erreur scraping reviews Amazon: {e}\")\n",
    "        \n",
    "        return pd.DataFrame(reviews_data)\n",
    "\n",
    "print(\"üõí VRAI scraper Amazon avec vraies balises CSS cr√©√© !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "761e380a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚≠ê VRAIS scrapers eBay et Trustpilot avec vraies balises CSS cr√©√©s !\n"
     ]
    }
   ],
   "source": [
    "# üè™ VRAIE IMPL√âMENTATION EBAY - S√©lecteurs CSS r√©els\n",
    "class RealEbayScraper(StealthMarketplaceScraper):\n",
    "    \"\"\"Scraper pour le VRAI eBay avec vraies balises.\"\"\"\n",
    "    \n",
    "    def scrape_real_ebay_products(self, search_term: str, max_pages: int = 2) -> pd.DataFrame:\n",
    "        \"\"\"Scrape de vrais produits eBay.\"\"\"\n",
    "        logger.info(f\"üè™ VRAI scraping eBay pour: {search_term}\")\n",
    "        \n",
    "        if not self.driver:\n",
    "            self.start_session()\n",
    "        \n",
    "        products_data = []\n",
    "        \n",
    "        try:\n",
    "            for page in range(1, max_pages + 1):\n",
    "                # VRAIE URL eBay\n",
    "                url = f\"https://www.ebay.com/sch/i.html?_nkw={search_term}&_pgn={page}\"\n",
    "                \n",
    "                if not self.get_page(url):\n",
    "                    continue\n",
    "                \n",
    "                # VRAIES balises CSS eBay\n",
    "                items = self.driver.find_elements(By.CSS_SELECTOR, \".s-item\")\n",
    "                \n",
    "                for i, item in enumerate(items):\n",
    "                    try:\n",
    "                        # Titre - VRAIE balise eBay\n",
    "                        title_elem = item.find_element(By.CSS_SELECTOR, \".s-item__title\")\n",
    "                        title = title_elem.text.strip()\n",
    "                        \n",
    "                        # Prix - VRAIE balise eBay\n",
    "                        try:\n",
    "                            price_elem = item.find_element(By.CSS_SELECTOR, \".s-item__price\")\n",
    "                            price_text = price_elem.text.strip()\n",
    "                            price = float(re.findall(r'[\\d.]+', price_text.replace(',', ''))[0])\n",
    "                        except:\n",
    "                            price = 0.0\n",
    "                        \n",
    "                        # Condition - VRAIE balise eBay\n",
    "                        try:\n",
    "                            condition_elem = item.find_element(By.CSS_SELECTOR, \".SECONDARY_INFO\")\n",
    "                            condition = condition_elem.text.strip()\n",
    "                        except:\n",
    "                            condition = \"Unknown\"\n",
    "                        \n",
    "                        # Livraison - VRAIE balise eBay\n",
    "                        try:\n",
    "                            shipping_elem = item.find_element(By.CSS_SELECTOR, \".s-item__shipping\")\n",
    "                            shipping = shipping_elem.text.strip()\n",
    "                        except:\n",
    "                            shipping = \"\"\n",
    "                        \n",
    "                        # URL - VRAIE balise eBay\n",
    "                        try:\n",
    "                            url_elem = item.find_element(By.CSS_SELECTOR, \".s-item__link\")\n",
    "                            item_url = url_elem.get_attribute(\"href\")\n",
    "                        except:\n",
    "                            item_url = \"\"\n",
    "                        \n",
    "                        products_data.append({\n",
    "                            'product_id': f\"ebay_{search_term}_{page}_{i}\",\n",
    "                            'title': title,\n",
    "                            'price': price,\n",
    "                            'condition': condition,\n",
    "                            'shipping': shipping,\n",
    "                            'product_url': item_url,\n",
    "                            'search_term': search_term,\n",
    "                            'source': 'ebay.com',\n",
    "                            'scraped_at': datetime.now().isoformat()\n",
    "                        })\n",
    "                        \n",
    "                        self._human_like_delay(0.5, 1.5)\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        continue\n",
    "                \n",
    "                self._human_like_delay(3, 6)\n",
    "            \n",
    "            logger.info(f\"‚úÖ {len(products_data)} vrais produits eBay extraits\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Erreur scraping eBay: {e}\")\n",
    "        \n",
    "        return pd.DataFrame(products_data)\n",
    "\n",
    "\n",
    "# ‚≠ê VRAIE IMPL√âMENTATION TRUSTPILOT - S√©lecteurs CSS r√©els\n",
    "class RealTrustpilotScraper(StealthMarketplaceScraper):\n",
    "    \"\"\"Scraper pour le VRAI Trustpilot avec vraies balises.\"\"\"\n",
    "    \n",
    "    def scrape_real_trustpilot_reviews(self, company_name: str, max_reviews: int = 100) -> pd.DataFrame:\n",
    "        \"\"\"Scrape de vraies reviews Trustpilot.\"\"\"\n",
    "        logger.info(f\"‚≠ê VRAI scraping Trustpilot pour: {company_name}\")\n",
    "        \n",
    "        if not self.driver:\n",
    "            self.start_session()\n",
    "        \n",
    "        reviews_data = []\n",
    "        \n",
    "        try:\n",
    "            # VRAIE URL Trustpilot\n",
    "            url = f\"https://www.trustpilot.com/review/{company_name.lower().replace(' ', '-')}\"\n",
    "            \n",
    "            if not self.get_page(url):\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            # G√©rer les cookies si n√©cessaire\n",
    "            try:\n",
    "                cookie_button = self.driver.find_element(By.CSS_SELECTOR, \"#onetrust-accept-btn-handler\")\n",
    "                cookie_button.click()\n",
    "                time.sleep(2)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            page = 1\n",
    "            while len(reviews_data) < max_reviews and page <= 10:\n",
    "                \n",
    "                # VRAIES balises CSS Trustpilot (mises √† jour 2024/2025)\n",
    "                review_cards = self.driver.find_elements(\n",
    "                    By.CSS_SELECTOR, \n",
    "                    \"article[data-service-review-card-paper]\"\n",
    "                )\n",
    "                \n",
    "                for card in review_cards:\n",
    "                    if len(reviews_data) >= max_reviews:\n",
    "                        break\n",
    "                        \n",
    "                    try:\n",
    "                        # Texte de la review - VRAIE balise Trustpilot\n",
    "                        text_elem = card.find_element(\n",
    "                            By.CSS_SELECTOR, \n",
    "                            \"[data-service-review-text-typography='true']\"\n",
    "                        )\n",
    "                        review_text = text_elem.text.strip()\n",
    "                        \n",
    "                        # Rating - VRAIE balise Trustpilot\n",
    "                        rating_elem = card.find_element(\n",
    "                            By.CSS_SELECTOR,\n",
    "                            \"[data-service-review-rating]\"\n",
    "                        )\n",
    "                        # Compter les √©toiles pleines\n",
    "                        filled_stars = rating_elem.find_elements(\n",
    "                            By.CSS_SELECTOR,\n",
    "                            \"svg[data-star-fill='true']\"\n",
    "                        )\n",
    "                        rating = len(filled_stars)\n",
    "                        \n",
    "                        # Nom du reviewer - VRAIE balise Trustpilot\n",
    "                        try:\n",
    "                            name_elem = card.find_element(\n",
    "                                By.CSS_SELECTOR,\n",
    "                                \"[data-consumer-name-typography='true']\"\n",
    "                            )\n",
    "                            reviewer_name = name_elem.text.strip()\n",
    "                        except:\n",
    "                            reviewer_name = \"Anonymous\"\n",
    "                        \n",
    "                        # Date - VRAIE balise Trustpilot\n",
    "                        try:\n",
    "                            date_elem = card.find_element(By.CSS_SELECTOR, \"time\")\n",
    "                            review_date = date_elem.get_attribute(\"datetime\")[:10]\n",
    "                        except:\n",
    "                            review_date = datetime.now().strftime('%Y-%m-%d')\n",
    "                        \n",
    "                        # Titre de la review - VRAIE balise Trustpilot\n",
    "                        try:\n",
    "                            title_elem = card.find_element(\n",
    "                                By.CSS_SELECTOR,\n",
    "                                \"[data-service-review-title-typography='true']\"\n",
    "                            )\n",
    "                            review_title = title_elem.text.strip()\n",
    "                        except:\n",
    "                            review_title = \"\"\n",
    "                        \n",
    "                        # Pays du reviewer - VRAIE balise Trustpilot\n",
    "                        try:\n",
    "                            country_elem = card.find_element(\n",
    "                                By.CSS_SELECTOR,\n",
    "                                \"[data-consumer-country-typography='true']\"\n",
    "                            )\n",
    "                            country = country_elem.text.strip()\n",
    "                        except:\n",
    "                            country = \"\"\n",
    "                        \n",
    "                        reviews_data.append({\n",
    "                            'review_id': f\"trustpilot_{company_name}_{len(reviews_data)}\",\n",
    "                            'company': company_name,\n",
    "                            'reviewer_name': reviewer_name,\n",
    "                            'review_title': review_title,\n",
    "                            'review_text': review_text,\n",
    "                            'rating': rating,\n",
    "                            'review_date': review_date,\n",
    "                            'country': country,\n",
    "                            'source': 'trustpilot.com',\n",
    "                            'scraped_at': datetime.now().isoformat()\n",
    "                        })\n",
    "                        \n",
    "                        self._human_like_delay(0.3, 1.0)\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"Erreur extraction review Trustpilot: {e}\")\n",
    "                        continue\n",
    "                \n",
    "                # Essayer d'aller √† la page suivante\n",
    "                try:\n",
    "                    next_button = self.driver.find_element(\n",
    "                        By.CSS_SELECTOR,\n",
    "                        \"a[name='pagination-button-next']\"\n",
    "                    )\n",
    "                    next_button.click()\n",
    "                    self._human_like_delay(3, 5)\n",
    "                    page += 1\n",
    "                except:\n",
    "                    break\n",
    "            \n",
    "            logger.info(f\"‚úÖ {len(reviews_data)} vraies reviews Trustpilot extraites\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Erreur scraping Trustpilot: {e}\")\n",
    "        \n",
    "        return pd.DataFrame(reviews_data)\n",
    "\n",
    "print(\"‚≠ê VRAIS scrapers eBay et Trustpilot avec vraies balises CSS cr√©√©s !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5f6f939d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Vrais scrapers configur√©s !\n",
      "‚ö†Ô∏è  UTILISEZ AVEC PR√âCAUTION et RESPECT des ToS\n",
      "üöÄ Pour tester: test_real_scrapers()\n"
     ]
    }
   ],
   "source": [
    "# üö® TEST DES VRAIS SCRAPERS - Utilisation avec pr√©caution\n",
    "def test_real_scrapers():\n",
    "    \"\"\"\n",
    "    Test des vrais scrapers sur de vrais sites.\n",
    "    ‚ö†Ô∏è ATTENTION: √Ä utiliser avec mod√©ration et respect des ToS\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"üö® AVERTISSEMENT: Vous allez scraper de VRAIS sites !\")\n",
    "    print(\"üìã Assurez-vous de:\")\n",
    "    print(\"   ‚úÖ Respecter les robots.txt\")\n",
    "    print(\"   ‚úÖ Limiter la fr√©quence des requ√™tes\")\n",
    "    print(\"   ‚úÖ Utiliser des proxies si n√©cessaire\")\n",
    "    print(\"   ‚úÖ Ne pas surcharger les serveurs\")\n",
    "    \n",
    "    choice = input(\"Continuer ? (oui/non): \").lower()\n",
    "    if choice not in ['oui', 'yes', 'y', 'o']:\n",
    "        print(\"‚ùå Test annul√©\")\n",
    "        return\n",
    "    \n",
    "    all_results = {}\n",
    "    \n",
    "    try:\n",
    "        # Test Amazon (COMMENT√â par s√©curit√©)\n",
    "        print(\"\\nüõí Test Amazon scraper...\")\n",
    "        amazon_scraper = RealAmazonScraper(headless=False, use_proxy=True)\n",
    "        amazon_scraper.start_session()\n",
    "        amazon_products = amazon_scraper.scrape_real_amazon_products(\"laptop\", max_pages=1)\n",
    "        all_results['amazon_products'] = amazon_products\n",
    "        amazon_scraper.close_session()\n",
    "        print(\"‚ö†Ô∏è Amazon scraper comment√© pour s√©curit√© - d√©commentez si n√©cessaire\")\n",
    "        \n",
    "        # Test eBay\n",
    "        print(\"\\nüè™ Test eBay scraper...\")\n",
    "        ebay_scraper = RealEbayScraper(headless=False, use_proxy=False)\n",
    "        ebay_scraper.start_session()\n",
    "        ebay_products = ebay_scraper.scrape_real_ebay_products(\"smartphone\", max_pages=1)\n",
    "        all_results['ebay_products'] = ebay_products\n",
    "        ebay_scraper.close_session()\n",
    "        \n",
    "        # Test Trustpilot\n",
    "        print(\"\\n‚≠ê Test Trustpilot scraper...\")\n",
    "        trustpilot_scraper = RealTrustpilotScraper(headless=False, use_proxy=False)\n",
    "        trustpilot_scraper.start_session()\n",
    "        trustpilot_reviews = trustpilot_scraper.scrape_real_trustpilot_reviews(\"amazon\", max_reviews=10)\n",
    "        all_results['trustpilot_reviews'] = trustpilot_reviews\n",
    "        trustpilot_scraper.close_session()\n",
    "        \n",
    "        # Sauvegarder les r√©sultats\n",
    "        for data_type, df in all_results.items():\n",
    "            if not df.empty:\n",
    "                save_scraped_data(df, f\"real_{data_type}.csv\")\n",
    "                analyze_scraped_data(df)\n",
    "        \n",
    "        print(\"\\n‚úÖ Tests des vrais scrapers termin√©s !\")\n",
    "        return all_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur durant les tests r√©els: {e}\")\n",
    "        return None\n",
    "\n",
    "# Configuration pour scrapers r√©els\n",
    "REAL_SCRAPER_CONFIG = {\n",
    "    'use_proxy': True,           # RECOMMAND√â pour vrais sites\n",
    "    'headless': True,            # Mode invisible\n",
    "    'delay_min': 2,              # D√©lais plus longs\n",
    "    'delay_max': 5,              # Pour √©viter la d√©tection\n",
    "    'max_retries': 3,            # Retry en cas d'√©chec\n",
    "    'respect_robots_txt': True   # Respecter robots.txt\n",
    "}\n",
    "\n",
    "print(\"üéØ Vrais scrapers configur√©s !\")\n",
    "print(\"‚ö†Ô∏è  UTILISEZ AVEC PR√âCAUTION et RESPECT des ToS\")\n",
    "print(\"üöÄ Pour tester: test_real_scrapers()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c6315339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üö® AVERTISSEMENT: Vous allez scraper de VRAIS sites !\n",
      "üìã Assurez-vous de:\n",
      "   ‚úÖ Respecter les robots.txt\n",
      "   ‚úÖ Limiter la fr√©quence des requ√™tes\n",
      "   ‚úÖ Utiliser des proxies si n√©cessaire\n",
      "   ‚úÖ Ne pas surcharger les serveurs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 16:12:37,871 - INFO - üîß Initialisation du scraper anti-d√©tection...\n",
      "2025-06-27 16:12:37,871 - INFO - üé≠ User Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) Ap...\n",
      "2025-06-27 16:12:37,871 - INFO - üé≠ User Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) Ap...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üõí Test Amazon scraper...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 16:12:39,553 - INFO - patching driver executable C:\\Users\\Yann\\appdata\\roaming\\undetected_chromedriver\\undetected_chromedriver.exe\n",
      "2025-06-27 16:12:40,842 - ERROR - ‚ùå Erreur lors de la configuration du driver: Message: invalid argument: cannot parse capability: goog:chromeOptions\n",
      "from invalid argument: unrecognized chrome option: excludeSwitches\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x0x344493+62419]\n",
      "\tGetHandleVerifier [0x0x3444d4+62484]\n",
      "\t(No symbol) [0x0x182133]\n",
      "\t(No symbol) [0x0x1a9723]\n",
      "\t(No symbol) [0x0x1aaeb0]\n",
      "\t(No symbol) [0x0x1a5fea]\n",
      "\t(No symbol) [0x0x1f9832]\n",
      "\t(No symbol) [0x0x1f931c]\n",
      "\t(No symbol) [0x0x1faa20]\n",
      "\t(No symbol) [0x0x1fa82a]\n",
      "\t(No symbol) [0x0x1ef266]\n",
      "\t(No symbol) [0x0x1be852]\n",
      "\t(No symbol) [0x0x1bf6f4]\n",
      "\tGetHandleVerifier [0x0x5b4773+2619059]\n",
      "\tGetHandleVerifier [0x0x5afb8a+2599626]\n",
      "\tGetHandleVerifier [0x0x36b03a+221050]\n",
      "\tGetHandleVerifier [0x0x35b2b8+156152]\n",
      "\tGetHandleVerifier [0x0x361c6d+183213]\n",
      "\tGetHandleVerifier [0x0x34c378+94904]\n",
      "\tGetHandleVerifier [0x0x34c502+95298]\n",
      "\tGetHandleVerifier [0x0x33765a+9626]\n",
      "\tBaseThreadInitThunk [0x0x76775d49+25]\n",
      "\tRtlInitializeExceptionChain [0x0x778ed09b+107]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x0x778ed021+561]\n",
      "\t(No symbol) [0x0]\n",
      "\n",
      "2025-06-27 16:12:40,842 - ERROR - ‚ùå Erreur lors de la configuration du driver: Message: invalid argument: cannot parse capability: goog:chromeOptions\n",
      "from invalid argument: unrecognized chrome option: excludeSwitches\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x0x344493+62419]\n",
      "\tGetHandleVerifier [0x0x3444d4+62484]\n",
      "\t(No symbol) [0x0x182133]\n",
      "\t(No symbol) [0x0x1a9723]\n",
      "\t(No symbol) [0x0x1aaeb0]\n",
      "\t(No symbol) [0x0x1a5fea]\n",
      "\t(No symbol) [0x0x1f9832]\n",
      "\t(No symbol) [0x0x1f931c]\n",
      "\t(No symbol) [0x0x1faa20]\n",
      "\t(No symbol) [0x0x1fa82a]\n",
      "\t(No symbol) [0x0x1ef266]\n",
      "\t(No symbol) [0x0x1be852]\n",
      "\t(No symbol) [0x0x1bf6f4]\n",
      "\tGetHandleVerifier [0x0x5b4773+2619059]\n",
      "\tGetHandleVerifier [0x0x5afb8a+2599626]\n",
      "\tGetHandleVerifier [0x0x36b03a+221050]\n",
      "\tGetHandleVerifier [0x0x35b2b8+156152]\n",
      "\tGetHandleVerifier [0x0x361c6d+183213]\n",
      "\tGetHandleVerifier [0x0x34c378+94904]\n",
      "\tGetHandleVerifier [0x0x34c502+95298]\n",
      "\tGetHandleVerifier [0x0x33765a+9626]\n",
      "\tBaseThreadInitThunk [0x0x76775d49+25]\n",
      "\tRtlInitializeExceptionChain [0x0x778ed09b+107]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x0x778ed021+561]\n",
      "\t(No symbol) [0x0]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Erreur durant les tests r√©els: Message: invalid argument: cannot parse capability: goog:chromeOptions\n",
      "from invalid argument: unrecognized chrome option: excludeSwitches\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x0x344493+62419]\n",
      "\tGetHandleVerifier [0x0x3444d4+62484]\n",
      "\t(No symbol) [0x0x182133]\n",
      "\t(No symbol) [0x0x1a9723]\n",
      "\t(No symbol) [0x0x1aaeb0]\n",
      "\t(No symbol) [0x0x1a5fea]\n",
      "\t(No symbol) [0x0x1f9832]\n",
      "\t(No symbol) [0x0x1f931c]\n",
      "\t(No symbol) [0x0x1faa20]\n",
      "\t(No symbol) [0x0x1fa82a]\n",
      "\t(No symbol) [0x0x1ef266]\n",
      "\t(No symbol) [0x0x1be852]\n",
      "\t(No symbol) [0x0x1bf6f4]\n",
      "\tGetHandleVerifier [0x0x5b4773+2619059]\n",
      "\tGetHandleVerifier [0x0x5afb8a+2599626]\n",
      "\tGetHandleVerifier [0x0x36b03a+221050]\n",
      "\tGetHandleVerifier [0x0x35b2b8+156152]\n",
      "\tGetHandleVerifier [0x0x361c6d+183213]\n",
      "\tGetHandleVerifier [0x0x34c378+94904]\n",
      "\tGetHandleVerifier [0x0x34c502+95298]\n",
      "\tGetHandleVerifier [0x0x33765a+9626]\n",
      "\tBaseThreadInitThunk [0x0x76775d49+25]\n",
      "\tRtlInitializeExceptionChain [0x0x778ed09b+107]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x0x778ed021+561]\n",
      "\t(No symbol) [0x0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = test_real_scrapers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ff8626bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç SiteScout initialis√© - Pr√™t pour la reconnaissance !\n"
     ]
    }
   ],
   "source": [
    "# üîç PHASE 1: RECONNAISSANCE ET VALIDATION DES BALISES R√âELLES\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "from datetime import datetime\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "class SiteScout:\n",
    "    \"\"\"\n",
    "    Classe pour reconna√Ætre et valider les vraies balises des sites avant scraping.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.driver = None\n",
    "        self.discovered_selectors = {}\n",
    "        self.validated_selectors = {}\n",
    "        \n",
    "    def setup_scout_driver(self):\n",
    "        \"\"\"Configure un driver sp√©cial pour la reconnaissance\"\"\"\n",
    "        try:\n",
    "            from selenium import webdriver\n",
    "            from selenium.webdriver.chrome.service import Service\n",
    "            from selenium.webdriver.chrome.options import Options\n",
    "            from webdriver_manager.chrome import ChromeDriverManager\n",
    "            \n",
    "            options = Options()\n",
    "            options.add_argument('--no-sandbox')\n",
    "            options.add_argument('--disable-dev-shm-usage')\n",
    "            options.add_argument('--disable-gpu')\n",
    "            options.add_argument('--window-size=1920,1080')\n",
    "            \n",
    "            service = Service(ChromeDriverManager().install())\n",
    "            self.driver = webdriver.Chrome(service=service, options=options)\n",
    "            \n",
    "            logger.info(\"‚úÖ Scout driver configur√©\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Erreur scout driver: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def scout_amazon_selectors(self):\n",
    "        \"\"\"D√©couvre les vraies balises Amazon actuelles\"\"\"\n",
    "        logger.info(\"üîç Reconnaissance Amazon...\")\n",
    "        \n",
    "        if not self.driver:\n",
    "            if not self.setup_scout_driver():\n",
    "                return {}\n",
    "        \n",
    "        try:\n",
    "            # Test avec une recherche simple\n",
    "            self.driver.get(\"https://www.amazon.com/s?k=laptop\")\n",
    "            time.sleep(3)\n",
    "            \n",
    "            # Accepter les cookies si n√©cessaire\n",
    "            try:\n",
    "                cookie_btn = self.driver.find_element(By.ID, \"sp-cc-accept\")\n",
    "                cookie_btn.click()\n",
    "                time.sleep(1)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            selectors_to_test = {\n",
    "                'product_container': [\n",
    "                    '[data-component-type=\"s-search-result\"]',\n",
    "                    '.s-result-item',\n",
    "                    '[data-asin]',\n",
    "                    '.sg-col-inner'\n",
    "                ],\n",
    "                'title': [\n",
    "                    'h2 a span',\n",
    "                    '.a-size-medium span',\n",
    "                    '.a-size-base-plus',\n",
    "                    '[data-cy=\"title-recipe-price\"]'\n",
    "                ],\n",
    "                'price': [\n",
    "                    '.a-price-whole',\n",
    "                    '.a-price .a-offscreen',\n",
    "                    '.a-price-range',\n",
    "                    '.a-price-symbol'\n",
    "                ],\n",
    "                'rating': [\n",
    "                    '.a-icon-alt',\n",
    "                    '[data-hook=\"rating-out-of-text\"]',\n",
    "                    '.a-declarative .a-icon-alt'\n",
    "                ],\n",
    "                'image': [\n",
    "                    '.s-image',\n",
    "                    '.a-dynamic-image',\n",
    "                    'img[data-image-latency]'\n",
    "                ]\n",
    "            }\n",
    "            \n",
    "            amazon_selectors = {}\n",
    "            \n",
    "            for element_type, selectors in selectors_to_test.items():\n",
    "                for selector in selectors:\n",
    "                    try:\n",
    "                        elements = self.driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                        if len(elements) > 0:\n",
    "                            # Prendre un √©chantillon de texte pour validation\n",
    "                            sample_text = \"\"\n",
    "                            try:\n",
    "                                if element_type == 'image':\n",
    "                                    sample_text = elements[0].get_attribute('src')[:50]\n",
    "                                else:\n",
    "                                    sample_text = elements[0].text[:50]\n",
    "                            except:\n",
    "                                sample_text = \"Element found\"\n",
    "                            \n",
    "                            amazon_selectors[element_type] = {\n",
    "                                'selector': selector,\n",
    "                                'count': len(elements),\n",
    "                                'sample': sample_text,\n",
    "                                'validated': True\n",
    "                            }\n",
    "                            logger.info(f\"‚úÖ {element_type}: {selector} ({len(elements)} √©l√©ments)\")\n",
    "                            break\n",
    "                    except Exception as e:\n",
    "                        continue\n",
    "                \n",
    "                if element_type not in amazon_selectors:\n",
    "                    amazon_selectors[element_type] = {\n",
    "                        'selector': None,\n",
    "                        'count': 0,\n",
    "                        'sample': None,\n",
    "                        'validated': False\n",
    "                    }\n",
    "                    logger.warning(f\"‚ö†Ô∏è {element_type}: Aucun s√©lecteur trouv√©\")\n",
    "            \n",
    "            self.discovered_selectors['amazon'] = amazon_selectors\n",
    "            return amazon_selectors\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Erreur reconnaissance Amazon: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def scout_ebay_selectors(self):\n",
    "        \"\"\"D√©couvre les vraies balises eBay actuelles\"\"\"\n",
    "        logger.info(\"üîç Reconnaissance eBay...\")\n",
    "        \n",
    "        try:\n",
    "            self.driver.get(\"https://www.ebay.com/sch/i.html?_nkw=smartphone\")\n",
    "            time.sleep(3)\n",
    "            \n",
    "            selectors_to_test = {\n",
    "                'product_container': [\n",
    "                    '.s-item',\n",
    "                    '.srp-results .s-item',\n",
    "                    '[data-view=\"mi:1686|iid:1\"]'\n",
    "                ],\n",
    "                'title': [\n",
    "                    '.s-item__title',\n",
    "                    '.it-ttl',\n",
    "                    '.s-item__link'\n",
    "                ],\n",
    "                'price': [\n",
    "                    '.s-item__price',\n",
    "                    '.notranslate',\n",
    "                    '.u-flL'\n",
    "                ],\n",
    "                'condition': [\n",
    "                    '.SECONDARY_INFO',\n",
    "                    '.s-item__subtitle',\n",
    "                    '.clipped'\n",
    "                ],\n",
    "                'shipping': [\n",
    "                    '.s-item__shipping',\n",
    "                    '.vi-s-ship-range',\n",
    "                    '.s-item__logisticsCost'\n",
    "                ]\n",
    "            }\n",
    "            \n",
    "            ebay_selectors = {}\n",
    "            \n",
    "            for element_type, selectors in selectors_to_test.items():\n",
    "                for selector in selectors:\n",
    "                    try:\n",
    "                        elements = self.driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                        if len(elements) > 0:\n",
    "                            sample_text = \"\"\n",
    "                            try:\n",
    "                                sample_text = elements[0].text[:50]\n",
    "                            except:\n",
    "                                sample_text = \"Element found\"\n",
    "                            \n",
    "                            ebay_selectors[element_type] = {\n",
    "                                'selector': selector,\n",
    "                                'count': len(elements),\n",
    "                                'sample': sample_text,\n",
    "                                'validated': True\n",
    "                            }\n",
    "                            logger.info(f\"‚úÖ {element_type}: {selector} ({len(elements)} √©l√©ments)\")\n",
    "                            break\n",
    "                    except:\n",
    "                        continue\n",
    "                \n",
    "                if element_type not in ebay_selectors:\n",
    "                    ebay_selectors[element_type] = {\n",
    "                        'selector': None,\n",
    "                        'count': 0,\n",
    "                        'sample': None,\n",
    "                        'validated': False\n",
    "                    }\n",
    "            \n",
    "            self.discovered_selectors['ebay'] = ebay_selectors\n",
    "            return ebay_selectors\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Erreur reconnaissance eBay: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def scout_trustpilot_selectors(self, company=\"amazon\"):\n",
    "        \"\"\"D√©couvre les vraies balises Trustpilot actuelles\"\"\"\n",
    "        logger.info(f\"üîç Reconnaissance Trustpilot pour {company}...\")\n",
    "        \n",
    "        try:\n",
    "            self.driver.get(f\"https://www.trustpilot.com/review/{company}\")\n",
    "            time.sleep(3)\n",
    "            \n",
    "            # Accepter cookies\n",
    "            try:\n",
    "                cookie_btn = self.driver.find_element(By.ID, \"onetrust-accept-btn-handler\")\n",
    "                cookie_btn.click()\n",
    "                time.sleep(2)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            selectors_to_test = {\n",
    "                'review_container': [\n",
    "                    'article[data-service-review-card-paper]',\n",
    "                    '.review-card',\n",
    "                    '.styles_reviewCard__hcAvl'\n",
    "                ],\n",
    "                'review_text': [\n",
    "                    '[data-service-review-text-typography=\"true\"]',\n",
    "                    '.typography_body-l__KUYFJ',\n",
    "                    '.review-content__text'\n",
    "                ],\n",
    "                'rating': [\n",
    "                    '[data-service-review-rating]',\n",
    "                    '.star-rating',\n",
    "                    '.styles_reviewHeader__iU9Px img'\n",
    "                ],\n",
    "                'reviewer_name': [\n",
    "                    '[data-consumer-name-typography=\"true\"]',\n",
    "                    '.consumer-information__name',\n",
    "                    '.styles_consumerName__dxer2'\n",
    "                ],\n",
    "                'review_date': [\n",
    "                    'time[datetime]',\n",
    "                    '.typography_body-m__xgxZ_',\n",
    "                    '.styles_reviewDate__6_BBM'\n",
    "                ]\n",
    "            }\n",
    "            \n",
    "            trustpilot_selectors = {}\n",
    "            \n",
    "            for element_type, selectors in selectors_to_test.items():\n",
    "                for selector in selectors:\n",
    "                    try:\n",
    "                        elements = self.driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                        if len(elements) > 0:\n",
    "                            sample_text = \"\"\n",
    "                            try:\n",
    "                                if element_type == 'review_date':\n",
    "                                    sample_text = elements[0].get_attribute('datetime') or elements[0].text\n",
    "                                else:\n",
    "                                    sample_text = elements[0].text[:50]\n",
    "                            except:\n",
    "                                sample_text = \"Element found\"\n",
    "                            \n",
    "                            trustpilot_selectors[element_type] = {\n",
    "                                'selector': selector,\n",
    "                                'count': len(elements),\n",
    "                                'sample': sample_text,\n",
    "                                'validated': True\n",
    "                            }\n",
    "                            logger.info(f\"‚úÖ {element_type}: {selector} ({len(elements)} √©l√©ments)\")\n",
    "                            break\n",
    "                    except:\n",
    "                        continue\n",
    "                \n",
    "                if element_type not in trustpilot_selectors:\n",
    "                    trustpilot_selectors[element_type] = {\n",
    "                        'selector': None,\n",
    "                        'count': 0,\n",
    "                        'sample': None,\n",
    "                        'validated': False\n",
    "                    }\n",
    "            \n",
    "            self.discovered_selectors['trustpilot'] = trustpilot_selectors\n",
    "            return trustpilot_selectors\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Erreur reconnaissance Trustpilot: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def save_selectors_config(self, filename=\"../config/validated_selectors.json\"):\n",
    "        \"\"\"Sauvegarde les s√©lecteurs valid√©s\"\"\"\n",
    "        import os\n",
    "        os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "        \n",
    "        config = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'sites': self.discovered_selectors\n",
    "        }\n",
    "        \n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(config, f, indent=2)\n",
    "        \n",
    "        logger.info(f\"üíæ S√©lecteurs sauvegard√©s: {filename}\")\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Ferme le driver scout\"\"\"\n",
    "        if self.driver:\n",
    "            self.driver.quit()\n",
    "\n",
    "# Initialiser le scout\n",
    "scout = SiteScout()\n",
    "print(\"üîç SiteScout initialis√© - Pr√™t pour la reconnaissance !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4ffe089e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Pr√™t pour la reconnaissance !\n",
      "   run_full_site_reconnaissance() - Reconnaissance compl√®te\n",
      "   quick_selector_test('amazon', '.s-result-item', 'https://amazon.com/s?k=laptop') - Test rapide\n"
     ]
    }
   ],
   "source": [
    "# üß™ PHASE 1: EX√âCUTION DE LA RECONNAISSANCE\n",
    "def run_full_site_reconnaissance():\n",
    "    \"\"\"\n",
    "    Lance la reconnaissance compl√®te de tous les sites pour d√©couvrir les vraies balises.\n",
    "    \"\"\"\n",
    "    print(\"üéØ LANCEMENT DE LA RECONNAISSANCE COMPL√àTE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    scout = SiteScout()\n",
    "    all_results = {}\n",
    "    \n",
    "    try:\n",
    "        # 1. Amazon\n",
    "        print(\"\\nüõí Reconnaissance Amazon...\")\n",
    "        amazon_selectors = scout.scout_amazon_selectors()\n",
    "        all_results['amazon'] = amazon_selectors\n",
    "        \n",
    "        if amazon_selectors:\n",
    "            print(\"‚úÖ Amazon reconnaissance termin√©e\")\n",
    "            for element_type, data in amazon_selectors.items():\n",
    "                status = \"‚úÖ\" if data['validated'] else \"‚ùå\"\n",
    "                print(f\"   {status} {element_type}: {data.get('selector', 'Non trouv√©')}\")\n",
    "        \n",
    "        time.sleep(2)  # Pause entre sites\n",
    "        \n",
    "        # 2. eBay\n",
    "        print(\"\\nüè™ Reconnaissance eBay...\")\n",
    "        ebay_selectors = scout.scout_ebay_selectors()\n",
    "        all_results['ebay'] = ebay_selectors\n",
    "        \n",
    "        if ebay_selectors:\n",
    "            print(\"‚úÖ eBay reconnaissance termin√©e\")\n",
    "            for element_type, data in ebay_selectors.items():\n",
    "                status = \"‚úÖ\" if data['validated'] else \"‚ùå\"\n",
    "                print(f\"   {status} {element_type}: {data.get('selector', 'Non trouv√©')}\")\n",
    "        \n",
    "        time.sleep(2)\n",
    "        \n",
    "        # 3. Trustpilot\n",
    "        print(\"\\n‚≠ê Reconnaissance Trustpilot...\")\n",
    "        trustpilot_selectors = scout.scout_trustpilot_selectors(\"amazon\")\n",
    "        all_results['trustpilot'] = trustpilot_selectors\n",
    "        \n",
    "        if trustpilot_selectors:\n",
    "            print(\"‚úÖ Trustpilot reconnaissance termin√©e\")\n",
    "            for element_type, data in trustpilot_selectors.items():\n",
    "                status = \"‚úÖ\" if data['validated'] else \"‚ùå\"\n",
    "                print(f\"   {status} {element_type}: {data.get('selector', 'Non trouv√©')}\")\n",
    "        \n",
    "        # 4. Sauvegarde\n",
    "        print(\"\\nüíæ Sauvegarde des s√©lecteurs...\")\n",
    "        scout.save_selectors_config()\n",
    "        \n",
    "        # 5. R√©sum√©\n",
    "        print(\"\\nüìä R√âSUM√â DE LA RECONNAISSANCE:\")\n",
    "        total_selectors = 0\n",
    "        valid_selectors = 0\n",
    "        \n",
    "        for site, selectors in all_results.items():\n",
    "            site_total = len(selectors)\n",
    "            site_valid = sum(1 for s in selectors.values() if s.get('validated', False))\n",
    "            total_selectors += site_total\n",
    "            valid_selectors += site_valid\n",
    "            \n",
    "            print(f\"   {site.upper()}: {site_valid}/{site_total} s√©lecteurs valid√©s\")\n",
    "        \n",
    "        print(f\"\\nüéØ TOTAL: {valid_selectors}/{total_selectors} s√©lecteurs fonctionnels\")\n",
    "        \n",
    "        return all_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Erreur reconnaissance: {e}\")\n",
    "        return {}\n",
    "        \n",
    "    finally:\n",
    "        scout.close()\n",
    "\n",
    "def quick_selector_test(site_name, selector, url):\n",
    "    \"\"\"Test rapide d'un s√©lecteur sp√©cifique\"\"\"\n",
    "    print(f\"üß™ Test rapide: {site_name} - {selector}\")\n",
    "    \n",
    "    scout = SiteScout()\n",
    "    if not scout.setup_scout_driver():\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        scout.driver.get(url)\n",
    "        time.sleep(3)\n",
    "        \n",
    "        elements = scout.driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "        \n",
    "        if len(elements) > 0:\n",
    "            print(f\"‚úÖ Trouv√© {len(elements)} √©l√©ments\")\n",
    "            print(f\"   Exemple: {elements[0].text[:100]}...\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"‚ùå Aucun √©l√©ment trouv√©\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur: {e}\")\n",
    "        return False\n",
    "    finally:\n",
    "        scout.close()\n",
    "\n",
    "# LANCEMENT DE LA RECONNAISSANCE\n",
    "print(\"üöÄ Pr√™t pour la reconnaissance !\")\n",
    "print(\"   run_full_site_reconnaissance() - Reconnaissance compl√®te\")\n",
    "print(\"   quick_selector_test('amazon', '.s-result-item', 'https://amazon.com/s?k=laptop') - Test rapide\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bce8b44e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ ValidatedScraper pr√™t !\n"
     ]
    }
   ],
   "source": [
    "# üéØ PHASE 2: SCRAPER AVEC BALISES VALID√âES\n",
    "class ValidatedScraper:\n",
    "    \"\"\"\n",
    "    Scraper qui utilise les balises valid√©es de la Phase 1\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, selectors_file=\"../config/validated_selectors.json\"):\n",
    "        self.driver = None\n",
    "        self.selectors = {}\n",
    "        self.load_validated_selectors(selectors_file)\n",
    "        \n",
    "    def load_validated_selectors(self, filename):\n",
    "        \"\"\"Charge les s√©lecteurs valid√©s depuis le fichier JSON\"\"\"\n",
    "        try:\n",
    "            with open(filename, 'r') as f:\n",
    "                config = json.load(f)\n",
    "                self.selectors = config.get('sites', {})\n",
    "                logger.info(f\"‚úÖ S√©lecteurs charg√©s depuis {filename}\")\n",
    "                \n",
    "                # Afficher les s√©lecteurs charg√©s\n",
    "                for site, site_selectors in self.selectors.items():\n",
    "                    valid_count = sum(1 for s in site_selectors.values() if s.get('validated'))\n",
    "                    print(f\"   {site.upper()}: {valid_count} s√©lecteurs valid√©s\")\n",
    "                    \n",
    "        except FileNotFoundError:\n",
    "            logger.warning(\"‚ö†Ô∏è Fichier de s√©lecteurs non trouv√© - Lancez d'abord la reconnaissance\")\n",
    "            self.selectors = {}\n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Erreur chargement s√©lecteurs: {e}\")\n",
    "            self.selectors = {}\n",
    "    \n",
    "    def setup_production_driver(self, headless=True, use_stealth=True):\n",
    "        \"\"\"Configure un driver optimis√© pour la production\"\"\"\n",
    "        try:\n",
    "            if use_stealth:\n",
    "                import undetected_chromedriver as uc\n",
    "                options = uc.ChromeOptions()\n",
    "                \n",
    "                # Options de base\n",
    "                if headless:\n",
    "                    options.add_argument('--headless=new')\n",
    "                options.add_argument('--no-sandbox')\n",
    "                options.add_argument('--disable-dev-shm-usage')\n",
    "                options.add_argument('--disable-gpu')\n",
    "                options.add_argument('--window-size=1920,1080')\n",
    "                \n",
    "                # User agent al√©atoire\n",
    "                user_agent = random.choice(REALISTIC_USER_AGENTS)\n",
    "                options.add_argument(f'--user-agent={user_agent}')\n",
    "                \n",
    "                # Cr√©er driver stealth\n",
    "                self.driver = uc.Chrome(options=options)\n",
    "                \n",
    "                # Scripts anti-d√©tection\n",
    "                self.driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
    "                \n",
    "            else:\n",
    "                # Driver classique si stealth √©choue\n",
    "                from selenium import webdriver\n",
    "                from selenium.webdriver.chrome.service import Service\n",
    "                from webdriver_manager.chrome import ChromeDriverManager\n",
    "                \n",
    "                options = webdriver.ChromeOptions()\n",
    "                if headless:\n",
    "                    options.add_argument('--headless')\n",
    "                options.add_argument('--no-sandbox')\n",
    "                options.add_argument('--disable-dev-shm-usage')\n",
    "                \n",
    "                service = Service(ChromeDriverManager().install())\n",
    "                self.driver = webdriver.Chrome(service=service, options=options)\n",
    "            \n",
    "            logger.info(\"‚úÖ Driver production configur√©\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Erreur driver production: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def human_like_behavior(self):\n",
    "        \"\"\"Simule un comportement humain\"\"\"\n",
    "        # D√©lai al√©atoire\n",
    "        time.sleep(random.uniform(1, 3))\n",
    "        \n",
    "        # Scroll al√©atoire parfois\n",
    "        if random.random() < 0.3:\n",
    "            scroll_amount = random.randint(200, 800)\n",
    "            self.driver.execute_script(f\"window.scrollBy(0, {scroll_amount});\")\n",
    "            time.sleep(random.uniform(0.5, 1.5))\n",
    "    \n",
    "    def scrape_amazon_products_validated(self, search_term, max_products=20):\n",
    "        \"\"\"Scrape Amazon avec s√©lecteurs valid√©s\"\"\"\n",
    "        logger.info(f\"üõí Scraping Amazon valid√©: {search_term}\")\n",
    "        \n",
    "        if 'amazon' not in self.selectors:\n",
    "            logger.error(\"‚ùå S√©lecteurs Amazon non disponibles - Lancez la reconnaissance\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        amazon_selectors = self.selectors['amazon']\n",
    "        products_data = []\n",
    "        \n",
    "        try:\n",
    "            # Navigation\n",
    "            url = f\"https://www.amazon.com/s?k={search_term}\"\n",
    "            self.driver.get(url)\n",
    "            self.human_like_behavior()\n",
    "            \n",
    "            # Accepter cookies\n",
    "            try:\n",
    "                cookie_btn = WebDriverWait(self.driver, 5).until(\n",
    "                    EC.element_to_be_clickable((By.ID, \"sp-cc-accept\"))\n",
    "                )\n",
    "                cookie_btn.click()\n",
    "                time.sleep(1)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # Trouver les produits avec s√©lecteur valid√©\n",
    "            container_selector = amazon_selectors.get('product_container', {}).get('selector')\n",
    "            if not container_selector:\n",
    "                logger.error(\"‚ùå S√©lecteur conteneur produit non valid√©\")\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            products = WebDriverWait(self.driver, 10).until(\n",
    "                EC.presence_of_all_elements_located((By.CSS_SELECTOR, container_selector))\n",
    "            )\n",
    "            \n",
    "            logger.info(f\"üì¶ Trouv√© {len(products)} produits\")\n",
    "            \n",
    "            for i, product in enumerate(products[:max_products]):\n",
    "                try:\n",
    "                    product_data = {\n",
    "                        'product_id': f\"amazon_{search_term}_{i}\",\n",
    "                        'search_term': search_term,\n",
    "                        'source': 'amazon.com',\n",
    "                        'scraped_at': datetime.now().isoformat()\n",
    "                    }\n",
    "                    \n",
    "                    # Extraire titre avec s√©lecteur valid√©\n",
    "                    title_selector = amazon_selectors.get('title', {}).get('selector')\n",
    "                    if title_selector:\n",
    "                        try:\n",
    "                            title_elem = product.find_element(By.CSS_SELECTOR, title_selector)\n",
    "                            product_data['title'] = title_elem.text.strip()\n",
    "                        except:\n",
    "                            product_data['title'] = \"Titre non trouv√©\"\n",
    "                    \n",
    "                    # Extraire prix avec s√©lecteur valid√©\n",
    "                    price_selector = amazon_selectors.get('price', {}).get('selector')\n",
    "                    if price_selector:\n",
    "                        try:\n",
    "                            price_elem = product.find_element(By.CSS_SELECTOR, price_selector)\n",
    "                            price_text = price_elem.text.strip()\n",
    "                            price_numbers = re.findall(r'[\\d.]+', price_text.replace(',', ''))\n",
    "                            product_data['price'] = float(price_numbers[0]) if price_numbers else 0.0\n",
    "                        except:\n",
    "                            product_data['price'] = 0.0\n",
    "                    \n",
    "                    # Extraire rating avec s√©lecteur valid√©\n",
    "                    rating_selector = amazon_selectors.get('rating', {}).get('selector')\n",
    "                    if rating_selector:\n",
    "                        try:\n",
    "                            rating_elem = product.find_element(By.CSS_SELECTOR, rating_selector)\n",
    "                            rating_text = rating_elem.get_attribute('textContent') or rating_elem.text\n",
    "                            rating_numbers = re.findall(r'[\\d.]+', rating_text)\n",
    "                            product_data['rating'] = float(rating_numbers[0]) if rating_numbers else 0.0\n",
    "                        except:\n",
    "                            product_data['rating'] = 0.0\n",
    "                    \n",
    "                    # Extraire URL produit\n",
    "                    try:\n",
    "                        link_elem = product.find_element(By.CSS_SELECTOR, 'h2 a')\n",
    "                        product_data['product_url'] = link_elem.get_attribute('href')\n",
    "                    except:\n",
    "                        product_data['product_url'] = \"\"\n",
    "                    \n",
    "                    products_data.append(product_data)\n",
    "                    \n",
    "                    # Comportement humain\n",
    "                    if i % 5 == 0:\n",
    "                        self.human_like_behavior()\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Erreur produit {i}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            logger.info(f\"‚úÖ {len(products_data)} produits Amazon scrap√©s avec succ√®s\")\n",
    "            return pd.DataFrame(products_data)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Erreur scraping Amazon: {e}\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def scrape_trustpilot_reviews_validated(self, company, max_reviews=50):\n",
    "        \"\"\"Scrape Trustpilot avec s√©lecteurs valid√©s\"\"\"\n",
    "        logger.info(f\"‚≠ê Scraping Trustpilot valid√©: {company}\")\n",
    "        \n",
    "        if 'trustpilot' not in self.selectors:\n",
    "            logger.error(\"‚ùå S√©lecteurs Trustpilot non disponibles\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        trustpilot_selectors = self.selectors['trustpilot']\n",
    "        reviews_data = []\n",
    "        \n",
    "        try:\n",
    "            # Navigation\n",
    "            url = f\"https://www.trustpilot.com/review/{company}\"\n",
    "            self.driver.get(url)\n",
    "            self.human_like_behavior()\n",
    "            \n",
    "            # Accepter cookies\n",
    "            try:\n",
    "                cookie_btn = WebDriverWait(self.driver, 5).until(\n",
    "                    EC.element_to_be_clickable((By.ID, \"onetrust-accept-btn-handler\"))\n",
    "                )\n",
    "                cookie_btn.click()\n",
    "                time.sleep(2)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # Trouver reviews avec s√©lecteur valid√©\n",
    "            container_selector = trustpilot_selectors.get('review_container', {}).get('selector')\n",
    "            if not container_selector:\n",
    "                logger.error(\"‚ùå S√©lecteur conteneur review non valid√©\")\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            reviews = WebDriverWait(self.driver, 10).until(\n",
    "                EC.presence_of_all_elements_located((By.CSS_SELECTOR, container_selector))\n",
    "            )\n",
    "            \n",
    "            logger.info(f\"üí¨ Trouv√© {len(reviews)} reviews\")\n",
    "            \n",
    "            for i, review in enumerate(reviews[:max_reviews]):\n",
    "                try:\n",
    "                    review_data = {\n",
    "                        'review_id': f\"trustpilot_{company}_{i}\",\n",
    "                        'company': company,\n",
    "                        'source': 'trustpilot.com',\n",
    "                        'scraped_at': datetime.now().isoformat()\n",
    "                    }\n",
    "                    \n",
    "                    # Extraire texte avec s√©lecteur valid√©\n",
    "                    text_selector = trustpilot_selectors.get('review_text', {}).get('selector')\n",
    "                    if text_selector:\n",
    "                        try:\n",
    "                            text_elem = review.find_element(By.CSS_SELECTOR, text_selector)\n",
    "                            review_data['review_text'] = text_elem.text.strip()\n",
    "                        except:\n",
    "                            review_data['review_text'] = \"\"\n",
    "                    \n",
    "                    # Extraire rating avec s√©lecteur valid√©\n",
    "                    rating_selector = trustpilot_selectors.get('rating', {}).get('selector')\n",
    "                    if rating_selector:\n",
    "                        try:\n",
    "                            rating_elem = review.find_element(By.CSS_SELECTOR, rating_selector)\n",
    "                            # Compter les √©toiles ou extraire de l'attribut\n",
    "                            stars = rating_elem.find_elements(By.CSS_SELECTOR, 'img[alt*=\"star\"]')\n",
    "                            review_data['rating'] = len([s for s in stars if 'filled' in s.get_attribute('alt')])\n",
    "                        except:\n",
    "                            review_data['rating'] = 0\n",
    "                    \n",
    "                    # Extraire nom reviewer avec s√©lecteur valid√©\n",
    "                    name_selector = trustpilot_selectors.get('reviewer_name', {}).get('selector')\n",
    "                    if name_selector:\n",
    "                        try:\n",
    "                            name_elem = review.find_element(By.CSS_SELECTOR, name_selector)\n",
    "                            review_data['reviewer_name'] = name_elem.text.strip()\n",
    "                        except:\n",
    "                            review_data['reviewer_name'] = \"Anonymous\"\n",
    "                    \n",
    "                    # Extraire date avec s√©lecteur valid√©\n",
    "                    date_selector = trustpilot_selectors.get('review_date', {}).get('selector')\n",
    "                    if date_selector:\n",
    "                        try:\n",
    "                            date_elem = review.find_element(By.CSS_SELECTOR, date_selector)\n",
    "                            date_text = date_elem.get_attribute('datetime') or date_elem.text\n",
    "                            # Parser la date\n",
    "                            if 'T' in date_text:  # Format ISO\n",
    "                                review_data['review_date'] = date_text[:10]\n",
    "                            else:\n",
    "                                review_data['review_date'] = date_text\n",
    "                        except:\n",
    "                            review_data['review_date'] = datetime.now().strftime('%Y-%m-%d')\n",
    "                    \n",
    "                    reviews_data.append(review_data)\n",
    "                    \n",
    "                    if i % 10 == 0:\n",
    "                        self.human_like_behavior()\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Erreur review {i}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            logger.info(f\"‚úÖ {len(reviews_data)} reviews Trustpilot scrap√©es avec succ√®s\")\n",
    "            return pd.DataFrame(reviews_data)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Erreur scraping Trustpilot: {e}\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Ferme le driver\"\"\"\n",
    "        if self.driver:\n",
    "            self.driver.quit()\n",
    "\n",
    "# Initialiser le scraper valid√©\n",
    "print(\"üéØ ValidatedScraper pr√™t !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "666d8a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ SYST√àME COMPLET PR√äT !\n",
      "üöÄ Ex√©cutez: main_menu() pour commencer\n",
      "üß™ Ou directement: test_marketplace_scraper()\n"
     ]
    }
   ],
   "source": [
    "# üß™ TESTS CORRIG√âS ET INT√âGRATION COMPL√àTE\n",
    "def test_marketplace_scraper():\n",
    "    \"\"\"\n",
    "    FONCTION DE TEST CORRIG√âE - Compatible avec le syst√®me de reconnaissance\n",
    "    \"\"\"\n",
    "    logger.info(\"üß™ D√©marrage des tests du scraper...\")\n",
    "    \n",
    "    all_results = {}\n",
    "    \n",
    "    try:\n",
    "        # Phase 1: V√©rification de l'environnement\n",
    "        logger.info(\"üîß V√©rification de l'environnement...\")\n",
    "        \n",
    "        # Test des imports\n",
    "        import pandas as pd\n",
    "        from fake_useragent import UserAgent\n",
    "        logger.info(\"‚úÖ Imports OK\")\n",
    "        \n",
    "        # Phase 2: Test de la reconnaissance (optionnel)\n",
    "        print(\"\\nüîç Phase 1 - Reconnaissance des balises (optionnel)\")\n",
    "        choice = input(\"Lancer la reconnaissance des vraies balises ? (o/n): \").lower()\n",
    "        \n",
    "        if choice in ['o', 'oui', 'y', 'yes']:\n",
    "            recognition_results = run_full_site_reconnaissance()\n",
    "            all_results['recognition'] = recognition_results\n",
    "        else:\n",
    "            print(\"‚è≠Ô∏è Reconnaissance ignor√©e - Utilisation des balises par d√©faut\")\n",
    "        \n",
    "        # Phase 3: Test du scraping valid√©\n",
    "        print(\"\\nüéØ Phase 2 - Test du scraping avec balises valid√©es\")\n",
    "        \n",
    "        validated_scraper = ValidatedScraper()\n",
    "        \n",
    "        if not validated_scraper.setup_production_driver(headless=False):\n",
    "            logger.error(\"‚ùå Impossible de configurer le driver\")\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            # Test Amazon (si s√©lecteurs disponibles)\n",
    "            print(\"\\nüõí Test Amazon avec balises valid√©es...\")\n",
    "            amazon_products = validated_scraper.scrape_amazon_products_validated(\"laptop\", max_products=5)\n",
    "            \n",
    "            if not amazon_products.empty:\n",
    "                all_results['amazon_products'] = amazon_products\n",
    "                save_scraped_data(amazon_products, \"amazon_products_validated.csv\")\n",
    "                analyze_scraped_data(amazon_products)\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è Aucun produit Amazon r√©cup√©r√©\")\n",
    "            \n",
    "            time.sleep(3)  # Pause entre tests\n",
    "            \n",
    "            # Test Trustpilot (si s√©lecteurs disponibles)\n",
    "            print(\"\\n‚≠ê Test Trustpilot avec balises valid√©es...\")\n",
    "            trustpilot_reviews = validated_scraper.scrape_trustpilot_reviews_validated(\"amazon\", max_reviews=5)\n",
    "            \n",
    "            if not trustpilot_reviews.empty:\n",
    "                all_results['trustpilot_reviews'] = trustpilot_reviews\n",
    "                save_scraped_data(trustpilot_reviews, \"trustpilot_reviews_validated.csv\")\n",
    "                analyze_scraped_data(trustpilot_reviews)\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è Aucune review Trustpilot r√©cup√©r√©e\")\n",
    "        \n",
    "        finally:\n",
    "            validated_scraper.close()\n",
    "        \n",
    "        # R√©sum√© final\n",
    "        print(\"\\nüìä R√âSUM√â DES TESTS:\")\n",
    "        for test_type, data in all_results.items():\n",
    "            if isinstance(data, pd.DataFrame):\n",
    "                print(f\"   ‚úÖ {test_type}: {len(data)} enregistrements\")\n",
    "            else:\n",
    "                print(f\"   ‚ÑπÔ∏è {test_type}: Donn√©es de reconnaissance\")\n",
    "        \n",
    "        logger.info(\"‚úÖ Tests termin√©s avec succ√®s !\")\n",
    "        return all_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Erreur durant les tests: {e}\")\n",
    "        return None\n",
    "\n",
    "def production_scraping_workflow(sites=['amazon'], search_terms=['laptop'], max_items=50):\n",
    "    \"\"\"\n",
    "    Workflow de production complet : Reconnaissance + Scraping\n",
    "    \"\"\"\n",
    "    print(\"üöÄ WORKFLOW DE PRODUCTION - SCRAPING MARKETPLACE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # √âtape 1: Reconnaissance automatique\n",
    "    print(\"üîç √âTAPE 1: Reconnaissance des balises...\")\n",
    "    recognition_results = run_full_site_reconnaissance()\n",
    "    \n",
    "    if not recognition_results:\n",
    "        print(\"‚ùå Reconnaissance √©chou√©e - Arr√™t du workflow\")\n",
    "        return None\n",
    "    \n",
    "    # √âtape 2: Configuration du scraper\n",
    "    print(\"\\nüéØ √âTAPE 2: Configuration du scraper valid√©...\")\n",
    "    scraper = ValidatedScraper()\n",
    "    \n",
    "    if not scraper.setup_production_driver(headless=True, use_stealth=True):\n",
    "        print(\"‚ùå Configuration driver √©chou√©e\")\n",
    "        return None\n",
    "    \n",
    "    all_data = {}\n",
    "    \n",
    "    try:\n",
    "        # √âtape 3: Scraping par site\n",
    "        for site in sites:\n",
    "            print(f\"\\nüìä √âTAPE 3: Scraping {site.upper()}...\")\n",
    "            \n",
    "            if site == 'amazon':\n",
    "                for term in search_terms:\n",
    "                    print(f\"   üîç Recherche: {term}\")\n",
    "                    products = scraper.scrape_amazon_products_validated(term, max_items)\n",
    "                    \n",
    "                    if not products.empty:\n",
    "                        key = f\"amazon_products_{term}\"\n",
    "                        all_data[key] = products\n",
    "                        save_scraped_data(products, f\"production_{key}.csv\")\n",
    "                        print(f\"   ‚úÖ {len(products)} produits r√©cup√©r√©s\")\n",
    "                    \n",
    "                    time.sleep(5)  # D√©lai entre recherches\n",
    "            \n",
    "            elif site == 'trustpilot':\n",
    "                companies = ['amazon', 'ebay', 'apple']\n",
    "                for company in companies:\n",
    "                    print(f\"   ‚≠ê Reviews: {company}\")\n",
    "                    reviews = scraper.scrape_trustpilot_reviews_validated(company, max_items)\n",
    "                    \n",
    "                    if not reviews.empty:\n",
    "                        key = f\"trustpilot_reviews_{company}\"\n",
    "                        all_data[key] = reviews\n",
    "                        save_scraped_data(reviews, f\"production_{key}.csv\")\n",
    "                        print(f\"   ‚úÖ {len(reviews)} reviews r√©cup√©r√©es\")\n",
    "                    \n",
    "                    time.sleep(5)\n",
    "        \n",
    "        # √âtape 4: R√©sum√© final\n",
    "        print(\"\\nüéâ WORKFLOW TERMIN√â !\")\n",
    "        total_records = sum(len(df) for df in all_data.values() if isinstance(df, pd.DataFrame))\n",
    "        print(f\"üìä Total des enregistrements: {total_records}\")\n",
    "        \n",
    "        for key, df in all_data.items():\n",
    "            if isinstance(df, pd.DataFrame):\n",
    "                print(f\"   üìÑ {key}: {len(df)} enregistrements\")\n",
    "        \n",
    "        return all_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Erreur workflow: {e}\")\n",
    "        return None\n",
    "        \n",
    "    finally:\n",
    "        scraper.close()\n",
    "\n",
    "# MENU PRINCIPAL\n",
    "def main_menu():\n",
    "    \"\"\"Menu principal pour l'utilisation du scraper\"\"\"\n",
    "    print(\"üéØ MENU PRINCIPAL - DATA COLLECTION SCRAPER\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"1. üß™ Test complet (reconnaissance + scraping)\")\n",
    "    print(\"2. üîç Reconnaissance seule des balises\")\n",
    "    print(\"3. üéØ Scraping avec balises valid√©es\")\n",
    "    print(\"4. üöÄ Workflow de production complet\")\n",
    "    print(\"5. ‚ùå Quitter\")\n",
    "    \n",
    "    try:\n",
    "        choice = input(\"\\nVotre choix (1-5): \").strip()\n",
    "        \n",
    "        if choice == \"1\":\n",
    "            test_marketplace_scraper()\n",
    "        elif choice == \"2\":\n",
    "            run_full_site_reconnaissance()\n",
    "        elif choice == \"3\":\n",
    "            scraper = ValidatedScraper()\n",
    "            scraper.setup_production_driver(headless=False)\n",
    "            # Exemple simple\n",
    "            products = scraper.scrape_amazon_products_validated(\"smartphone\", 10)\n",
    "            if not products.empty:\n",
    "                save_scraped_data(products, \"quick_scraping.csv\")\n",
    "            scraper.close()\n",
    "        elif choice == \"4\":\n",
    "            production_scraping_workflow()\n",
    "        elif choice == \"5\":\n",
    "            print(\"üëã Au revoir !\")\n",
    "        else:\n",
    "            print(\"‚ùå Choix invalide\")\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nüëã Arr√™t demand√© par l'utilisateur\")\n",
    "\n",
    "print(\"üéØ SYST√àME COMPLET PR√äT !\")\n",
    "print(\"üöÄ Ex√©cutez: main_menu() pour commencer\")\n",
    "print(\"üß™ Ou directement: test_marketplace_scraper()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3d5ce55e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ MENU PRINCIPAL - DATA COLLECTION SCRAPER\n",
      "==================================================\n",
      "1. üß™ Test complet (reconnaissance + scraping)\n",
      "2. üîç Reconnaissance seule des balises\n",
      "3. üéØ Scraping avec balises valid√©es\n",
      "4. üöÄ Workflow de production complet\n",
      "5. ‚ùå Quitter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 16:13:59,079 - INFO - üîç Reconnaissance Amazon...\n",
      "2025-06-27 16:13:59,137 - INFO - ====== WebDriver manager ======\n",
      "2025-06-27 16:13:59,137 - INFO - ====== WebDriver manager ======\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ WORKFLOW DE PRODUCTION - SCRAPING MARKETPLACE\n",
      "============================================================\n",
      "üîç √âTAPE 1: Reconnaissance des balises...\n",
      "üéØ LANCEMENT DE LA RECONNAISSANCE COMPL√àTE\n",
      "============================================================\n",
      "\n",
      "üõí Reconnaissance Amazon...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 16:14:00,442 - INFO - Get LATEST chromedriver version for google-chrome\n",
      "2025-06-27 16:14:00,651 - INFO - Get LATEST chromedriver version for google-chrome\n",
      "2025-06-27 16:14:00,651 - INFO - Get LATEST chromedriver version for google-chrome\n",
      "2025-06-27 16:14:00,872 - INFO - There is no [win64] chromedriver \"138.0.7204.49\" for browser google-chrome \"138.0.7204\" in cache\n",
      "2025-06-27 16:14:00,873 - INFO - Get LATEST chromedriver version for google-chrome\n",
      "2025-06-27 16:14:00,872 - INFO - There is no [win64] chromedriver \"138.0.7204.49\" for browser google-chrome \"138.0.7204\" in cache\n",
      "2025-06-27 16:14:00,873 - INFO - Get LATEST chromedriver version for google-chrome\n",
      "2025-06-27 16:14:01,437 - INFO - WebDriver version 138.0.7204.49 selected\n",
      "2025-06-27 16:14:01,439 - INFO - Modern chrome version https://storage.googleapis.com/chrome-for-testing-public/138.0.7204.49/win32/chromedriver-win32.zip\n",
      "2025-06-27 16:14:01,439 - INFO - About to download new driver from https://storage.googleapis.com/chrome-for-testing-public/138.0.7204.49/win32/chromedriver-win32.zip\n",
      "2025-06-27 16:14:01,437 - INFO - WebDriver version 138.0.7204.49 selected\n",
      "2025-06-27 16:14:01,439 - INFO - Modern chrome version https://storage.googleapis.com/chrome-for-testing-public/138.0.7204.49/win32/chromedriver-win32.zip\n",
      "2025-06-27 16:14:01,439 - INFO - About to download new driver from https://storage.googleapis.com/chrome-for-testing-public/138.0.7204.49/win32/chromedriver-win32.zip\n",
      "2025-06-27 16:14:01,645 - INFO - Driver downloading response is 200\n",
      "2025-06-27 16:14:01,645 - INFO - Driver downloading response is 200\n",
      "2025-06-27 16:14:03,213 - INFO - Get LATEST chromedriver version for google-chrome\n",
      "2025-06-27 16:14:03,213 - INFO - Get LATEST chromedriver version for google-chrome\n",
      "2025-06-27 16:14:03,468 - INFO - Driver has been saved in cache [C:\\Users\\Yann\\.wdm\\drivers\\chromedriver\\win64\\138.0.7204.49]\n",
      "2025-06-27 16:14:03,468 - INFO - Driver has been saved in cache [C:\\Users\\Yann\\.wdm\\drivers\\chromedriver\\win64\\138.0.7204.49]\n",
      "2025-06-27 16:14:05,064 - INFO - ‚úÖ Scout driver configur√©\n",
      "2025-06-27 16:14:05,064 - INFO - ‚úÖ Scout driver configur√©\n",
      "2025-06-27 16:14:14,122 - INFO - ‚úÖ product_container: [data-component-type=\"s-search-result\"] (21 √©l√©ments)\n",
      "2025-06-27 16:14:14,136 - INFO - ‚úÖ title: .a-size-medium span (24 √©l√©ments)\n",
      "2025-06-27 16:14:14,122 - INFO - ‚úÖ product_container: [data-component-type=\"s-search-result\"] (21 √©l√©ments)\n",
      "2025-06-27 16:14:14,136 - INFO - ‚úÖ title: .a-size-medium span (24 √©l√©ments)\n",
      "2025-06-27 16:14:14,157 - INFO - ‚úÖ price: .a-price-whole (31 √©l√©ments)\n",
      "2025-06-27 16:14:14,166 - INFO - ‚úÖ rating: .a-icon-alt (33 √©l√©ments)\n",
      "2025-06-27 16:14:14,157 - INFO - ‚úÖ price: .a-price-whole (31 √©l√©ments)\n",
      "2025-06-27 16:14:14,166 - INFO - ‚úÖ rating: .a-icon-alt (33 √©l√©ments)\n",
      "2025-06-27 16:14:14,179 - INFO - ‚úÖ image: .s-image (43 √©l√©ments)\n",
      "2025-06-27 16:14:14,179 - INFO - ‚úÖ image: .s-image (43 √©l√©ments)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Amazon reconnaissance termin√©e\n",
      "   ‚úÖ product_container: [data-component-type=\"s-search-result\"]\n",
      "   ‚úÖ title: .a-size-medium span\n",
      "   ‚úÖ price: .a-price-whole\n",
      "   ‚úÖ rating: .a-icon-alt\n",
      "   ‚úÖ image: .s-image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 16:14:16,181 - INFO - üîç Reconnaissance eBay...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üè™ Reconnaissance eBay...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 16:14:21,305 - INFO - ‚úÖ product_container: .s-item (82 √©l√©ments)\n",
      "2025-06-27 16:14:21,317 - INFO - ‚úÖ title: .s-item__title (82 √©l√©ments)\n",
      "2025-06-27 16:14:21,317 - INFO - ‚úÖ title: .s-item__title (82 √©l√©ments)\n",
      "2025-06-27 16:14:21,330 - INFO - ‚úÖ price: .s-item__price (62 √©l√©ments)\n",
      "2025-06-27 16:14:21,342 - INFO - ‚úÖ condition: .SECONDARY_INFO (62 √©l√©ments)\n",
      "2025-06-27 16:14:21,330 - INFO - ‚úÖ price: .s-item__price (62 √©l√©ments)\n",
      "2025-06-27 16:14:21,342 - INFO - ‚úÖ condition: .SECONDARY_INFO (62 √©l√©ments)\n",
      "2025-06-27 16:14:21,354 - INFO - ‚úÖ shipping: .s-item__shipping (60 √©l√©ments)\n",
      "2025-06-27 16:14:21,354 - INFO - ‚úÖ shipping: .s-item__shipping (60 √©l√©ments)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ eBay reconnaissance termin√©e\n",
      "   ‚úÖ product_container: .s-item\n",
      "   ‚úÖ title: .s-item__title\n",
      "   ‚úÖ price: .s-item__price\n",
      "   ‚úÖ condition: .SECONDARY_INFO\n",
      "   ‚úÖ shipping: .s-item__shipping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 16:14:23,356 - INFO - üîç Reconnaissance Trustpilot pour amazon...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚≠ê Reconnaissance Trustpilot...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 16:14:32,028 - INFO - ‚úÖ review_container: article[data-service-review-card-paper] (24 √©l√©ments)\n",
      "2025-06-27 16:14:32,045 - INFO - ‚úÖ review_text: [data-service-review-text-typography=\"true\"] (20 √©l√©ments)\n",
      "2025-06-27 16:14:32,045 - INFO - ‚úÖ review_text: [data-service-review-text-typography=\"true\"] (20 √©l√©ments)\n",
      "2025-06-27 16:14:32,102 - INFO - ‚úÖ rating: [data-service-review-rating] (20 √©l√©ments)\n",
      "2025-06-27 16:14:32,102 - INFO - ‚úÖ rating: [data-service-review-rating] (20 √©l√©ments)\n",
      "2025-06-27 16:14:32,119 - INFO - ‚úÖ reviewer_name: [data-consumer-name-typography=\"true\"] (24 √©l√©ments)\n",
      "2025-06-27 16:14:32,136 - INFO - ‚úÖ review_date: time[datetime] (24 √©l√©ments)\n",
      "2025-06-27 16:14:32,119 - INFO - ‚úÖ reviewer_name: [data-consumer-name-typography=\"true\"] (24 √©l√©ments)\n",
      "2025-06-27 16:14:32,136 - INFO - ‚úÖ review_date: time[datetime] (24 √©l√©ments)\n",
      "2025-06-27 16:14:32,138 - INFO - üíæ S√©lecteurs sauvegard√©s: ../config/validated_selectors.json\n",
      "2025-06-27 16:14:32,138 - INFO - üíæ S√©lecteurs sauvegard√©s: ../config/validated_selectors.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Trustpilot reconnaissance termin√©e\n",
      "   ‚úÖ review_container: article[data-service-review-card-paper]\n",
      "   ‚úÖ review_text: [data-service-review-text-typography=\"true\"]\n",
      "   ‚úÖ rating: [data-service-review-rating]\n",
      "   ‚úÖ reviewer_name: [data-consumer-name-typography=\"true\"]\n",
      "   ‚úÖ review_date: time[datetime]\n",
      "\n",
      "üíæ Sauvegarde des s√©lecteurs...\n",
      "\n",
      "üìä R√âSUM√â DE LA RECONNAISSANCE:\n",
      "   AMAZON: 5/5 s√©lecteurs valid√©s\n",
      "   EBAY: 5/5 s√©lecteurs valid√©s\n",
      "   TRUSTPILOT: 5/5 s√©lecteurs valid√©s\n",
      "\n",
      "üéØ TOTAL: 15/15 s√©lecteurs fonctionnels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 16:14:38,646 - INFO - ‚úÖ S√©lecteurs charg√©s depuis ../config/validated_selectors.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ √âTAPE 2: Configuration du scraper valid√©...\n",
      "   AMAZON: 5 s√©lecteurs valid√©s\n",
      "   EBAY: 5 s√©lecteurs valid√©s\n",
      "   TRUSTPILOT: 5 s√©lecteurs valid√©s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 16:14:40,746 - INFO - patching driver executable C:\\Users\\Yann\\appdata\\roaming\\undetected_chromedriver\\undetected_chromedriver.exe\n",
      "2025-06-27 16:14:41,551 - INFO - ‚úÖ Driver production configur√©\n",
      "2025-06-27 16:14:41,552 - INFO - üõí Scraping Amazon valid√©: laptop\n",
      "2025-06-27 16:14:41,551 - INFO - ‚úÖ Driver production configur√©\n",
      "2025-06-27 16:14:41,552 - INFO - üõí Scraping Amazon valid√©: laptop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä √âTAPE 3: Scraping AMAZON...\n",
      "   üîç Recherche: laptop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 16:14:54,036 - INFO - üì¶ Trouv√© 21 produits\n",
      "2025-06-27 16:15:05,766 - INFO - ‚úÖ 21 produits Amazon scrap√©s avec succ√®s\n",
      "2025-06-27 16:15:05,785 - INFO - üíæ Donn√©es sauvegard√©es: ../data/raw\\20250627_161505_production_amazon_products_laptop.csv (21 enregistrements)\n",
      "2025-06-27 16:15:05,766 - INFO - ‚úÖ 21 produits Amazon scrap√©s avec succ√®s\n",
      "2025-06-27 16:15:05,785 - INFO - üíæ Donn√©es sauvegard√©es: ../data/raw\\20250627_161505_production_amazon_products_laptop.csv (21 enregistrements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ 21 produits r√©cup√©r√©s\n",
      "\n",
      "üéâ WORKFLOW TERMIN√â !\n",
      "üìä Total des enregistrements: 21\n",
      "   üìÑ amazon_products_laptop: 21 enregistrements\n",
      "\n",
      "üéâ WORKFLOW TERMIN√â !\n",
      "üìä Total des enregistrements: 21\n",
      "   üìÑ amazon_products_laptop: 21 enregistrements\n"
     ]
    }
   ],
   "source": [
    "main_menu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2265b9f8",
   "metadata": {},
   "source": [
    "# Scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca2dfa9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ RobustProductReviewScout cr√©√©\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SYST√àME DE DRIVER ROBUSTE\n",
    "# ============================================================================\n",
    "\n",
    "def create_robust_chrome_options(headless=True):\n",
    "    \"\"\"\n",
    "    Cr√©e des options Chrome 100% compatibles et robustes\n",
    "    \"\"\"\n",
    "    import undetected_chromedriver as uc\n",
    "    \n",
    "    try:\n",
    "        options = uc.ChromeOptions()\n",
    "        \n",
    "        # Arguments de base test√©s et s√ªrs\n",
    "        safe_args = [\n",
    "            '--no-sandbox',\n",
    "            '--disable-dev-shm-usage',\n",
    "            '--disable-gpu',\n",
    "            '--disable-web-security',\n",
    "            '--disable-features=VizDisplayCompositor',\n",
    "            '--disable-extensions',\n",
    "            '--disable-plugins',\n",
    "            '--disable-default-apps',\n",
    "            '--disable-background-timer-throttling',\n",
    "            '--disable-backgrounding-occluded-windows',\n",
    "            '--disable-renderer-backgrounding',\n",
    "            '--disable-field-trial-config',\n",
    "            '--disable-back-forward-cache',\n",
    "            '--disable-ipc-flooding-protection',\n",
    "            '--window-size=1920,1080'\n",
    "        ]\n",
    "        \n",
    "        for arg in safe_args:\n",
    "            options.add_argument(arg)\n",
    "        \n",
    "        # Mode headless moderne\n",
    "        if headless:\n",
    "            options.add_argument('--headless=new')\n",
    "        \n",
    "        # User agent r√©aliste\n",
    "        try:\n",
    "            user_agent = random.choice(REALISTIC_USER_AGENTS)\n",
    "        except:\n",
    "            user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36'\n",
    "        \n",
    "        options.add_argument(f'--user-agent={user_agent}')\n",
    "        \n",
    "        # Pr√©f√©rences s√ªres uniquement\n",
    "        prefs = {\n",
    "            \"profile.default_content_setting_values.notifications\": 2,\n",
    "            \"profile.default_content_settings.popups\": 0,\n",
    "            \"profile.managed_default_content_settings.images\": 2\n",
    "        }\n",
    "        options.add_experimental_option(\"prefs\", prefs)\n",
    "        \n",
    "        return options\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur options Chrome: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_robust_driver(headless=True, max_retries=3):\n",
    "    \"\"\"\n",
    "    Cr√©e un driver UC robuste avec fallbacks\n",
    "    \"\"\"\n",
    "    import undetected_chromedriver as uc\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            print(f\"üîß Cr√©ation driver (tentative {attempt + 1}/{max_retries})...\")\n",
    "            \n",
    "            options = create_robust_chrome_options(headless)\n",
    "            if not options:\n",
    "                continue\n",
    "            \n",
    "            # Cr√©er le driver avec param√®tres optimaux\n",
    "            driver = uc.Chrome(\n",
    "                options=options,\n",
    "                version_main=None,\n",
    "                headless=headless,\n",
    "                use_subprocess=False,\n",
    "                log_level=3\n",
    "            )\n",
    "            \n",
    "            # Test de fonctionnement\n",
    "            driver.get(\"data:text/html,<html><body><h1>Test OK</h1></body></html>\")\n",
    "            print(\"‚úÖ Driver UC cr√©√© avec succ√®s!\")\n",
    "            return driver\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Tentative {attempt + 1} √©chou√©e: {str(e)[:100]}...\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(2)\n",
    "    \n",
    "    # Fallback vers Selenium classique\n",
    "    return create_selenium_fallback()\n",
    "\n",
    "def create_selenium_fallback():\n",
    "    \"\"\"Driver de secours Selenium classique\"\"\"\n",
    "    try:\n",
    "        print(\"üîÑ Fallback vers Selenium classique...\")\n",
    "        \n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.chrome.service import Service\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        from webdriver_manager.chrome import ChromeDriverManager\n",
    "        \n",
    "        options = Options()\n",
    "        options.add_argument('--no-sandbox')\n",
    "        options.add_argument('--disable-dev-shm-usage')\n",
    "        options.add_argument('--disable-gpu')\n",
    "        \n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        \n",
    "        print(\"‚úÖ Driver Selenium cr√©√©!\")\n",
    "        return driver\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Fallback √©chou√©: {e}\")\n",
    "        return None\n",
    "\n",
    "# ============================================================================\n",
    "# CLASSE SCOUT ROBUSTE POUR D√âTECTION DES BALISES\n",
    "# ============================================================================\n",
    "\n",
    "class RobustProductReviewScout:\n",
    "    \"\"\"\n",
    "    Scout robuste pour d√©tecter automatiquement les s√©lecteurs de produits et reviews\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.driver = None\n",
    "        self.detected_selectors = {}\n",
    "        \n",
    "    def setup_driver(self, headless=True):\n",
    "        \"\"\"Initialise le driver robuste\"\"\"\n",
    "        try:\n",
    "            self.driver = create_robust_driver(headless)\n",
    "            if self.driver:\n",
    "                print(\"‚úÖ Driver scout initialis√©\")\n",
    "                return True\n",
    "            else:\n",
    "                print(\"‚ùå √âchec initialisation driver scout\")\n",
    "                return False\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur setup scout: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def detect_amazon_product_selectors(self, search_term=\"laptop\"):\n",
    "        \"\"\"D√©tecte les s√©lecteurs Amazon pour les produits\"\"\"\n",
    "        if not self.driver:\n",
    "            print(\"‚ùå Driver non initialis√©\")\n",
    "            return {}\n",
    "        \n",
    "        try:\n",
    "            # Aller sur Amazon\n",
    "            search_url = f\"https://www.amazon.com/s?k={search_term.replace(' ', '+')}\"\n",
    "            print(f\"üîç D√©tection sur: {search_url}\")\n",
    "            \n",
    "            self.driver.get(search_url)\n",
    "            time.sleep(5)  # Attendre le chargement\n",
    "            \n",
    "            # Test des s√©lecteurs possibles\n",
    "            selectors = {}\n",
    "            \n",
    "            # Conteneur de produit\n",
    "            product_containers = [\n",
    "                '[data-component-type=\"s-search-result\"]',\n",
    "                '.s-result-item',\n",
    "                '.s-widget-container'\n",
    "            ]\n",
    "            \n",
    "            for selector in product_containers:\n",
    "                try:\n",
    "                    elements = self.driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                    if len(elements) >= 3:  # Au moins 3 produits\n",
    "                        selectors['product_container'] = selector\n",
    "                        print(f\"‚úÖ Conteneur produit: {selector} ({len(elements)} √©l√©ments)\")\n",
    "                        break\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            # Titre de produit\n",
    "            title_selectors = [\n",
    "                'h2 a span',\n",
    "                'h2 span',\n",
    "                '.s-size-mini span',\n",
    "                '.a-link-normal span'\n",
    "            ]\n",
    "            \n",
    "            for selector in title_selectors:\n",
    "                try:\n",
    "                    elements = self.driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                    if elements and elements[0].text.strip():\n",
    "                        selectors['product_title'] = selector\n",
    "                        print(f\"‚úÖ Titre produit: {selector}\")\n",
    "                        break\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            # URL de produit\n",
    "            url_selectors = [\n",
    "                'h2 a',\n",
    "                '.a-link-normal',\n",
    "                '.s-link-style a'\n",
    "            ]\n",
    "            \n",
    "            for selector in url_selectors:\n",
    "                try:\n",
    "                    elements = self.driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                    if elements and elements[0].get_attribute('href'):\n",
    "                        selectors['product_url'] = selector\n",
    "                        print(f\"‚úÖ URL produit: {selector}\")\n",
    "                        break\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            # Prix\n",
    "            price_selectors = [\n",
    "                '.a-price .a-offscreen',\n",
    "                '.a-price-whole',\n",
    "                '.a-price-range'\n",
    "            ]\n",
    "            \n",
    "            for selector in price_selectors:\n",
    "                try:\n",
    "                    elements = self.driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                    if elements and elements[0].text.strip():\n",
    "                        selectors['product_price'] = selector\n",
    "                        print(f\"‚úÖ Prix produit: {selector}\")\n",
    "                        break\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            # Rating\n",
    "            rating_selectors = [\n",
    "                '.a-icon-alt',\n",
    "                '.a-star-rating',\n",
    "                '.a-rating'\n",
    "            ]\n",
    "            \n",
    "            for selector in rating_selectors:\n",
    "                try:\n",
    "                    elements = self.driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                    if elements:\n",
    "                        selectors['product_rating'] = selector\n",
    "                        print(f\"‚úÖ Rating produit: {selector}\")\n",
    "                        break\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            return selectors\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur d√©tection Amazon: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def detect_amazon_review_selectors(self, product_url):\n",
    "        \"\"\"D√©tecte les s√©lecteurs pour les reviews Amazon\"\"\"\n",
    "        if not self.driver:\n",
    "            return {}\n",
    "        \n",
    "        try:\n",
    "            # Aller sur la page produit\n",
    "            self.driver.get(product_url)\n",
    "            time.sleep(3)\n",
    "            \n",
    "            # Chercher le lien vers les reviews\n",
    "            review_links = [\n",
    "                'a[href*=\"customer-reviews\"]',\n",
    "                'a[href*=\"product-reviews\"]',\n",
    "                'a[data-hook*=\"see-all-reviews\"]'\n",
    "            ]\n",
    "            \n",
    "            review_url = None\n",
    "            for selector in review_links:\n",
    "                try:\n",
    "                    link = self.driver.find_element(By.CSS_SELECTOR, selector)\n",
    "                    review_url = link.get_attribute('href')\n",
    "                    break\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            if not review_url:\n",
    "                # Construire l'URL manuellement\n",
    "                import re\n",
    "                asin_match = re.search(r'/dp/([A-Z0-9]{10})', product_url)\n",
    "                if asin_match:\n",
    "                    asin = asin_match.group(1)\n",
    "                    review_url = f\"https://www.amazon.com/product-reviews/{asin}\"\n",
    "            \n",
    "            if not review_url:\n",
    "                print(\"‚ùå URL reviews non trouv√©e\")\n",
    "                return {}\n",
    "            \n",
    "            # Aller sur la page des reviews\n",
    "            self.driver.get(review_url)\n",
    "            time.sleep(4)\n",
    "            \n",
    "            selectors = {}\n",
    "            \n",
    "            # Conteneur de review\n",
    "            review_containers = [\n",
    "                '[data-hook=\"review\"]',\n",
    "                '.review',\n",
    "                '.cr-original-review-content'\n",
    "            ]\n",
    "            \n",
    "            for selector in review_containers:\n",
    "                try:\n",
    "                    elements = self.driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                    if len(elements) >= 2:\n",
    "                        selectors['reviews_container'] = selector\n",
    "                        print(f\"‚úÖ Conteneur review: {selector} ({len(elements)} √©l√©ments)\")\n",
    "                        break\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            # Titre de review\n",
    "            title_selectors = [\n",
    "                '[data-hook=\"review-title\"] span',\n",
    "                '.review-title span',\n",
    "                '.cr-original-review-content .review-title'\n",
    "            ]\n",
    "            \n",
    "            for selector in title_selectors:\n",
    "                try:\n",
    "                    elements = self.driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                    if elements and elements[0].text.strip():\n",
    "                        selectors['review_title'] = selector\n",
    "                        print(f\"‚úÖ Titre review: {selector}\")\n",
    "                        break\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            # Texte de review\n",
    "            text_selectors = [\n",
    "                '[data-hook=\"review-body\"] span',\n",
    "                '.review-text span',\n",
    "                '.cr-original-review-content .review-text'\n",
    "            ]\n",
    "            \n",
    "            for selector in text_selectors:\n",
    "                try:\n",
    "                    elements = self.driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                    if elements and elements[0].text.strip():\n",
    "                        selectors['review_text'] = selector\n",
    "                        print(f\"‚úÖ Texte review: {selector}\")\n",
    "                        break\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            # Rating de review\n",
    "            rating_selectors = [\n",
    "                '[data-hook=\"review-star-rating\"] .a-icon-alt',\n",
    "                '.review-rating .a-icon-alt',\n",
    "                '.a-star-rating .a-icon-alt'\n",
    "            ]\n",
    "            \n",
    "            for selector in rating_selectors:\n",
    "                try:\n",
    "                    elements = self.driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                    if elements:\n",
    "                        selectors['review_rating'] = selector\n",
    "                        print(f\"‚úÖ Rating review: {selector}\")\n",
    "                        break\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            # Nom du reviewer\n",
    "            name_selectors = [\n",
    "                '.a-profile-name',\n",
    "                '.review-author',\n",
    "                '[data-hook=\"review-author\"]'\n",
    "            ]\n",
    "            \n",
    "            for selector in name_selectors:\n",
    "                try:\n",
    "                    elements = self.driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                    if elements and elements[0].text.strip():\n",
    "                        selectors['reviewer_name'] = selector\n",
    "                        print(f\"‚úÖ Nom reviewer: {selector}\")\n",
    "                        break\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            # Date de review\n",
    "            date_selectors = [\n",
    "                '[data-hook=\"review-date\"]',\n",
    "                '.review-date',\n",
    "                '.a-color-secondary'\n",
    "            ]\n",
    "            \n",
    "            for selector in date_selectors:\n",
    "                try:\n",
    "                    elements = self.driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                    if elements and elements[0].text.strip():\n",
    "                        selectors['review_date'] = selector\n",
    "                        print(f\"‚úÖ Date review: {selector}\")\n",
    "                        break\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            return selectors\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur d√©tection reviews: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def save_selectors(self, site, product_selectors, review_selectors, filename=None):\n",
    "        \"\"\"Sauvegarde les s√©lecteurs d√©tect√©s\"\"\"\n",
    "        if not filename:\n",
    "            filename = f\"../config/robust_{site}_selectors.json\"\n",
    "        \n",
    "        try:\n",
    "            validated_selectors = {\n",
    "                site: {\n",
    "                    'products': product_selectors,\n",
    "                    'reviews': review_selectors,\n",
    "                    'detected_at': datetime.now().isoformat()\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "            with open(filename, 'w', encoding='utf-8') as f:\n",
    "                json.dump(validated_selectors, f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "            print(f\"‚úÖ S√©lecteurs sauvegard√©s: {filename}\")\n",
    "            return filename\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur sauvegarde: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Ferme le driver\"\"\"\n",
    "        if self.driver:\n",
    "            try:\n",
    "                self.driver.quit()\n",
    "            except:\n",
    "                pass\n",
    "            self.driver = None\n",
    "\n",
    "print(\"‚úÖ RobustProductReviewScout cr√©√©\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a701baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Classe ProductReviewScraper cr√©√©e\n"
     ]
    }
   ],
   "source": [
    "class ProductReviewScraper:\n",
    "    \"\"\"\n",
    "    Scraper sp√©cialis√© pour r√©cup√©rer les reviews de produits avec balises valid√©es\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, selectors_file=\"../config/product_review_selectors.json\"):\n",
    "        self.driver = None\n",
    "        self.selectors = self.load_selectors(selectors_file)\n",
    "        self.scraped_data = []\n",
    "        \n",
    "    def load_selectors(self, filename):\n",
    "        \"\"\"Charge les s√©lecteurs valid√©s\"\"\"\n",
    "        try:\n",
    "            with open(filename, 'r', encoding='utf-8') as f:\n",
    "                return json.load(f)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Erreur chargement s√©lecteurs: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def setup_driver(self, headless=True):\n",
    "        \"\"\"Configure le driver anti-d√©tection pour le scraping\"\"\"\n",
    "        try:\n",
    "            options = uc.ChromeOptions()\n",
    "            options.add_argument('--no-sandbox')\n",
    "            options.add_argument('--disable-dev-shm-usage')\n",
    "            options.add_argument('--disable-gpu')\n",
    "            options.add_argument('--disable-extensions')\n",
    "            options.add_argument('--disable-notifications')\n",
    "            options.add_argument('--no-first-run')\n",
    "            \n",
    "            if headless:\n",
    "                options.add_argument('--headless')\n",
    "            \n",
    "            # User agent al√©atoire\n",
    "            user_agent = random.choice(REALISTIC_USER_AGENTS)\n",
    "            options.add_argument(f'--user-agent={user_agent}')\n",
    "            \n",
    "            # Options exp√©rimentales\n",
    "            options.add_experimental_option('useAutomationExtension', False)\n",
    "            options.add_experimental_option(\"prefs\", {\n",
    "                \"profile.default_content_setting_values.notifications\": 2\n",
    "            })\n",
    "            \n",
    "            self.driver = uc.Chrome(options=options, version_main=None)\n",
    "            \n",
    "            # Scripts anti-d√©tection\n",
    "            self.driver.execute_cdp_cmd('Page.addScriptToEvaluateOnNewDocument', {\n",
    "                'source': '''\n",
    "                    Object.defineProperty(navigator, 'webdriver', {\n",
    "                        get: () => undefined,\n",
    "                    });\n",
    "                '''\n",
    "            })\n",
    "            \n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur setup driver: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def scrape_category_product_reviews(self, site_url, category_search, max_products=10, reviews_per_rating=50):\n",
    "        \"\"\"\n",
    "        Scrape les reviews de produits d'une cat√©gorie sp√©cifique\n",
    "        \n",
    "        Args:\n",
    "            site_url: URL du site (amazon.com, ebay.com)\n",
    "            category_search: terme de recherche pour la cat√©gorie\n",
    "            max_products: nombre max de produits √† scraper (d√©faut: 10)\n",
    "            reviews_per_rating: nombre de reviews par note (d√©faut: 50)\n",
    "        \"\"\"\n",
    "        if not self.driver:\n",
    "            print(\"‚ùå Driver non initialis√©\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        try:\n",
    "            print(f\"üîç Recherche de produits pour: {category_search}\")\n",
    "            \n",
    "            # 1. R√©cup√©rer la liste des produits\n",
    "            products = self._get_products_list(site_url, category_search, max_products)\n",
    "            \n",
    "            if not products:\n",
    "                print(\"‚ùå Aucun produit trouv√©\")\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            print(f\"‚úÖ {len(products)} produits trouv√©s\")\n",
    "            \n",
    "            # 2. Pour chaque produit, r√©cup√©rer les reviews\n",
    "            all_reviews = []\n",
    "            \n",
    "            for i, product in enumerate(products[:max_products], 1):\n",
    "                print(f\"üì¶ Produit {i}/{len(products)}: {product['title'][:50]}...\")\n",
    "                \n",
    "                product_reviews = self._scrape_product_reviews(\n",
    "                    product, \n",
    "                    reviews_per_rating\n",
    "                )\n",
    "                \n",
    "                if product_reviews:\n",
    "                    all_reviews.extend(product_reviews)\n",
    "                    print(f\"‚úÖ {len(product_reviews)} reviews r√©cup√©r√©es\")\n",
    "                else:\n",
    "                    print(\"‚ö†Ô∏è Aucune review trouv√©e\")\n",
    "                \n",
    "                # D√©lai entre produits\n",
    "                time.sleep(random.uniform(2, 5))\n",
    "            \n",
    "            # 3. Cr√©er le DataFrame final\n",
    "            df = pd.DataFrame(all_reviews)\n",
    "            \n",
    "            if not df.empty:\n",
    "                # Nettoyage des donn√©es\n",
    "                df = self._clean_review_data(df)\n",
    "                print(f\"‚úÖ Total: {len(df)} reviews r√©cup√©r√©es\")\n",
    "            \n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur scraping: {e}\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def _get_products_list(self, site_url, search_term, max_products):\n",
    "        \"\"\"R√©cup√®re la liste des produits √† partir de la recherche\"\"\"\n",
    "        try:\n",
    "            # Construire l'URL de recherche\n",
    "            search_url = self._build_search_url(site_url, search_term)\n",
    "            print(f\"üîó URL: {search_url}\")\n",
    "            \n",
    "            self.driver.get(search_url)\n",
    "            time.sleep(3)\n",
    "            \n",
    "            products = []\n",
    "            site_type = 'amazon' if 'amazon' in site_url else 'ebay'\n",
    "            \n",
    "            # Utiliser les s√©lecteurs appropri√©s\n",
    "            if site_type in self.selectors and 'products' in self.selectors[site_type]:\n",
    "                selectors = self.selectors[site_type]['products']\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è S√©lecteurs non trouv√©s pour {site_type}\")\n",
    "                return []\n",
    "            \n",
    "            # R√©cup√©rer les conteneurs de produits\n",
    "            product_containers = self.driver.find_elements(\n",
    "                By.CSS_SELECTOR, \n",
    "                selectors.get('product_container', '.s-result-item')\n",
    "            )\n",
    "            \n",
    "            for container in product_containers[:max_products]:\n",
    "                try:\n",
    "                    # Extraire les infos du produit\n",
    "                    title_elem = container.find_element(\n",
    "                        By.CSS_SELECTOR, \n",
    "                        selectors.get('product_title', 'h2 span')\n",
    "                    )\n",
    "                    \n",
    "                    url_elem = container.find_element(\n",
    "                        By.CSS_SELECTOR, \n",
    "                        selectors.get('product_url', 'h2 a')\n",
    "                    )\n",
    "                    \n",
    "                    product_data = {\n",
    "                        'title': title_elem.text.strip(),\n",
    "                        'url': url_elem.get_attribute('href'),\n",
    "                        'category': search_term\n",
    "                    }\n",
    "                    \n",
    "                    # Prix optionnel\n",
    "                    try:\n",
    "                        price_elem = container.find_element(\n",
    "                            By.CSS_SELECTOR, \n",
    "                            selectors.get('product_price', '.a-price')\n",
    "                        )\n",
    "                        product_data['price'] = price_elem.text.strip()\n",
    "                    except:\n",
    "                        product_data['price'] = 'N/A'\n",
    "                    \n",
    "                    # Rating optionnel\n",
    "                    try:\n",
    "                        rating_elem = container.find_element(\n",
    "                            By.CSS_SELECTOR, \n",
    "                            selectors.get('product_rating', '.a-icon-alt')\n",
    "                        )\n",
    "                        product_data['rating'] = rating_elem.get_attribute('textContent')\n",
    "                    except:\n",
    "                        product_data['rating'] = 'N/A'\n",
    "                    \n",
    "                    if product_data['title'] and product_data['url']:\n",
    "                        products.append(product_data)\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    continue  # Ignorer les produits probl√©matiques\n",
    "            \n",
    "            return products\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur r√©cup√©ration produits: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def _scrape_product_reviews(self, product, reviews_per_rating):\n",
    "        \"\"\"Scrape les reviews d'un produit sp√©cifique\"\"\"\n",
    "        try:\n",
    "            # Aller sur la page produit\n",
    "            self.driver.get(product['url'])\n",
    "            time.sleep(2)\n",
    "            \n",
    "            # Chercher le lien vers les reviews\n",
    "            reviews_url = self._find_reviews_url(product['url'])\n",
    "            \n",
    "            if not reviews_url:\n",
    "                print(\"‚ö†Ô∏è Lien reviews non trouv√©\")\n",
    "                return []\n",
    "            \n",
    "            # Aller sur la page des reviews\n",
    "            self.driver.get(reviews_url)\n",
    "            time.sleep(3)\n",
    "            \n",
    "            reviews = []\n",
    "            site_type = 'amazon' if 'amazon' in product['url'] else 'ebay'\n",
    "            \n",
    "            # R√©cup√©rer les reviews par note (5, 4, 3, 2, 1 √©toiles)\n",
    "            for rating in [5, 4, 3, 2, 1]:\n",
    "                rating_reviews = self._scrape_reviews_by_rating(\n",
    "                    site_type, \n",
    "                    rating, \n",
    "                    reviews_per_rating,\n",
    "                    product\n",
    "                )\n",
    "                reviews.extend(rating_reviews)\n",
    "                \n",
    "                # D√©lai entre les notes\n",
    "                time.sleep(random.uniform(1, 3))\n",
    "            \n",
    "            return reviews\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur scraping reviews produit: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def _find_reviews_url(self, product_url):\n",
    "        \"\"\"Trouve l'URL de la page des reviews\"\"\"\n",
    "        try:\n",
    "            # Chercher les liens vers les reviews\n",
    "            review_selectors = [\n",
    "                'a[href*=\"customer-reviews\"]',\n",
    "                'a[href*=\"reviews\"]',\n",
    "                'a[href*=\"review\"]',\n",
    "                '.a-link-emphasis[href*=\"review\"]'\n",
    "            ]\n",
    "            \n",
    "            for selector in review_selectors:\n",
    "                try:\n",
    "                    link = self.driver.find_element(By.CSS_SELECTOR, selector)\n",
    "                    return link.get_attribute('href')\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            # Si aucun lien trouv√©, construire l'URL\n",
    "            if 'amazon' in product_url:\n",
    "                # Extraire l'ASIN depuis l'URL\n",
    "                import re\n",
    "                asin_match = re.search(r'/dp/([A-Z0-9]{10})', product_url)\n",
    "                if asin_match:\n",
    "                    asin = asin_match.group(1)\n",
    "                    return f\"https://www.amazon.com/product-reviews/{asin}\"\n",
    "            \n",
    "            return None\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur recherche URL reviews: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _scrape_reviews_by_rating(self, site_type, rating, max_reviews, product_info):\n",
    "        \"\"\"Scrape les reviews pour une note sp√©cifique\"\"\"\n",
    "        reviews = []\n",
    "        \n",
    "        try:\n",
    "            # Filtrer par note si possible\n",
    "            self._filter_by_rating(site_type, rating)\n",
    "            time.sleep(2)\n",
    "            \n",
    "            # R√©cup√©rer les reviews\n",
    "            selectors = self.selectors.get(site_type, {}).get('reviews', {})\n",
    "            pages_scraped = 0\n",
    "            max_pages = 10  # Limite de pages\n",
    "            \n",
    "            while len(reviews) < max_reviews and pages_scraped < max_pages:\n",
    "                # Reviews de la page actuelle\n",
    "                page_reviews = self._extract_reviews_from_page(selectors, product_info, rating)\n",
    "                \n",
    "                if not page_reviews:\n",
    "                    break\n",
    "                \n",
    "                reviews.extend(page_reviews)\n",
    "                \n",
    "                # Passer √† la page suivante\n",
    "                if not self._go_to_next_page(selectors):\n",
    "                    break\n",
    "                \n",
    "                pages_scraped += 1\n",
    "                time.sleep(random.uniform(2, 4))\n",
    "            \n",
    "            # Limiter au nombre demand√©\n",
    "            return reviews[:max_reviews]\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur scraping rating {rating}: {e}\")\n",
    "            return reviews\n",
    "    \n",
    "    def _filter_by_rating(self, site_type, rating):\n",
    "        \"\"\"Filtre les reviews par note\"\"\"\n",
    "        try:\n",
    "            if site_type == 'amazon':\n",
    "                # Chercher le filtre par √©toiles\n",
    "                filter_selector = f'a[href*=\"filterByStar=five_star\"], a[href*=\"star_rating={rating}\"]'\n",
    "                filter_links = self.driver.find_elements(By.CSS_SELECTOR, filter_selector)\n",
    "                \n",
    "                for link in filter_links:\n",
    "                    if f\"{rating}\" in link.get_attribute('href') or f\"{rating} star\" in link.text:\n",
    "                        link.click()\n",
    "                        return True\n",
    "        except:\n",
    "            pass\n",
    "        return False\n",
    "    \n",
    "    def _extract_reviews_from_page(self, selectors, product_info, target_rating):\n",
    "        \"\"\"Extrait les reviews de la page actuelle\"\"\"\n",
    "        reviews = []\n",
    "        \n",
    "        try:\n",
    "            # R√©cup√©rer tous les conteneurs de reviews\n",
    "            review_containers = self.driver.find_elements(\n",
    "                By.CSS_SELECTOR, \n",
    "                selectors.get('reviews_container', '[data-hook=\"review\"]')\n",
    "            )\n",
    "            \n",
    "            for container in review_containers:\n",
    "                try:\n",
    "                    review_data = {\n",
    "                        'product_name': product_info['title'],\n",
    "                        'product_category': product_info['category'],\n",
    "                        'product_url': product_info['url'],\n",
    "                        'target_rating': target_rating\n",
    "                    }\n",
    "                    \n",
    "                    # Titre de la review\n",
    "                    try:\n",
    "                        title_elem = container.find_element(\n",
    "                            By.CSS_SELECTOR, \n",
    "                            selectors.get('review_title', '[data-hook=\"review-title\"]')\n",
    "                        )\n",
    "                        review_data['review_title'] = title_elem.text.strip()\n",
    "                    except:\n",
    "                        review_data['review_title'] = ''\n",
    "                    \n",
    "                    # Texte de la review\n",
    "                    try:\n",
    "                        text_elem = container.find_element(\n",
    "                            By.CSS_SELECTOR, \n",
    "                            selectors.get('review_text', '[data-hook=\"review-body\"]')\n",
    "                        )\n",
    "                        review_data['review_text'] = text_elem.text.strip()\n",
    "                    except:\n",
    "                        review_data['review_text'] = ''\n",
    "                    \n",
    "                    # Note de la review\n",
    "                    try:\n",
    "                        rating_elem = container.find_element(\n",
    "                            By.CSS_SELECTOR, \n",
    "                            selectors.get('review_rating', '.a-icon-alt')\n",
    "                        )\n",
    "                        rating_text = rating_elem.get_attribute('textContent') or rating_elem.text\n",
    "                        # Extraire le chiffre de la note\n",
    "                        import re\n",
    "                        rating_match = re.search(r'(\\d+(?:\\.\\d+)?)', rating_text)\n",
    "                        review_data['user_rating'] = rating_match.group(1) if rating_match else 'N/A'\n",
    "                    except:\n",
    "                        review_data['user_rating'] = 'N/A'\n",
    "                    \n",
    "                    # Nom du reviewer\n",
    "                    try:\n",
    "                        name_elem = container.find_element(\n",
    "                            By.CSS_SELECTOR, \n",
    "                            selectors.get('reviewer_name', '.a-profile-name')\n",
    "                        )\n",
    "                        review_data['reviewer_name'] = name_elem.text.strip()\n",
    "                    except:\n",
    "                        review_data['reviewer_name'] = 'Anonymous'\n",
    "                    \n",
    "                    # Date de la review\n",
    "                    try:\n",
    "                        date_elem = container.find_element(\n",
    "                            By.CSS_SELECTOR, \n",
    "                            selectors.get('review_date', '[data-hook=\"review-date\"]')\n",
    "                        )\n",
    "                        review_data['review_date'] = date_elem.text.strip()\n",
    "                    except:\n",
    "                        review_data['review_date'] = 'N/A'\n",
    "                    \n",
    "                    # Timestamp de scraping\n",
    "                    review_data['scraped_at'] = datetime.now().isoformat()\n",
    "                    \n",
    "                    # Ajouter seulement si on a du contenu\n",
    "                    if review_data['review_text'] or review_data['review_title']:\n",
    "                        reviews.append(review_data)\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    continue  # Ignorer les reviews probl√©matiques\n",
    "            \n",
    "            return reviews\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur extraction reviews: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def _go_to_next_page(self, selectors):\n",
    "        \"\"\"Passe √† la page suivante des reviews\"\"\"\n",
    "        try:\n",
    "            next_button = self.driver.find_element(\n",
    "                By.CSS_SELECTOR, \n",
    "                selectors.get('next_page', '.a-pagination .a-last a')\n",
    "            )\n",
    "            next_button.click()\n",
    "            time.sleep(2)\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def _build_search_url(self, site_url, search_term):\n",
    "        \"\"\"Construit l'URL de recherche\"\"\"\n",
    "        if 'amazon' in site_url:\n",
    "            return f\"https://www.amazon.com/s?k={search_term.replace(' ', '+')}\"\n",
    "        elif 'ebay' in site_url:\n",
    "            return f\"https://www.ebay.com/sch/i.html?_nkw={search_term.replace(' ', '+')}\"\n",
    "        return site_url\n",
    "    \n",
    "    def _clean_review_data(self, df):\n",
    "        \"\"\"Nettoie et structure les donn√©es de reviews\"\"\"\n",
    "        try:\n",
    "            # Supprimer les doublons\n",
    "            df = df.drop_duplicates(subset=['review_text', 'product_name'], keep='first')\n",
    "            \n",
    "            # Nettoyer les textes\n",
    "            df['review_text'] = df['review_text'].str.strip()\n",
    "            df['review_title'] = df['review_title'].str.strip()\n",
    "            \n",
    "            # Convertir les ratings en num√©rique\n",
    "            df['user_rating'] = pd.to_numeric(df['user_rating'], errors='coerce')\n",
    "            \n",
    "            # Ajouter une colonne de longueur de texte\n",
    "            df['review_length'] = df['review_text'].str.len()\n",
    "            \n",
    "            # Filtrer les reviews trop courtes\n",
    "            df = df[df['review_length'] > 10]\n",
    "            \n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur nettoyage donn√©es: {e}\")\n",
    "            return df\n",
    "    \n",
    "    def save_reviews(self, df, filename=None):\n",
    "        \"\"\"Sauvegarde les reviews dans un fichier CSV\"\"\"\n",
    "        try:\n",
    "            if filename is None:\n",
    "                timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "                filename = f\"../data/raw/product_reviews_{timestamp}.csv\"\n",
    "            \n",
    "            os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "            df.to_csv(filename, index=False, encoding='utf-8')\n",
    "            print(f\"‚úÖ Reviews sauvegard√©es: {filename}\")\n",
    "            return filename\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur sauvegarde: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Ferme le driver\"\"\"\n",
    "        if self.driver:\n",
    "            self.driver.quit()\n",
    "\n",
    "print(\"‚úÖ Classe ProductReviewScraper cr√©√©e\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c106055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Workflow de reviews de produits cr√©√©\n",
      "üìñ Utilisez reviews_workflow_menu() pour commencer\n"
     ]
    }
   ],
   "source": [
    "def product_reviews_workflow(category_search=\"laptop\", site=\"amazon\", max_products=10, reviews_per_rating=50):\n",
    "    \"\"\"\n",
    "    Workflow complet pour r√©cup√©rer les reviews de produits d'une cat√©gorie\n",
    "    \n",
    "    Phase 1: D√©tection automatique des balises\n",
    "    Phase 2: Scraping des reviews avec balises valid√©es\n",
    "    \n",
    "    Args:\n",
    "        category_search: cat√©gorie de produits √† rechercher\n",
    "        site: site √† scraper ('amazon' ou 'ebay')\n",
    "        max_products: nombre de produits √† analyser (d√©faut: 10)\n",
    "        reviews_per_rating: nombre de reviews par note 1-5 √©toiles (d√©faut: 50)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"üéØ WORKFLOW SP√âCIALIS√â - REVIEWS DE PRODUITS\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"üì¶ Cat√©gorie: {category_search}\")\n",
    "    print(f\"üåê Site: {site}\")\n",
    "    print(f\"üìä Produits: {max_products}\")\n",
    "    print(f\"‚≠ê Reviews par note: {reviews_per_rating}\")\n",
    "    print(f\"üìà Total estim√©: {max_products * 5 * reviews_per_rating} reviews max\")\n",
    "    print()\n",
    "    \n",
    "    # URLs des sites\n",
    "    site_urls = {\n",
    "        'amazon': 'https://www.amazon.com',\n",
    "        'ebay': 'https://www.ebay.com'\n",
    "    }\n",
    "    \n",
    "    if site not in site_urls:\n",
    "        print(f\"‚ùå Site non support√©: {site}\")\n",
    "        return None\n",
    "    \n",
    "    site_url = site_urls[site]\n",
    "    \n",
    "    # ============================================================================\n",
    "    # PHASE 1: D√âTECTION DES BALISES\n",
    "    # ============================================================================\n",
    "    print(\"üîç PHASE 1: D√âTECTION AUTOMATIQUE DES BALISES\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    scout = ProductReviewScout()\n",
    "    \n",
    "    try:\n",
    "        # Setup du driver pour la d√©tection\n",
    "        if not scout.setup_driver(headless=True):\n",
    "            print(\"‚ùå √âchec initialisation driver de d√©tection\")\n",
    "            return None\n",
    "        \n",
    "        print(\"‚úÖ Driver de d√©tection initialis√©\")\n",
    "        \n",
    "        # D√©tection des s√©lecteurs de produits\n",
    "        print(f\"üîç D√©tection des s√©lecteurs de produits sur {site}...\")\n",
    "        product_selectors = scout.detect_product_selectors(site_url, category_search)\n",
    "        \n",
    "        if not product_selectors:\n",
    "            print(\"‚ùå √âchec d√©tection s√©lecteurs produits\")\n",
    "            scout.close()\n",
    "            return None\n",
    "        \n",
    "        print(\"‚úÖ S√©lecteurs produits d√©tect√©s:\")\n",
    "        for key, value in product_selectors.items():\n",
    "            print(f\"   ‚Ä¢ {key}: {value}\")\n",
    "        \n",
    "        # Test sur un produit pour d√©tecter les s√©lecteurs de reviews\n",
    "        print(\"üîç Test d√©tection s√©lecteurs de reviews...\")\n",
    "        \n",
    "        # Simuler la r√©cup√©ration d'un produit test\n",
    "        scout.driver.get(scout._build_search_url(site_url, category_search))\n",
    "        time.sleep(3)\n",
    "        \n",
    "        test_product_url = None\n",
    "        try:\n",
    "            # Trouver le premier produit\n",
    "            product_links = scout.driver.find_elements(By.CSS_SELECTOR, 'h2 a, .s-item__link')\n",
    "            if product_links:\n",
    "                test_product_url = product_links[0].get_attribute('href')\n",
    "                print(f\"üì¶ Produit test: {test_product_url[:80]}...\")\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        review_selectors = None\n",
    "        if test_product_url:\n",
    "            review_selectors = scout.detect_review_selectors(test_product_url)\n",
    "        \n",
    "        if review_selectors:\n",
    "            print(\"‚úÖ S√©lecteurs reviews d√©tect√©s:\")\n",
    "            for key, value in review_selectors.items():\n",
    "                print(f\"   ‚Ä¢ {key}: {value}\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è S√©lecteurs reviews non d√©tect√©s, utilisation des s√©lecteurs par d√©faut\")\n",
    "            review_selectors = scout.selectors.get(site, {}).get('reviews', {})\n",
    "        \n",
    "        # Sauvegarder les s√©lecteurs valid√©s\n",
    "        validated_selectors = {\n",
    "            site: {\n",
    "                'products': product_selectors,\n",
    "                'reviews': review_selectors\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        selectors_file = f\"../config/product_review_selectors_{site}.json\"\n",
    "        if scout.save_selectors(validated_selectors, selectors_file):\n",
    "            print(f\"‚úÖ S√©lecteurs sauvegard√©s: {selectors_file}\")\n",
    "        \n",
    "        scout.close()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur phase d√©tection: {e}\")\n",
    "        scout.close()\n",
    "        return None\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    \n",
    "    # ============================================================================\n",
    "    # PHASE 2: SCRAPING DES REVIEWS\n",
    "    # ============================================================================\n",
    "    print(\"üìä PHASE 2: SCRAPING DES REVIEWS\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    scraper = ProductReviewScraper(selectors_file)\n",
    "    \n",
    "    try:\n",
    "        # Setup du driver pour le scraping\n",
    "        if not scraper.setup_driver(headless=False):  # Visible pour monitoring\n",
    "            print(\"‚ùå √âchec initialisation driver de scraping\")\n",
    "            return None\n",
    "        \n",
    "        print(\"‚úÖ Driver de scraping initialis√©\")\n",
    "        print(f\"üé≠ User-Agent: {scraper.driver.execute_script('return navigator.userAgent;')[:80]}...\")\n",
    "        \n",
    "        # Avertissement utilisateur\n",
    "        print(\"\\n\" + \"üö® AVERTISSEMENT: Scraping en cours sur site r√©el!\")\n",
    "        print(\"‚è∞ Estimation dur√©e: {} minutes\".format(max_products * 5))  # ~5min par produit\n",
    "        print(\"üìù Respect des ToS et limitations de d√©bit\")\n",
    "        \n",
    "        input(\"Appuyer sur Entr√©e pour continuer ou Ctrl+C pour annuler...\")\n",
    "        \n",
    "        # Lancement du scraping\n",
    "        print(f\"\\nüöÄ D√©but du scraping pour '{category_search}'...\")\n",
    "        \n",
    "        df_reviews = scraper.scrape_category_product_reviews(\n",
    "            site_url=site_url,\n",
    "            category_search=category_search,\n",
    "            max_products=max_products,\n",
    "            reviews_per_rating=reviews_per_rating\n",
    "        )\n",
    "        \n",
    "        if df_reviews.empty:\n",
    "            print(\"‚ùå Aucune review r√©cup√©r√©e\")\n",
    "            scraper.close()\n",
    "            return None\n",
    "        \n",
    "        # Analyse des r√©sultats\n",
    "        print(\"\\n\" + \"üìä R√âSULTATS DU SCRAPING\")\n",
    "        print(\"-\" * 30)\n",
    "        print(f\"‚úÖ Total reviews: {len(df_reviews)}\")\n",
    "        print(f\"üì¶ Produits uniques: {df_reviews['product_name'].nunique()}\")\n",
    "        print(f\"‚≠ê Distribution des notes:\")\n",
    "        \n",
    "        rating_dist = df_reviews['user_rating'].value_counts().sort_index()\n",
    "        for rating, count in rating_dist.items():\n",
    "            print(f\"   {rating} √©toiles: {count} reviews\")\n",
    "        \n",
    "        print(f\"üìù Longueur moyenne: {df_reviews['review_length'].mean():.0f} caract√®res\")\n",
    "        \n",
    "        # Sauvegarde\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        output_file = f\"../data/raw/{site}_{category_search}_reviews_{timestamp}.csv\"\n",
    "        \n",
    "        saved_file = scraper.save_reviews(df_reviews, output_file)\n",
    "        \n",
    "        if saved_file:\n",
    "            print(f\"‚úÖ Donn√©es sauvegard√©es: {saved_file}\")\n",
    "            \n",
    "            # Aper√ßu des donn√©es\n",
    "            print(\"\\nüìã APER√áU DES DONN√âES:\")\n",
    "            print(df_reviews[['product_name', 'user_rating', 'review_text']].head(3).to_string())\n",
    "            \n",
    "        scraper.close()\n",
    "        \n",
    "        print(\"\\n\" + \"üéâ WORKFLOW TERMIN√â AVEC SUCC√àS!\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        return df_reviews\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n‚èπÔ∏è Arr√™t demand√© par l'utilisateur\")\n",
    "        scraper.close()\n",
    "        return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur phase scraping: {e}\")\n",
    "        scraper.close()\n",
    "        return None\n",
    "\n",
    "def quick_review_test(product_url, max_reviews=20):\n",
    "    \"\"\"\n",
    "    Test rapide pour scraper les reviews d'un produit sp√©cifique\n",
    "    \"\"\"\n",
    "    print(f\"üß™ TEST RAPIDE - Reviews d'un produit\")\n",
    "    print(f\"üîó URL: {product_url}\")\n",
    "    print(f\"üìä Reviews max: {max_reviews}\")\n",
    "    \n",
    "    scraper = ProductReviewScraper()\n",
    "    \n",
    "    try:\n",
    "        if not scraper.setup_driver(headless=False):\n",
    "            print(\"‚ùå √âchec setup driver\")\n",
    "            return None\n",
    "        \n",
    "        # Simuler un produit\n",
    "        fake_product = {\n",
    "            'title': 'Produit Test',\n",
    "            'url': product_url,\n",
    "            'category': 'test'\n",
    "        }\n",
    "        \n",
    "        # Scraper les reviews\n",
    "        reviews = scraper._scrape_product_reviews(fake_product, max_reviews)\n",
    "        \n",
    "        if reviews:\n",
    "            df = pd.DataFrame(reviews)\n",
    "            print(f\"‚úÖ {len(reviews)} reviews r√©cup√©r√©es\")\n",
    "            print(\"\\nüìã Aper√ßu:\")\n",
    "            for i, review in enumerate(reviews[:3], 1):\n",
    "                print(f\"\\nReview {i}:\")\n",
    "                print(f\"  Note: {review.get('user_rating', 'N/A')}\")\n",
    "                print(f\"  Titre: {review.get('review_title', 'N/A')[:50]}...\")\n",
    "                print(f\"  Texte: {review.get('review_text', 'N/A')[:100]}...\")\n",
    "            \n",
    "            return df\n",
    "        else:\n",
    "            print(\"‚ùå Aucune review trouv√©e\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur test: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        scraper.close()\n",
    "\n",
    "def reviews_workflow_menu():\n",
    "    \"\"\"Menu principal pour le workflow de reviews\"\"\"\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"üéØ WORKFLOW REVIEWS DE PRODUITS - MENU PRINCIPAL\")\n",
    "    print(\"=\"*80)\n",
    "    print()\n",
    "    print(\"1Ô∏è‚É£ Workflow complet (d√©tection + scraping)\")\n",
    "    print(\"2Ô∏è‚É£ Test rapide sur un produit\")\n",
    "    print(\"3Ô∏è‚É£ Configuration personnalis√©e\")\n",
    "    print(\"4Ô∏è‚É£ Voir les s√©lecteurs sauvegard√©s\")\n",
    "    print(\"5Ô∏è‚É£ Quitter\")\n",
    "    print()\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            choice = input(\"üëâ Votre choix (1-5): \").strip()\n",
    "            \n",
    "            if choice == '1':\n",
    "                # Workflow complet\n",
    "                print(\"\\nüìã Configuration du workflow complet:\")\n",
    "                category = input(\"üè∑Ô∏è Cat√©gorie de produits (ex: 'laptop', 'smartphone'): \").strip() or \"laptop\"\n",
    "                site = input(\"üåê Site (amazon/ebay): \").strip() or \"amazon\"\n",
    "                \n",
    "                try:\n",
    "                    max_products = int(input(\"üì¶ Nombre de produits (d√©faut: 10): \") or \"10\")\n",
    "                    reviews_per_rating = int(input(\"‚≠ê Reviews par note (d√©faut: 50): \") or \"50\")\n",
    "                except ValueError:\n",
    "                    max_products, reviews_per_rating = 10, 50\n",
    "                \n",
    "                return product_reviews_workflow(category, site, max_products, reviews_per_rating)\n",
    "                \n",
    "            elif choice == '2':\n",
    "                # Test rapide\n",
    "                product_url = input(\"üîó URL du produit √† tester: \").strip()\n",
    "                if product_url:\n",
    "                    try:\n",
    "                        max_reviews = int(input(\"üìä Nombre max de reviews (d√©faut: 20): \") or \"20\")\n",
    "                    except ValueError:\n",
    "                        max_reviews = 20\n",
    "                    return quick_review_test(product_url, max_reviews)\n",
    "                else:\n",
    "                    print(\"‚ùå URL requise\")\n",
    "                    \n",
    "            elif choice == '3':\n",
    "                # Configuration avanc√©e\n",
    "                print(\"\\n‚öôÔ∏è Configuration personnalis√©e disponible dans product_reviews_workflow()\")\n",
    "                print(\"üìñ Consultez la documentation de la fonction\")\n",
    "                \n",
    "            elif choice == '4':\n",
    "                # Voir s√©lecteurs\n",
    "                print(\"\\nüìã S√©lecteurs sauvegard√©s:\")\n",
    "                for filename in ['../config/product_review_selectors_amazon.json', '../config/product_review_selectors_ebay.json']:\n",
    "                    if os.path.exists(filename):\n",
    "                        print(f\"‚úÖ {filename}\")\n",
    "                        try:\n",
    "                            with open(filename, 'r') as f:\n",
    "                                data = json.load(f)\n",
    "                                print(f\"   Sites: {list(data.keys())}\")\n",
    "                        except:\n",
    "                            pass\n",
    "                    else:\n",
    "                        print(f\"‚ùå {filename} (non trouv√©)\")\n",
    "                        \n",
    "            elif choice == '5':\n",
    "                print(\"üëã √Ä bient√¥t!\")\n",
    "                break\n",
    "                \n",
    "            else:\n",
    "                print(\"‚ùå Choix invalide, veuillez r√©essayer\")\n",
    "                \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nüëã √Ä bient√¥t!\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur: {e}\")\n",
    "\n",
    "print(\"‚úÖ Workflow de reviews de produits cr√©√©\")\n",
    "print(\"üìñ Utilisez reviews_workflow_menu() pour commencer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4218b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ WORKFLOW REVIEWS DE PRODUITS - PR√äT √Ä UTILISER\n",
      "================================================================================\n",
      "\n",
      "üìã EXEMPLES D'UTILISATION:\n",
      "\n",
      "1Ô∏è‚É£ Workflow complet automatique:\n",
      "   df = product_reviews_workflow('laptop', 'amazon', 10, 50)\n",
      "   # R√©cup√®re 50 reviews par note (1-5) pour 10 laptops sur Amazon\n",
      "\n",
      "2Ô∏è‚É£ Menu interactif:\n",
      "   reviews_workflow_menu()\n",
      "   # Interface guid√©e pour configurer le scraping\n",
      "\n",
      "3Ô∏è‚É£ Test rapide d'un produit:\n",
      "   df = quick_review_test('https://amazon.com/dp/XXXXXXXXXX', 20)\n",
      "   # Test sur un produit sp√©cifique\n",
      "\n",
      "4Ô∏è‚É£ Configurations personnalis√©es:\n",
      "   # Smartphones sur eBay\n",
      "   df = product_reviews_workflow('smartphone', 'ebay', 5, 30)\n",
      "\n",
      "   # Casques audio sur Amazon\n",
      "   df = product_reviews_workflow('headphones', 'amazon', 15, 40)\n",
      "\n",
      "üìä DONN√âES R√âCUP√âR√âES:\n",
      "   ‚Ä¢ product_name: nom du produit\n",
      "   ‚Ä¢ product_category: cat√©gorie recherch√©e\n",
      "   ‚Ä¢ review_title: titre de la review\n",
      "   ‚Ä¢ review_text: texte complet de la review\n",
      "   ‚Ä¢ user_rating: note donn√©e (1-5)\n",
      "   ‚Ä¢ reviewer_name: nom du reviewer\n",
      "   ‚Ä¢ review_date: date de la review\n",
      "   ‚Ä¢ scraped_at: timestamp du scraping\n",
      "\n",
      "üíæ SAUVEGARDE AUTOMATIQUE:\n",
      "   ‚Ä¢ Format CSV dans ../data/raw/\n",
      "   ‚Ä¢ Nom: {site}_{categorie}_reviews_{timestamp}.csv\n",
      "\n",
      "üõ°Ô∏è S√âCURIT√â:\n",
      "   ‚Ä¢ Anti-d√©tection avec user-agents al√©atoires\n",
      "   ‚Ä¢ D√©lais humains entre requ√™tes\n",
      "   ‚Ä¢ Respect des limitations de d√©bit\n",
      "   ‚Ä¢ Options Chrome optimis√©es\n",
      "\n",
      "‚ö†Ô∏è IMPORTANT:\n",
      "   ‚Ä¢ Respecter les ToS des sites\n",
      "   ‚Ä¢ Utiliser avec mod√©ration\n",
      "   ‚Ä¢ V√©rifier robots.txt\n",
      "   ‚Ä¢ Ne pas surcharger les serveurs\n",
      "\n",
      "üöÄ Pour commencer, utilisez:\n",
      "   reviews_workflow_menu()\n",
      "\n",
      "‚úÖ Workflow pr√™t! Tapez reviews_workflow_menu() pour commencer\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# EXEMPLES D'UTILISATION DU WORKFLOW REVIEWS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"üéØ WORKFLOW REVIEWS DE PRODUITS - PR√äT √Ä UTILISER\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "print(\"üìã EXEMPLES D'UTILISATION:\")\n",
    "print()\n",
    "print(\"1Ô∏è‚É£ Workflow complet automatique:\")\n",
    "print(\"   df = product_reviews_workflow('laptop', 'amazon', 10, 50)\")\n",
    "print(\"   # R√©cup√®re 50 reviews par note (1-5) pour 10 laptops sur Amazon\")\n",
    "print()\n",
    "print(\"2Ô∏è‚É£ Menu interactif:\")\n",
    "print(\"   reviews_workflow_menu()\")\n",
    "print(\"   # Interface guid√©e pour configurer le scraping\")\n",
    "print()\n",
    "print(\"3Ô∏è‚É£ Test rapide d'un produit:\")\n",
    "print(\"   df = quick_review_test('https://amazon.com/dp/XXXXXXXXXX', 20)\")\n",
    "print(\"   # Test sur un produit sp√©cifique\")\n",
    "print()\n",
    "print(\"4Ô∏è‚É£ Configurations personnalis√©es:\")\n",
    "print(\"   # Smartphones sur eBay\")\n",
    "print(\"   df = product_reviews_workflow('smartphone', 'ebay', 5, 30)\")\n",
    "print()\n",
    "print(\"   # Casques audio sur Amazon\")\n",
    "print(\"   df = product_reviews_workflow('headphones', 'amazon', 15, 40)\")\n",
    "print()\n",
    "print(\"üìä DONN√âES R√âCUP√âR√âES:\")\n",
    "print(\"   ‚Ä¢ product_name: nom du produit\")\n",
    "print(\"   ‚Ä¢ product_category: cat√©gorie recherch√©e\")\n",
    "print(\"   ‚Ä¢ review_title: titre de la review\")\n",
    "print(\"   ‚Ä¢ review_text: texte complet de la review\")\n",
    "print(\"   ‚Ä¢ user_rating: note donn√©e (1-5)\")\n",
    "print(\"   ‚Ä¢ reviewer_name: nom du reviewer\")\n",
    "print(\"   ‚Ä¢ review_date: date de la review\")\n",
    "print(\"   ‚Ä¢ scraped_at: timestamp du scraping\")\n",
    "print()\n",
    "print(\"üíæ SAUVEGARDE AUTOMATIQUE:\")\n",
    "print(\"   ‚Ä¢ Format CSV dans ../data/raw/\")\n",
    "print(\"   ‚Ä¢ Nom: {site}_{categorie}_reviews_{timestamp}.csv\")\n",
    "print()\n",
    "print(\"üõ°Ô∏è S√âCURIT√â:\")\n",
    "print(\"   ‚Ä¢ Anti-d√©tection avec user-agents al√©atoires\")\n",
    "print(\"   ‚Ä¢ D√©lais humains entre requ√™tes\")\n",
    "print(\"   ‚Ä¢ Respect des limitations de d√©bit\")\n",
    "print(\"   ‚Ä¢ Options Chrome optimis√©es\")\n",
    "print()\n",
    "print(\"‚ö†Ô∏è IMPORTANT:\")\n",
    "print(\"   ‚Ä¢ Respecter les ToS des sites\")\n",
    "print(\"   ‚Ä¢ Utiliser avec mod√©ration\")\n",
    "print(\"   ‚Ä¢ V√©rifier robots.txt\")\n",
    "print(\"   ‚Ä¢ Ne pas surcharger les serveurs\")\n",
    "print()\n",
    "print(\"üöÄ Pour commencer, utilisez:\")\n",
    "print(\"   reviews_workflow_menu()\")\n",
    "\n",
    "# Exemple de configuration pr√™te √† l'emploi\n",
    "EXAMPLE_CONFIGS = {\n",
    "    'laptops_amazon': {\n",
    "        'category_search': 'laptop gaming',\n",
    "        'site': 'amazon',\n",
    "        'max_products': 8,\n",
    "        'reviews_per_rating': 40,\n",
    "        'description': 'Reviews de laptops gaming sur Amazon'\n",
    "    },\n",
    "    'smartphones_ebay': {\n",
    "        'category_search': 'smartphone iphone',\n",
    "        'site': 'ebay', \n",
    "        'max_products': 5,\n",
    "        'reviews_per_rating': 30,\n",
    "        'description': 'Reviews d\\'iPhones sur eBay'\n",
    "    },\n",
    "    'headphones_amazon': {\n",
    "        'category_search': 'wireless headphones',\n",
    "        'site': 'amazon',\n",
    "        'max_products': 12,\n",
    "        'reviews_per_rating': 35,\n",
    "        'description': 'Reviews de casques sans-fil sur Amazon'\n",
    "    }\n",
    "}\n",
    "\n",
    "def run_example_config(config_name):\n",
    "    \"\"\"Ex√©cute une configuration d'exemple\"\"\"\n",
    "    if config_name in EXAMPLE_CONFIGS:\n",
    "        config = EXAMPLE_CONFIGS[config_name]\n",
    "        print(f\"üöÄ Lancement: {config['description']}\")\n",
    "        return product_reviews_workflow(**{k:v for k,v in config.items() if k != 'description'})\n",
    "    else:\n",
    "        print(f\"‚ùå Configuration '{config_name}' non trouv√©e\")\n",
    "        print(f\"üìã Disponibles: {list(EXAMPLE_CONFIGS.keys())}\")\n",
    "        return None\n",
    "\n",
    "print(\"\\n‚úÖ Workflow pr√™t! Tapez reviews_workflow_menu() pour commencer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6933b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üéØ WORKFLOW REVIEWS DE PRODUITS - MENU PRINCIPAL\n",
      "================================================================================\n",
      "\n",
      "1Ô∏è‚É£ Workflow complet (d√©tection + scraping)\n",
      "2Ô∏è‚É£ Test rapide sur un produit\n",
      "3Ô∏è‚É£ Configuration personnalis√©e\n",
      "4Ô∏è‚É£ Voir les s√©lecteurs sauvegard√©s\n",
      "5Ô∏è‚É£ Quitter\n",
      "\n",
      "\n",
      "üìã Configuration du workflow complet:\n",
      "\n",
      "üìã Configuration du workflow complet:\n",
      "================================================================================\n",
      "üéØ WORKFLOW SP√âCIALIS√â - REVIEWS DE PRODUITS\n",
      "================================================================================\n",
      "üì¶ Cat√©gorie: laptop\n",
      "üåê Site: amazon\n",
      "üìä Produits: 10\n",
      "‚≠ê Reviews par note: 10\n",
      "üìà Total estim√©: 500 reviews max\n",
      "\n",
      "üîç PHASE 1: D√âTECTION AUTOMATIQUE DES BALISES\n",
      "--------------------------------------------------\n",
      "‚ùå Erreur: name 'ProductReviewScout' is not defined\n",
      "\n",
      "üìã Configuration du workflow complet:\n",
      "================================================================================\n",
      "üéØ WORKFLOW SP√âCIALIS√â - REVIEWS DE PRODUITS\n",
      "================================================================================\n",
      "üì¶ Cat√©gorie: laptop\n",
      "üåê Site: amazon\n",
      "üìä Produits: 10\n",
      "‚≠ê Reviews par note: 10\n",
      "üìà Total estim√©: 500 reviews max\n",
      "\n",
      "üîç PHASE 1: D√âTECTION AUTOMATIQUE DES BALISES\n",
      "--------------------------------------------------\n",
      "‚ùå Erreur: name 'ProductReviewScout' is not defined\n",
      "\n",
      "üìã Configuration du workflow complet:\n",
      "================================================================================\n",
      "üéØ WORKFLOW SP√âCIALIS√â - REVIEWS DE PRODUITS\n",
      "================================================================================\n",
      "üì¶ Cat√©gorie: escape\n",
      "üåê Site: escape\n",
      "üìä Produits: 10\n",
      "‚≠ê Reviews par note: 50\n",
      "üìà Total estim√©: 2500 reviews max\n",
      "\n",
      "‚ùå Site non support√©: escape\n",
      "================================================================================\n",
      "üéØ WORKFLOW SP√âCIALIS√â - REVIEWS DE PRODUITS\n",
      "================================================================================\n",
      "üì¶ Cat√©gorie: escape\n",
      "üåê Site: escape\n",
      "üìä Produits: 10\n",
      "‚≠ê Reviews par note: 50\n",
      "üìà Total estim√©: 2500 reviews max\n",
      "\n",
      "‚ùå Site non support√©: escape\n"
     ]
    }
   ],
   "source": [
    "reviews_workflow_menu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66698f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Test du syst√®me de cr√©ation de driver robuste...\n",
      "üîß Tentative 1/3 - Cr√©ation driver...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-29 14:58:09,103 - INFO - patching driver executable C:\\Users\\Yann\\appdata\\roaming\\undetected_chromedriver\\undetected_chromedriver.exe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Driver robuste cr√©√© avec succ√®s!\n",
      "‚úÖ Navigation test r√©ussie!\n",
      "‚úÖ Syst√®me de driver robuste pr√™t!\n",
      "‚úÖ Navigation test r√©ussie!\n",
      "‚úÖ Syst√®me de driver robuste pr√™t!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CORRECTION ROBUSTE DES OPTIONS CHROME\n",
    "# ============================================================================\n",
    "\n",
    "def create_robust_chrome_options(headless=True):\n",
    "    \"\"\"\n",
    "    Cr√©e des options Chrome 100% compatibles avec toutes les versions\n",
    "    √âvite toutes les options probl√©matiques\n",
    "    \"\"\"\n",
    "    import undetected_chromedriver as uc\n",
    "    \n",
    "    try:\n",
    "        options = uc.ChromeOptions()\n",
    "        \n",
    "        # Arguments de base s√ªrs et test√©s\n",
    "        safe_args = [\n",
    "            '--no-sandbox',\n",
    "            '--disable-dev-shm-usage',\n",
    "            '--disable-gpu',\n",
    "            '--disable-web-security',\n",
    "            '--disable-features=VizDisplayCompositor',\n",
    "            '--disable-extensions',\n",
    "            '--disable-plugins',\n",
    "            '--disable-default-apps',\n",
    "            '--disable-background-timer-throttling',\n",
    "            '--disable-backgrounding-occluded-windows',\n",
    "            '--disable-renderer-backgrounding',\n",
    "            '--disable-field-trial-config',\n",
    "            '--disable-back-forward-cache',\n",
    "            '--disable-ipc-flooding-protection',\n",
    "            '--window-size=1920,1080',\n",
    "            '--remote-debugging-port=9222'\n",
    "        ]\n",
    "        \n",
    "        # Ajouter les arguments s√ªrs\n",
    "        for arg in safe_args:\n",
    "            options.add_argument(arg)\n",
    "        \n",
    "        # Mode headless si demand√©\n",
    "        if headless:\n",
    "            options.add_argument('--headless=new')  # Nouveau mode headless\n",
    "        \n",
    "        # User agent al√©atoire\n",
    "        try:\n",
    "            user_agent = random.choice(REALISTIC_USER_AGENTS)\n",
    "            options.add_argument(f'--user-agent={user_agent}')\n",
    "        except:\n",
    "            # Fallback si REALISTIC_USER_AGENTS n'existe pas\n",
    "            options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')\n",
    "        \n",
    "        # Pr√©f√©rences s√ªres SEULEMENT\n",
    "        prefs = {\n",
    "            \"profile.default_content_setting_values.notifications\": 2,\n",
    "            \"profile.default_content_settings.popups\": 0,\n",
    "            \"profile.managed_default_content_settings.images\": 2,\n",
    "            \"profile.default_content_setting_values.media_stream_mic\": 2,\n",
    "            \"profile.default_content_setting_values.media_stream_camera\": 2,\n",
    "            \"profile.default_content_setting_values.geolocation\": 2\n",
    "        }\n",
    "        \n",
    "        options.add_experimental_option(\"prefs\", prefs)\n",
    "        \n",
    "        # NE PAS AJOUTER: excludeSwitches, useAutomationExtension\n",
    "        # Ces options causent des erreurs dans les nouvelles versions\n",
    "        \n",
    "        return options\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur cr√©ation options Chrome: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_robust_driver(headless=True, max_retries=3):\n",
    "    \"\"\"\n",
    "    Cr√©e un driver robuste avec plusieurs tentatives et fallbacks\n",
    "    \"\"\"\n",
    "    import undetected_chromedriver as uc\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            print(f\"üîß Tentative {attempt + 1}/{max_retries} - Cr√©ation driver...\")\n",
    "            \n",
    "            # Options robustes\n",
    "            options = create_robust_chrome_options(headless)\n",
    "            if not options:\n",
    "                continue\n",
    "            \n",
    "            # Cr√©ation du driver avec param√®tres optimaux\n",
    "            driver = uc.Chrome(\n",
    "                options=options,\n",
    "                version_main=None,  # Auto-d√©tection\n",
    "                headless=headless,\n",
    "                use_subprocess=False,\n",
    "                log_level=3  # R√©duire les logs\n",
    "            )\n",
    "            \n",
    "            # Test rapide\n",
    "            driver.get(\"data:text/html,<html><body><h1>Test</h1></body></html>\")\n",
    "            \n",
    "            print(\"‚úÖ Driver robuste cr√©√© avec succ√®s!\")\n",
    "            return driver\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Tentative {attempt + 1} √©chou√©e: {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                print(\"üîÑ Nouvelle tentative...\")\n",
    "                time.sleep(2)\n",
    "            else:\n",
    "                print(\"‚ùå Toutes les tentatives ont √©chou√©\")\n",
    "                return create_selenium_fallback_driver()\n",
    "    \n",
    "    return None\n",
    "\n",
    "def create_selenium_fallback_driver():\n",
    "    \"\"\"\n",
    "    Driver de secours avec Selenium classique\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"üîÑ Fallback vers Selenium classique...\")\n",
    "        \n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.chrome.service import Service\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        from webdriver_manager.chrome import ChromeDriverManager\n",
    "        \n",
    "        # Options Selenium classiques\n",
    "        options = Options()\n",
    "        options.add_argument('--no-sandbox')\n",
    "        options.add_argument('--disable-dev-shm-usage')\n",
    "        options.add_argument('--disable-gpu')\n",
    "        options.add_argument('--window-size=1920,1080')\n",
    "        \n",
    "        # Service g√©r√© automatiquement\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        \n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        \n",
    "        print(\"‚úÖ Driver Selenium classique cr√©√©!\")\n",
    "        return driver\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Fallback Selenium √©chou√©: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test du syst√®me robuste\n",
    "print(\"üß™ Test du syst√®me de cr√©ation de driver robuste...\")\n",
    "\n",
    "test_driver = create_robust_driver(headless=True)\n",
    "if test_driver:\n",
    "    try:\n",
    "        test_driver.get(\"https://httpbin.org/user-agent\")\n",
    "        print(\"‚úÖ Navigation test r√©ussie!\")\n",
    "        test_driver.quit()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Test navigation: {e}\")\n",
    "        test_driver.quit()\n",
    "else:\n",
    "    print(\"‚ùå Impossible de cr√©er un driver robuste\")\n",
    "\n",
    "print(\"‚úÖ Syst√®me de driver robuste pr√™t!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d981db",
   "metadata": {},
   "source": [
    "# Robus Scrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40c0611e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Scout robuste cr√©√©!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SCOUT ROBUSTE POUR D√âTECTION DE BALISES\n",
    "# ============================================================================\n",
    "\n",
    "class RobustProductReviewScout:\n",
    "    \"\"\"\n",
    "    Scout ultra-robuste pour d√©tecter automatiquement les balises de produits et reviews\n",
    "    Avec gestion d'erreurs compl√®te et fallbacks multiples\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.driver = None\n",
    "        self.detected_selectors = {}\n",
    "        self.base_selectors = {\n",
    "            'amazon': {\n",
    "                'products': {\n",
    "                    'containers': [\n",
    "                        '[data-component-type=\"s-search-result\"]',\n",
    "                        '.s-result-item',\n",
    "                        '.s-widget-container .s-card-container',\n",
    "                        '[data-asin]:not([data-asin=\"\"])'\n",
    "                    ],\n",
    "                    'titles': [\n",
    "                        'h2 span',\n",
    "                        'h2 a span',\n",
    "                        '.s-size-mini span',\n",
    "                        '.a-size-base-plus',\n",
    "                        '[data-cy=\"title-recipe-title\"]'\n",
    "                    ],\n",
    "                    'urls': [\n",
    "                        'h2 a',\n",
    "                        '.a-link-normal',\n",
    "                        'a[href*=\"/dp/\"]',\n",
    "                        'a[href*=\"/gp/product/\"]'\n",
    "                    ],\n",
    "                    'prices': [\n",
    "                        '.a-price .a-offscreen',\n",
    "                        '.a-price-whole',\n",
    "                        '.a-price-range .a-offscreen',\n",
    "                        '.a-price-symbol + .a-price-whole'\n",
    "                    ],\n",
    "                    'ratings': [\n",
    "                        '.a-icon-alt',\n",
    "                        '.a-star-mini .a-icon-alt',\n",
    "                        'span[aria-label*=\"stars\"]'\n",
    "                    ]\n",
    "                },\n",
    "                'reviews': {\n",
    "                    'containers': [\n",
    "                        '[data-hook=\"review\"]',\n",
    "                        '.review',\n",
    "                        '.cr-original-review-content',\n",
    "                        '.reviewText'\n",
    "                    ],\n",
    "                    'titles': [\n",
    "                        '[data-hook=\"review-title\"] span',\n",
    "                        '.review-title',\n",
    "                        '.cr-original-review-content .review-title'\n",
    "                    ],\n",
    "                    'texts': [\n",
    "                        '[data-hook=\"review-body\"] span',\n",
    "                        '.review-text',\n",
    "                        '.cr-original-review-content .review-text',\n",
    "                        '.reviewText'\n",
    "                    ],\n",
    "                    'ratings': [\n",
    "                        '[data-hook=\"review-star-rating\"] .a-icon-alt',\n",
    "                        '.review-rating .a-icon-alt',\n",
    "                        '.cr-original-review-content .a-icon-alt'\n",
    "                    ],\n",
    "                    'authors': [\n",
    "                        '.a-profile-name',\n",
    "                        '.review-author',\n",
    "                        '.cr-original-review-content .author'\n",
    "                    ],\n",
    "                    'dates': [\n",
    "                        '[data-hook=\"review-date\"]',\n",
    "                        '.review-date',\n",
    "                        '.cr-original-review-content .review-date'\n",
    "                    ]\n",
    "                }\n",
    "            },\n",
    "            'ebay': {\n",
    "                'products': {\n",
    "                    'containers': [\n",
    "                        '.s-item',\n",
    "                        '.srp-results .s-item',\n",
    "                        '.b-listing__wrap'\n",
    "                    ],\n",
    "                    'titles': [\n",
    "                        '.s-item__title',\n",
    "                        '.it-ttl',\n",
    "                        '.b-listing__title'\n",
    "                    ],\n",
    "                    'urls': [\n",
    "                        '.s-item__link',\n",
    "                        '.it-ttl a',\n",
    "                        '.b-listing__title a'\n",
    "                    ],\n",
    "                    'prices': [\n",
    "                        '.s-item__price',\n",
    "                        '.notranslate',\n",
    "                        '.b-listing__price'\n",
    "                    ]\n",
    "                },\n",
    "                'reviews': {\n",
    "                    'containers': [\n",
    "                        '.review-item',\n",
    "                        '.ebay-review',\n",
    "                        '.reviews .review'\n",
    "                    ],\n",
    "                    'texts': [\n",
    "                        '.review-item-content',\n",
    "                        '.ebay-review-text',\n",
    "                        '.review-content'\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def setup_robust_driver(self, headless=True, timeout=30):\n",
    "        \"\"\"Configuration driver ultra-robuste avec fallbacks\"\"\"\n",
    "        \n",
    "        max_attempts = 3\n",
    "        \n",
    "        for attempt in range(max_attempts):\n",
    "            try:\n",
    "                print(f\"üîß Tentative {attempt + 1}/{max_attempts} - Setup driver scout...\")\n",
    "                \n",
    "                # Fermer le driver existant si n√©cessaire\n",
    "                if self.driver:\n",
    "                    try:\n",
    "                        self.driver.quit()\n",
    "                    except:\n",
    "                        pass\n",
    "                    self.driver = None\n",
    "                \n",
    "                # Options ultra-s√ªres\n",
    "                options = self._create_safe_options(headless)\n",
    "                \n",
    "                # Cr√©ation driver avec timeout\n",
    "                import undetected_chromedriver as uc\n",
    "                \n",
    "                self.driver = uc.Chrome(\n",
    "                    options=options,\n",
    "                    version_main=None,\n",
    "                    headless=headless,\n",
    "                    use_subprocess=False,\n",
    "                    log_level=3\n",
    "                )\n",
    "                \n",
    "                # Configuration des timeouts\n",
    "                self.driver.set_page_load_timeout(timeout)\n",
    "                self.driver.implicitly_wait(10)\n",
    "                \n",
    "                # Test de fonctionnement\n",
    "                self.driver.get(\"data:text/html,<html><body><h1>Test Scout</h1></body></html>\")\n",
    "                \n",
    "                print(\"‚úÖ Driver scout initialis√© avec succ√®s!\")\n",
    "                return True\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Tentative {attempt + 1} √©chou√©e: {str(e)[:100]}...\")\n",
    "                if attempt < max_attempts - 1:\n",
    "                    time.sleep(3)\n",
    "                else:\n",
    "                    print(\"‚ùå Impossible de cr√©er le driver scout\")\n",
    "                    return False\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def _create_safe_options(self, headless=True):\n",
    "        \"\"\"Cr√©e des options Chrome ultra-s√ªres\"\"\"\n",
    "        \n",
    "        import undetected_chromedriver as uc\n",
    "        \n",
    "        options = uc.ChromeOptions()\n",
    "        \n",
    "        # Arguments de base seulement\n",
    "        safe_args = [\n",
    "            '--no-sandbox',\n",
    "            '--disable-dev-shm-usage',\n",
    "            '--disable-gpu',\n",
    "            '--disable-web-security',\n",
    "            '--disable-features=VizDisplayCompositor',\n",
    "            '--window-size=1920,1080',\n",
    "            '--start-maximized'\n",
    "        ]\n",
    "        \n",
    "        for arg in safe_args:\n",
    "            options.add_argument(arg)\n",
    "        \n",
    "        if headless:\n",
    "            options.add_argument('--headless=new')\n",
    "        \n",
    "        # User agent al√©atoire\n",
    "        try:\n",
    "            user_agent = random.choice(REALISTIC_USER_AGENTS)\n",
    "            options.add_argument(f'--user-agent={user_agent}')\n",
    "        except:\n",
    "            options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')\n",
    "        \n",
    "        # Pr√©f√©rences minimales\n",
    "        prefs = {\n",
    "            \"profile.default_content_setting_values.notifications\": 2,\n",
    "            \"profile.default_content_settings.popups\": 0\n",
    "        }\n",
    "        options.add_experimental_option(\"prefs\", prefs)\n",
    "        \n",
    "        return options\n",
    "    \n",
    "    def detect_site_selectors(self, site_url, search_term=\"laptop\", max_retries=3):\n",
    "        \"\"\"\n",
    "        D√©tecte automatiquement tous les s√©lecteurs pour un site\n",
    "        \"\"\"\n",
    "        \n",
    "        if not self.driver:\n",
    "            print(\"‚ùå Driver non initialis√©\")\n",
    "            return {}\n",
    "        \n",
    "        site_type = 'amazon' if 'amazon' in site_url else 'ebay' if 'ebay' in site_url else 'unknown'\n",
    "        \n",
    "        if site_type == 'unknown':\n",
    "            print(f\"‚ùå Site non support√©: {site_url}\")\n",
    "            return {}\n",
    "        \n",
    "        print(f\"üîç D√©tection des s√©lecteurs pour {site_type}...\")\n",
    "        \n",
    "        detected = {\n",
    "            'site': site_type,\n",
    "            'products': {},\n",
    "            'reviews': {}\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Phase 1: D√©tection s√©lecteurs produits\n",
    "            product_selectors = self._detect_product_selectors(site_url, search_term, site_type)\n",
    "            detected['products'] = product_selectors\n",
    "            \n",
    "            if product_selectors:\n",
    "                print(f\"‚úÖ S√©lecteurs produits d√©tect√©s: {len(product_selectors)}\")\n",
    "                \n",
    "                # Phase 2: D√©tection s√©lecteurs reviews\n",
    "                review_selectors = self._detect_review_selectors(site_type, product_selectors)\n",
    "                detected['reviews'] = review_selectors\n",
    "                \n",
    "                if review_selectors:\n",
    "                    print(f\"‚úÖ S√©lecteurs reviews d√©tect√©s: {len(review_selectors)}\")\n",
    "                else:\n",
    "                    print(\"‚ö†Ô∏è S√©lecteurs reviews non d√©tect√©s, utilisation des d√©fauts\")\n",
    "                    detected['reviews'] = self.base_selectors[site_type]['reviews']\n",
    "            else:\n",
    "                print(\"‚ùå Impossible de d√©tecter les s√©lecteurs produits\")\n",
    "                return {}\n",
    "            \n",
    "            # Sauvegarde des s√©lecteurs\n",
    "            self._save_detected_selectors(detected, site_type)\n",
    "            \n",
    "            return detected\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur d√©tection: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def _detect_product_selectors(self, site_url, search_term, site_type):\n",
    "        \"\"\"D√©tecte les s√©lecteurs de produits avec tests multiples\"\"\"\n",
    "        \n",
    "        try:\n",
    "            # Construire URL de recherche\n",
    "            if site_type == 'amazon':\n",
    "                search_url = f\"https://www.amazon.com/s?k={search_term.replace(' ', '+')}\"\n",
    "            elif site_type == 'ebay':\n",
    "                search_url = f\"https://www.ebay.com/sch/i.html?_nkw={search_term.replace(' ', '+')}\"\n",
    "            else:\n",
    "                return {}\n",
    "            \n",
    "            print(f\"üåê Navigation vers: {search_url}\")\n",
    "            self.driver.get(search_url)\n",
    "            \n",
    "            # Attendre le chargement\n",
    "            time.sleep(5)\n",
    "            \n",
    "            # Tester les s√©lecteurs de conteneurs\n",
    "            selectors = {}\n",
    "            base_selectors = self.base_selectors[site_type]['products']\n",
    "            \n",
    "            # Test conteneurs de produits\n",
    "            for selector in base_selectors['containers']:\n",
    "                try:\n",
    "                    elements = self.driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                    if len(elements) >= 3:  # Au moins 3 produits\n",
    "                        selectors['container'] = selector\n",
    "                        print(f\"‚úÖ Conteneur: {selector} ({len(elements)} √©l√©ments)\")\n",
    "                        break\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            if not selectors.get('container'):\n",
    "                print(\"‚ùå Aucun conteneur de produit trouv√©\")\n",
    "                return {}\n",
    "            \n",
    "            # Test autres s√©lecteurs dans le contexte du conteneur\n",
    "            container_elements = self.driver.find_elements(By.CSS_SELECTOR, selectors['container'])\n",
    "            \n",
    "            if container_elements:\n",
    "                first_container = container_elements[0]\n",
    "                \n",
    "                # Test titres\n",
    "                for title_selector in base_selectors['titles']:\n",
    "                    try:\n",
    "                        title_elem = first_container.find_element(By.CSS_SELECTOR, title_selector)\n",
    "                        if title_elem.text.strip():\n",
    "                            selectors['title'] = title_selector\n",
    "                            print(f\"‚úÖ Titre: {title_selector}\")\n",
    "                            break\n",
    "                    except:\n",
    "                        continue\n",
    "                \n",
    "                # Test URLs\n",
    "                for url_selector in base_selectors['urls']:\n",
    "                    try:\n",
    "                        url_elem = first_container.find_element(By.CSS_SELECTOR, url_selector)\n",
    "                        href = url_elem.get_attribute('href')\n",
    "                        if href and ('amazon.com' in href or 'ebay.com' in href):\n",
    "                            selectors['url'] = url_selector\n",
    "                            print(f\"‚úÖ URL: {url_selector}\")\n",
    "                            break\n",
    "                    except:\n",
    "                        continue\n",
    "                \n",
    "                # Test prix\n",
    "                for price_selector in base_selectors['prices']:\n",
    "                    try:\n",
    "                        price_elem = first_container.find_element(By.CSS_SELECTOR, price_selector)\n",
    "                        if price_elem.text.strip() and ('$' in price_elem.text or '‚Ç¨' in price_elem.text):\n",
    "                            selectors['price'] = price_selector\n",
    "                            print(f\"‚úÖ Prix: {price_selector}\")\n",
    "                            break\n",
    "                    except:\n",
    "                        continue\n",
    "                \n",
    "                # Test ratings\n",
    "                for rating_selector in base_selectors['ratings']:\n",
    "                    try:\n",
    "                        rating_elem = first_container.find_element(By.CSS_SELECTOR, rating_selector)\n",
    "                        rating_text = rating_elem.get_attribute('textContent') or rating_elem.text\n",
    "                        if rating_text and ('star' in rating_text.lower() or '√©toile' in rating_text.lower()):\n",
    "                            selectors['rating'] = rating_selector\n",
    "                            print(f\"‚úÖ Rating: {rating_selector}\")\n",
    "                            break\n",
    "                    except:\n",
    "                        continue\n",
    "            \n",
    "            return selectors\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur d√©tection produits: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def _detect_review_selectors(self, site_type, product_selectors):\n",
    "        \"\"\"D√©tecte les s√©lecteurs de reviews en naviguant vers un produit\"\"\"\n",
    "        \n",
    "        try:\n",
    "            # Trouver un lien produit\n",
    "            if not product_selectors.get('url'):\n",
    "                print(\"‚ùå Pas de s√©lecteur URL disponible\")\n",
    "                return {}\n",
    "            \n",
    "            # R√©cup√©rer le premier lien produit\n",
    "            product_links = self.driver.find_elements(By.CSS_SELECTOR, product_selectors['url'])\n",
    "            \n",
    "            if not product_links:\n",
    "                print(\"‚ùå Aucun lien produit trouv√©\")\n",
    "                return {}\n",
    "            \n",
    "            product_url = product_links[0].get_attribute('href')\n",
    "            print(f\"üîó Test reviews sur: {product_url[:80]}...\")\n",
    "            \n",
    "            # Naviguer vers le produit\n",
    "            self.driver.get(product_url)\n",
    "            time.sleep(3)\n",
    "            \n",
    "            # Chercher les reviews sur la page produit ou naviguer vers la page reviews\n",
    "            review_selectors = {}\n",
    "            base_selectors = self.base_selectors[site_type]['reviews']\n",
    "            \n",
    "            # D'abord, chercher un lien vers les reviews\n",
    "            review_link_selectors = [\n",
    "                'a[href*=\"customer-reviews\"]',\n",
    "                'a[href*=\"reviews\"]',\n",
    "                'a[href*=\"review\"]',\n",
    "                '.cr-widget-ACR a'\n",
    "            ]\n",
    "            \n",
    "            review_page_found = False\n",
    "            for link_selector in review_link_selectors:\n",
    "                try:\n",
    "                    review_links = self.driver.find_elements(By.CSS_SELECTOR, link_selector)\n",
    "                    for link in review_links:\n",
    "                        href = link.get_attribute('href')\n",
    "                        if href and 'review' in href:\n",
    "                            print(f\"üîó Navigation vers page reviews: {href[:80]}...\")\n",
    "                            self.driver.get(href)\n",
    "                            time.sleep(3)\n",
    "                            review_page_found = True\n",
    "                            break\n",
    "                    if review_page_found:\n",
    "                        break\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            # Tester les s√©lecteurs de reviews\n",
    "            # Test conteneurs\n",
    "            for container_selector in base_selectors['containers']:\n",
    "                try:\n",
    "                    review_elements = self.driver.find_elements(By.CSS_SELECTOR, container_selector)\n",
    "                    if len(review_elements) >= 2:  # Au moins 2 reviews\n",
    "                        review_selectors['container'] = container_selector\n",
    "                        print(f\"‚úÖ Conteneur reviews: {container_selector} ({len(review_elements)} reviews)\")\n",
    "                        break\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            if review_selectors.get('container'):\n",
    "                # Tester les autres s√©lecteurs dans le contexte\n",
    "                review_containers = self.driver.find_elements(By.CSS_SELECTOR, review_selectors['container'])\n",
    "                \n",
    "                if review_containers:\n",
    "                    first_review = review_containers[0]\n",
    "                    \n",
    "                    # Test texte de review\n",
    "                    for text_selector in base_selectors['texts']:\n",
    "                        try:\n",
    "                            text_elem = first_review.find_element(By.CSS_SELECTOR, text_selector)\n",
    "                            if text_elem.text.strip() and len(text_elem.text.strip()) > 20:\n",
    "                                review_selectors['text'] = text_selector\n",
    "                                print(f\"‚úÖ Texte review: {text_selector}\")\n",
    "                                break\n",
    "                        except:\n",
    "                            continue\n",
    "                    \n",
    "                    # Test titre de review\n",
    "                    for title_selector in base_selectors['titles']:\n",
    "                        try:\n",
    "                            title_elem = first_review.find_element(By.CSS_SELECTOR, title_selector)\n",
    "                            if title_elem.text.strip():\n",
    "                                review_selectors['title'] = title_selector\n",
    "                                print(f\"‚úÖ Titre review: {title_selector}\")\n",
    "                                break\n",
    "                        except:\n",
    "                            continue\n",
    "                    \n",
    "                    # Test rating\n",
    "                    for rating_selector in base_selectors['ratings']:\n",
    "                        try:\n",
    "                            rating_elem = first_review.find_element(By.CSS_SELECTOR, rating_selector)\n",
    "                            rating_text = rating_elem.get_attribute('textContent') or rating_elem.text\n",
    "                            if rating_text and ('star' in rating_text.lower() or '√©toile' in rating_text.lower()):\n",
    "                                review_selectors['rating'] = rating_selector\n",
    "                                print(f\"‚úÖ Rating review: {rating_selector}\")\n",
    "                                break\n",
    "                        except:\n",
    "                            continue\n",
    "                    \n",
    "                    # Test auteur\n",
    "                    for author_selector in base_selectors['authors']:\n",
    "                        try:\n",
    "                            author_elem = first_review.find_element(By.CSS_SELECTOR, author_selector)\n",
    "                            if author_elem.text.strip():\n",
    "                                review_selectors['author'] = author_selector\n",
    "                                print(f\"‚úÖ Auteur review: {author_selector}\")\n",
    "                                break\n",
    "                        except:\n",
    "                            continue\n",
    "                    \n",
    "                    # Test date\n",
    "                    for date_selector in base_selectors['dates']:\n",
    "                        try:\n",
    "                            date_elem = first_review.find_element(By.CSS_SELECTOR, date_selector)\n",
    "                            if date_elem.text.strip():\n",
    "                                review_selectors['date'] = date_selector\n",
    "                                print(f\"‚úÖ Date review: {date_selector}\")\n",
    "                                break\n",
    "                        except:\n",
    "                            continue\n",
    "            \n",
    "            return review_selectors\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur d√©tection reviews: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def _save_detected_selectors(self, selectors, site_type):\n",
    "        \"\"\"Sauvegarde les s√©lecteurs d√©tect√©s\"\"\"\n",
    "        \n",
    "        try:\n",
    "            import os\n",
    "            import json\n",
    "            \n",
    "            config_dir = \"../config\"\n",
    "            os.makedirs(config_dir, exist_ok=True)\n",
    "            \n",
    "            filename = f\"{config_dir}/detected_selectors_{site_type}.json\"\n",
    "            \n",
    "            with open(filename, 'w', encoding='utf-8') as f:\n",
    "                json.dump(selectors, f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "            print(f\"‚úÖ S√©lecteurs sauvegard√©s: {filename}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur sauvegarde: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Ferme proprement le driver\"\"\"\n",
    "        if self.driver:\n",
    "            try:\n",
    "                self.driver.quit()\n",
    "                print(\"‚úÖ Driver scout ferm√©\")\n",
    "            except:\n",
    "                pass\n",
    "            self.driver = None\n",
    "\n",
    "print(\"‚úÖ Scout robuste cr√©√©!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06957f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Scraper robuste cr√©√©!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SCRAPER ROBUSTE POUR REVIEWS DE PRODUITS\n",
    "# ============================================================================\n",
    "\n",
    "class RobustProductReviewScraper:\n",
    "    \"\"\"\n",
    "    Scraper ultra-robuste pour r√©cup√©rer les reviews de produits\n",
    "    Utilise les s√©lecteurs d√©tect√©s par le scout\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, selectors_file=None):\n",
    "        self.driver = None\n",
    "        self.selectors = {}\n",
    "        self.scraped_data = []\n",
    "        self.session_stats = {\n",
    "            'products_processed': 0,\n",
    "            'reviews_collected': 0,\n",
    "            'errors': 0,\n",
    "            'start_time': None\n",
    "        }\n",
    "        \n",
    "        if selectors_file:\n",
    "            self.load_selectors(selectors_file)\n",
    "    \n",
    "    def load_selectors(self, filename):\n",
    "        \"\"\"Charge les s√©lecteurs depuis un fichier JSON\"\"\"\n",
    "        try:\n",
    "            import json\n",
    "            with open(filename, 'r', encoding='utf-8') as f:\n",
    "                self.selectors = json.load(f)\n",
    "            print(f\"‚úÖ S√©lecteurs charg√©s: {filename}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur chargement s√©lecteurs: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def setup_robust_driver(self, headless=False, timeout=60):\n",
    "        \"\"\"Configuration driver ultra-robuste pour scraping\"\"\"\n",
    "        \n",
    "        max_attempts = 3\n",
    "        \n",
    "        for attempt in range(max_attempts):\n",
    "            try:\n",
    "                print(f\"üöÄ Tentative {attempt + 1}/{max_attempts} - Setup driver scraper...\")\n",
    "                \n",
    "                # Fermer le driver existant\n",
    "                if self.driver:\n",
    "                    try:\n",
    "                        self.driver.quit()\n",
    "                    except:\n",
    "                        pass\n",
    "                    self.driver = None\n",
    "                \n",
    "                # Options anti-d√©tection\n",
    "                options = self._create_stealth_options(headless)\n",
    "                \n",
    "                # Cr√©ation du driver\n",
    "                import undetected_chromedriver as uc\n",
    "                \n",
    "                self.driver = uc.Chrome(\n",
    "                    options=options,\n",
    "                    version_main=None,\n",
    "                    headless=headless,\n",
    "                    use_subprocess=False,\n",
    "                    log_level=3\n",
    "                )\n",
    "                \n",
    "                # Configuration timeouts\n",
    "                self.driver.set_page_load_timeout(timeout)\n",
    "                self.driver.implicitly_wait(15)\n",
    "                \n",
    "                # Scripts anti-d√©tection\n",
    "                self._inject_stealth_scripts()\n",
    "                \n",
    "                # Test fonctionnement\n",
    "                self.driver.get(\"data:text/html,<html><body><h1>Scraper Ready</h1></body></html>\")\n",
    "                \n",
    "                print(\"‚úÖ Driver scraper pr√™t!\")\n",
    "                print(f\"üé≠ User-Agent: {self.driver.execute_script('return navigator.userAgent;')[:80]}...\")\n",
    "                \n",
    "                return True\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Tentative {attempt + 1} √©chou√©e: {str(e)[:100]}...\")\n",
    "                if attempt < max_attempts - 1:\n",
    "                    time.sleep(5)\n",
    "                else:\n",
    "                    print(\"‚ùå Impossible de cr√©er le driver scraper\")\n",
    "                    return False\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def _create_stealth_options(self, headless=True):\n",
    "        \"\"\"Cr√©e des options Chrome furtives\"\"\"\n",
    "        \n",
    "        import undetected_chromedriver as uc\n",
    "        \n",
    "        options = uc.ChromeOptions()\n",
    "        \n",
    "        # Arguments furtifs\n",
    "        stealth_args = [\n",
    "            '--no-sandbox',\n",
    "            '--disable-dev-shm-usage',\n",
    "            '--disable-gpu',\n",
    "            '--disable-web-security',\n",
    "            '--disable-features=VizDisplayCompositor',\n",
    "            '--disable-blink-features=AutomationControlled',\n",
    "            '--disable-extensions',\n",
    "            '--no-first-run',\n",
    "            '--no-default-browser-check',\n",
    "            '--disable-default-apps',\n",
    "            '--disable-background-timer-throttling',\n",
    "            '--disable-backgrounding-occluded-windows',\n",
    "            '--disable-renderer-backgrounding',\n",
    "            '--window-size=1920,1080',\n",
    "            '--start-maximized'\n",
    "        ]\n",
    "        \n",
    "        for arg in stealth_args:\n",
    "            options.add_argument(arg)\n",
    "        \n",
    "        if headless:\n",
    "            options.add_argument('--headless=new')\n",
    "        \n",
    "        # User agent al√©atoire r√©aliste\n",
    "        try:\n",
    "            user_agent = random.choice(REALISTIC_USER_AGENTS)\n",
    "            options.add_argument(f'--user-agent={user_agent}')\n",
    "        except:\n",
    "            options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')\n",
    "        \n",
    "        # Pr√©f√©rences furtives\n",
    "        prefs = {\n",
    "            \"profile.default_content_setting_values.notifications\": 2,\n",
    "            \"profile.default_content_settings.popups\": 0,\n",
    "            \"profile.managed_default_content_settings.images\": 1,  # Charger les images\n",
    "            \"profile.default_content_setting_values.plugins\": 1,\n",
    "            \"profile.content_settings.plugin_whitelist.adobe-flash-player\": 1,\n",
    "            \"profile.content_settings.exceptions.plugins.*,*.per_resource.adobe-flash-player\": 1\n",
    "        }\n",
    "        options.add_experimental_option(\"prefs\", prefs)\n",
    "        \n",
    "        return options\n",
    "    \n",
    "    def _inject_stealth_scripts(self):\n",
    "        \"\"\"Injecte des scripts anti-d√©tection\"\"\"\n",
    "        try:\n",
    "            stealth_script = '''\n",
    "                Object.defineProperty(navigator, 'webdriver', {\n",
    "                    get: () => undefined,\n",
    "                });\n",
    "                \n",
    "                Object.defineProperty(navigator, 'plugins', {\n",
    "                    get: () => [1, 2, 3, 4, 5],\n",
    "                });\n",
    "                \n",
    "                Object.defineProperty(navigator, 'languages', {\n",
    "                    get: () => ['en-US', 'en'],\n",
    "                });\n",
    "                \n",
    "                window.chrome = {\n",
    "                    runtime: {},\n",
    "                };\n",
    "                \n",
    "                Object.defineProperty(navigator, 'permissions', {\n",
    "                    get: () => ({\n",
    "                        query: () => Promise.resolve({ state: 'granted' }),\n",
    "                    }),\n",
    "                });\n",
    "            '''\n",
    "            \n",
    "            self.driver.execute_cdp_cmd('Page.addScriptToEvaluateOnNewDocument', {\n",
    "                'source': stealth_script\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Scripts anti-d√©tection non inject√©s: {e}\")\n",
    "    \n",
    "    def scrape_category_reviews(self, category, site='amazon', max_products=10, reviews_per_rating=50):\n",
    "        \"\"\"\n",
    "        Scrape principal pour r√©cup√©rer les reviews d'une cat√©gorie\n",
    "        \"\"\"\n",
    "        \n",
    "        if not self.driver:\n",
    "            print(\"‚ùå Driver non initialis√©\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        if not self.selectors:\n",
    "            print(\"‚ùå S√©lecteurs non charg√©s\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        print(\"=\"*80)\n",
    "        print(\"üéØ D√âBUT DU SCRAPING ROBUSTE\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"üì¶ Cat√©gorie: {category}\")\n",
    "        print(f\"üåê Site: {site}\")\n",
    "        print(f\"üìä Produits max: {max_products}\")\n",
    "        print(f\"‚≠ê Reviews par note: {reviews_per_rating}\")\n",
    "        print()\n",
    "        \n",
    "        # Initialiser les stats\n",
    "        self.session_stats['start_time'] = time.time()\n",
    "        \n",
    "        try:\n",
    "            # Phase 1: R√©cup√©rer la liste des produits\n",
    "            products = self._get_products_list(category, site, max_products)\n",
    "            \n",
    "            if not products:\n",
    "                print(\"‚ùå Aucun produit trouv√©\")\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            print(f\"‚úÖ {len(products)} produits trouv√©s\")\n",
    "            \n",
    "            # Phase 2: Scraper les reviews de chaque produit\n",
    "            all_reviews = []\n",
    "            \n",
    "            for i, product in enumerate(products, 1):\n",
    "                print(f\"\\nüì¶ PRODUIT {i}/{len(products)}\")\n",
    "                print(f\"üè∑Ô∏è {product['title'][:60]}...\")\n",
    "                \n",
    "                try:\n",
    "                    product_reviews = self._scrape_product_reviews(\n",
    "                        product, \n",
    "                        reviews_per_rating,\n",
    "                        max_pages=5\n",
    "                    )\n",
    "                    \n",
    "                    if product_reviews:\n",
    "                        all_reviews.extend(product_reviews)\n",
    "                        self.session_stats['reviews_collected'] += len(product_reviews)\n",
    "                        print(f\"‚úÖ {len(product_reviews)} reviews r√©cup√©r√©es\")\n",
    "                    else:\n",
    "                        print(\"‚ö†Ô∏è Aucune review trouv√©e\")\n",
    "                    \n",
    "                    self.session_stats['products_processed'] += 1\n",
    "                    \n",
    "                    # D√©lai humain entre produits\n",
    "                    delay = random.uniform(3, 8)\n",
    "                    print(f\"‚è≥ D√©lai: {delay:.1f}s...\")\n",
    "                    time.sleep(delay)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå Erreur produit {i}: {e}\")\n",
    "                    self.session_stats['errors'] += 1\n",
    "                    continue\n",
    "            \n",
    "            # Phase 3: Cr√©er et nettoyer le DataFrame\n",
    "            if all_reviews:\n",
    "                df = pd.DataFrame(all_reviews)\n",
    "                df = self._clean_review_data(df)\n",
    "                \n",
    "                # Stats finales\n",
    "                duration = time.time() - self.session_stats['start_time']\n",
    "                print(\"\\n\" + \"=\"*80)\n",
    "                print(\"üìä SCRAPING TERMIN√â - STATISTIQUES\")\n",
    "                print(\"=\"*80)\n",
    "                print(f\"‚è±Ô∏è Dur√©e: {duration/60:.1f} minutes\")\n",
    "                print(f\"üì¶ Produits trait√©s: {self.session_stats['products_processed']}\")\n",
    "                print(f\"üìù Reviews r√©cup√©r√©es: {self.session_stats['reviews_collected']}\")\n",
    "                print(f\"‚ùå Erreurs: {self.session_stats['errors']}\")\n",
    "                print(f\"üìà Taux de succ√®s: {(1-self.session_stats['errors']/max(1,len(products)))*100:.1f}%\")\n",
    "                \n",
    "                return df\n",
    "            else:\n",
    "                print(\"‚ùå Aucune review r√©cup√©r√©e\")\n",
    "                return pd.DataFrame()\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur scraping global: {e}\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def _get_products_list(self, category, site, max_products):\n",
    "        \"\"\"R√©cup√®re la liste des produits √† scraper\"\"\"\n",
    "        \n",
    "        try:\n",
    "            # URL de recherche\n",
    "            if site == 'amazon':\n",
    "                search_url = f\"https://www.amazon.com/s?k={category.replace(' ', '+')}\"\n",
    "            elif site == 'ebay':\n",
    "                search_url = f\"https://www.ebay.com/sch/i.html?_nkw={category.replace(' ', '+')}\"\n",
    "            else:\n",
    "                print(f\"‚ùå Site non support√©: {site}\")\n",
    "                return []\n",
    "            \n",
    "            print(f\"üîç Recherche: {search_url}\")\n",
    "            self.driver.get(search_url)\n",
    "            \n",
    "            # Attendre le chargement\n",
    "            time.sleep(5)\n",
    "            \n",
    "            # S√©lecteurs pour ce site\n",
    "            site_selectors = self.selectors.get('products', {})\n",
    "            \n",
    "            if not site_selectors:\n",
    "                print(\"‚ùå S√©lecteurs produits non disponibles\")\n",
    "                return []\n",
    "            \n",
    "            # R√©cup√©rer les conteneurs de produits\n",
    "            container_selector = site_selectors.get('container')\n",
    "            if not container_selector:\n",
    "                print(\"‚ùå S√©lecteur conteneur manquant\")\n",
    "                return []\n",
    "            \n",
    "            containers = self.driver.find_elements(By.CSS_SELECTOR, container_selector)\n",
    "            print(f\"üì¶ {len(containers)} conteneurs trouv√©s\")\n",
    "            \n",
    "            products = []\n",
    "            \n",
    "            for i, container in enumerate(containers[:max_products]):\n",
    "                try:\n",
    "                    product_data = self._extract_product_info(container, site_selectors, category)\n",
    "                    \n",
    "                    if product_data and product_data.get('url'):\n",
    "                        products.append(product_data)\n",
    "                        print(f\"‚úÖ Produit {len(products)}: {product_data['title'][:40]}...\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è Produit {i+1} ignor√©: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            return products\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur r√©cup√©ration produits: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def _extract_product_info(self, container, selectors, category):\n",
    "        \"\"\"Extrait les infos d'un produit depuis son conteneur\"\"\"\n",
    "        \n",
    "        product_data = {\n",
    "            'category': category,\n",
    "            'scraped_at': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        # Titre\n",
    "        try:\n",
    "            if selectors.get('title'):\n",
    "                title_elem = container.find_element(By.CSS_SELECTOR, selectors['title'])\n",
    "                product_data['title'] = title_elem.text.strip()\n",
    "        except:\n",
    "            product_data['title'] = 'Titre non trouv√©'\n",
    "        \n",
    "        # URL\n",
    "        try:\n",
    "            if selectors.get('url'):\n",
    "                url_elem = container.find_element(By.CSS_SELECTOR, selectors['url'])\n",
    "                product_data['url'] = url_elem.get_attribute('href')\n",
    "        except:\n",
    "            product_data['url'] = None\n",
    "        \n",
    "        # Prix\n",
    "        try:\n",
    "            if selectors.get('price'):\n",
    "                price_elem = container.find_element(By.CSS_SELECTOR, selectors['price'])\n",
    "                product_data['price'] = price_elem.text.strip()\n",
    "        except:\n",
    "            product_data['price'] = 'N/A'\n",
    "        \n",
    "        # Rating\n",
    "        try:\n",
    "            if selectors.get('rating'):\n",
    "                rating_elem = container.find_element(By.CSS_SELECTOR, selectors['rating'])\n",
    "                rating_text = rating_elem.get_attribute('textContent') or rating_elem.text\n",
    "                product_data['rating'] = rating_text.strip()\n",
    "        except:\n",
    "            product_data['rating'] = 'N/A'\n",
    "        \n",
    "        return product_data\n",
    "    \n",
    "    def _scrape_product_reviews(self, product, reviews_per_rating, max_pages=5):\n",
    "        \"\"\"Scrape les reviews d'un produit sp√©cifique\"\"\"\n",
    "        \n",
    "        try:\n",
    "            # Naviguer vers le produit\n",
    "            self.driver.get(product['url'])\n",
    "            time.sleep(3)\n",
    "            \n",
    "            # Trouver la page des reviews\n",
    "            reviews_url = self._find_reviews_page(product['url'])\n",
    "            \n",
    "            if reviews_url:\n",
    "                self.driver.get(reviews_url)\n",
    "                time.sleep(3)\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è Page reviews non trouv√©e, tentative sur page produit\")\n",
    "            \n",
    "            # R√©cup√©rer les reviews\n",
    "            all_reviews = []\n",
    "            \n",
    "            # S√©lecteurs reviews\n",
    "            review_selectors = self.selectors.get('reviews', {})\n",
    "            \n",
    "            if not review_selectors:\n",
    "                print(\"‚ùå S√©lecteurs reviews non disponibles\")\n",
    "                return []\n",
    "            \n",
    "            # Scraper les reviews page par page\n",
    "            for page in range(max_pages):\n",
    "                try:\n",
    "                    page_reviews = self._extract_reviews_from_page(product, review_selectors)\n",
    "                    \n",
    "                    if page_reviews:\n",
    "                        all_reviews.extend(page_reviews)\n",
    "                        print(f\"üìù Page {page+1}: {len(page_reviews)} reviews\")\n",
    "                        \n",
    "                        # Essayer de passer √† la page suivante\n",
    "                        if not self._go_to_next_page():\n",
    "                            print(\"üìÑ Plus de pages disponibles\")\n",
    "                            break\n",
    "                        \n",
    "                        time.sleep(random.uniform(2, 4))\n",
    "                    else:\n",
    "                        print(f\"üìÑ Page {page+1}: aucune review\")\n",
    "                        break\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå Erreur page {page+1}: {e}\")\n",
    "                    break\n",
    "            \n",
    "            # Limiter le nombre de reviews si n√©cessaire\n",
    "            if len(all_reviews) > reviews_per_rating * 5:  # 5 notes possibles\n",
    "                all_reviews = all_reviews[:reviews_per_rating * 5]\n",
    "            \n",
    "            return all_reviews\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur scraping reviews produit: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def _find_reviews_page(self, product_url):\n",
    "        \"\"\"Trouve l'URL de la page des reviews\"\"\"\n",
    "        \n",
    "        try:\n",
    "            # S√©lecteurs de liens vers reviews\n",
    "            review_link_selectors = [\n",
    "                'a[href*=\"customer-reviews\"]',\n",
    "                'a[href*=\"reviews\"]',\n",
    "                '[data-hook=\"see-all-reviews-link-foot\"]',\n",
    "                '.cr-widget-ACR a'\n",
    "            ]\n",
    "            \n",
    "            for selector in review_link_selectors:\n",
    "                try:\n",
    "                    links = self.driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                    for link in links:\n",
    "                        href = link.get_attribute('href')\n",
    "                        if href and 'review' in href:\n",
    "                            return href\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            # Si aucun lien trouv√©, construire l'URL pour Amazon\n",
    "            if 'amazon.com' in product_url:\n",
    "                import re\n",
    "                asin_match = re.search(r'/dp/([A-Z0-9]{10})', product_url)\n",
    "                if asin_match:\n",
    "                    asin = asin_match.group(1)\n",
    "                    return f\"https://www.amazon.com/product-reviews/{asin}\"\n",
    "            \n",
    "            return None\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Erreur recherche page reviews: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _extract_reviews_from_page(self, product, review_selectors):\n",
    "        \"\"\"Extrait les reviews de la page actuelle\"\"\"\n",
    "        \n",
    "        reviews = []\n",
    "        \n",
    "        try:\n",
    "            # R√©cup√©rer les conteneurs de reviews\n",
    "            container_selector = review_selectors.get('container')\n",
    "            if not container_selector:\n",
    "                return []\n",
    "            \n",
    "            review_containers = self.driver.find_elements(By.CSS_SELECTOR, container_selector)\n",
    "            \n",
    "            for container in review_containers:\n",
    "                try:\n",
    "                    review_data = {\n",
    "                        'product_name': product['title'],\n",
    "                        'product_category': product['category'],\n",
    "                        'product_url': product['url'],\n",
    "                        'product_price': product.get('price', 'N/A'),\n",
    "                        'scraped_at': datetime.now().isoformat()\n",
    "                    }\n",
    "                    \n",
    "                    # Texte de la review\n",
    "                    if review_selectors.get('text'):\n",
    "                        try:\n",
    "                            text_elem = container.find_element(By.CSS_SELECTOR, review_selectors['text'])\n",
    "                            review_data['review_text'] = text_elem.text.strip()\n",
    "                        except:\n",
    "                            review_data['review_text'] = ''\n",
    "                    \n",
    "                    # Titre de la review\n",
    "                    if review_selectors.get('title'):\n",
    "                        try:\n",
    "                            title_elem = container.find_element(By.CSS_SELECTOR, review_selectors['title'])\n",
    "                            review_data['review_title'] = title_elem.text.strip()\n",
    "                        except:\n",
    "                            review_data['review_title'] = ''\n",
    "                    \n",
    "                    # Note de la review\n",
    "                    if review_selectors.get('rating'):\n",
    "                        try:\n",
    "                            rating_elem = container.find_element(By.CSS_SELECTOR, review_selectors['rating'])\n",
    "                            rating_text = rating_elem.get_attribute('textContent') or rating_elem.text\n",
    "                            # Extraire le chiffre de la note\n",
    "                            import re\n",
    "                            rating_match = re.search(r'(\\d+(?:\\.\\d+)?)', rating_text)\n",
    "                            review_data['user_rating'] = float(rating_match.group(1)) if rating_match else None\n",
    "                        except:\n",
    "                            review_data['user_rating'] = None\n",
    "                    \n",
    "                    # Auteur\n",
    "                    if review_selectors.get('author'):\n",
    "                        try:\n",
    "                            author_elem = container.find_element(By.CSS_SELECTOR, review_selectors['author'])\n",
    "                            review_data['reviewer_name'] = author_elem.text.strip()\n",
    "                        except:\n",
    "                            review_data['reviewer_name'] = 'Anonymous'\n",
    "                    \n",
    "                    # Date\n",
    "                    if review_selectors.get('date'):\n",
    "                        try:\n",
    "                            date_elem = container.find_element(By.CSS_SELECTOR, review_selectors['date'])\n",
    "                            review_data['review_date'] = date_elem.text.strip()\n",
    "                        except:\n",
    "                            review_data['review_date'] = 'N/A'\n",
    "                    \n",
    "                    # Ajouter si on a du contenu utile\n",
    "                    if (review_data.get('review_text') and len(review_data['review_text']) > 10) or \\\n",
    "                       (review_data.get('review_title') and len(review_data['review_title']) > 5):\n",
    "                        reviews.append(review_data)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    continue  # Ignorer les reviews probl√©matiques\n",
    "            \n",
    "            return reviews\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur extraction reviews: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def _go_to_next_page(self):\n",
    "        \"\"\"Tente de passer √† la page suivante\"\"\"\n",
    "        \n",
    "        try:\n",
    "            # S√©lecteurs de bouton \"suivant\"\n",
    "            next_selectors = [\n",
    "                '.a-pagination .a-last a',\n",
    "                'a[aria-label=\"Next page\"]',\n",
    "                '.a-pagination li:last-child a',\n",
    "                'a[href*=\"pageNumber\"]'\n",
    "            ]\n",
    "            \n",
    "            for selector in next_selectors:\n",
    "                try:\n",
    "                    next_button = self.driver.find_element(By.CSS_SELECTOR, selector)\n",
    "                    if next_button.is_enabled():\n",
    "                        next_button.click()\n",
    "                        time.sleep(2)\n",
    "                        return True\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            return False\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Erreur navigation page suivante: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def _clean_review_data(self, df):\n",
    "        \"\"\"Nettoie et optimise les donn√©es r√©cup√©r√©es\"\"\"\n",
    "        \n",
    "        try:\n",
    "            print(\"üßπ Nettoyage des donn√©es...\")\n",
    "            \n",
    "            # Supprimer les doublons\n",
    "            initial_count = len(df)\n",
    "            df = df.drop_duplicates(subset=['review_text', 'product_name'], keep='first')\n",
    "            print(f\"üìù Doublons supprim√©s: {initial_count - len(df)}\")\n",
    "            \n",
    "            # Nettoyer les textes\n",
    "            df['review_text'] = df['review_text'].str.strip()\n",
    "            df['review_title'] = df['review_title'].str.strip()\n",
    "            df['product_name'] = df['product_name'].str.strip()\n",
    "            \n",
    "            # Filtrer les reviews trop courtes\n",
    "            df = df[df['review_text'].str.len() > 15]\n",
    "            \n",
    "            # Ajouter des m√©triques\n",
    "            df['review_length'] = df['review_text'].str.len()\n",
    "            df['word_count'] = df['review_text'].str.split().str.len()\n",
    "            \n",
    "            # Nettoyer les ratings\n",
    "            df['user_rating'] = pd.to_numeric(df['user_rating'], errors='coerce')\n",
    "            \n",
    "            print(f\"‚úÖ {len(df)} reviews nettoy√©es\")\n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur nettoyage: {e}\")\n",
    "            return df\n",
    "    \n",
    "    def save_data(self, df, filename=None):\n",
    "        \"\"\"Sauvegarde les donn√©es avec horodatage\"\"\"\n",
    "        \n",
    "        try:\n",
    "            if filename is None:\n",
    "                timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "                filename = f\"../data/raw/reviews_robustes_{timestamp}.csv\"\n",
    "            \n",
    "            import os\n",
    "            os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "            \n",
    "            df.to_csv(filename, index=False, encoding='utf-8')\n",
    "            print(f\"üíæ Donn√©es sauvegard√©es: {filename}\")\n",
    "            \n",
    "            # Sauvegarder aussi en JSON pour backup\n",
    "            json_filename = filename.replace('.csv', '.json')\n",
    "            df.to_json(json_filename, orient='records', force_ascii=False, indent=2)\n",
    "            \n",
    "            return filename\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur sauvegarde: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Ferme proprement le scraper\"\"\"\n",
    "        if self.driver:\n",
    "            try:\n",
    "                self.driver.quit()\n",
    "                print(\"‚úÖ Driver scraper ferm√©\")\n",
    "            except:\n",
    "                pass\n",
    "            self.driver = None\n",
    "\n",
    "print(\"‚úÖ Scraper robuste cr√©√©!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97beaf6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Workflow robuste pr√™t!\n",
      "üìñ Utilisez robust_reviews_menu() pour commencer\n",
      "üöÄ Ou run_example('laptops_gaming') pour un test rapide\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# WORKFLOW PRINCIPAL ROBUSTE\n",
    "# ============================================================================\n",
    "\n",
    "def robust_reviews_workflow(category=\"laptop\", site=\"amazon\", max_products=10, reviews_per_rating=50, headless=False):\n",
    "    \"\"\"\n",
    "    Workflow principal ultra-robuste pour scraper les reviews de produits\n",
    "    \n",
    "    Phase 1: D√©tection automatique des balises (Scout)\n",
    "    Phase 2: Scraping des reviews avec balises valid√©es (Scraper)\n",
    "    \n",
    "    Args:\n",
    "        category: cat√©gorie de produits (ex: \"laptop\", \"smartphone\")\n",
    "        site: site √† scraper (\"amazon\" ou \"ebay\")\n",
    "        max_products: nombre max de produits √† analyser\n",
    "        reviews_per_rating: nombre de reviews √† r√©cup√©rer par note\n",
    "        headless: mode sans interface (True) ou visible (False)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\"*100)\n",
    "    print(\"üöÄ WORKFLOW ROBUSTE - SCRAPING REVIEWS DE PRODUITS\")\n",
    "    print(\"=\"*100)\n",
    "    print(f\"üì¶ Cat√©gorie: {category}\")\n",
    "    print(f\"üåê Site: {site}\")\n",
    "    print(f\"üìä Produits max: {max_products}\")\n",
    "    print(f\"‚≠ê Reviews par note: {reviews_per_rating}\")\n",
    "    print(f\"üëÅÔ∏è Mode: {'Headless' if headless else 'Visible'}\")\n",
    "    print(f\"üìà Total estim√©: {max_products * 5 * reviews_per_rating} reviews max\")\n",
    "    print()\n",
    "    \n",
    "    # Variables pour cleanup\n",
    "    scout = None\n",
    "    scraper = None\n",
    "    \n",
    "    try:\n",
    "        # ====================================================================\n",
    "        # PHASE 1: D√âTECTION DES BALISES (SCOUT)\n",
    "        # ====================================================================\n",
    "        print(\"üîç PHASE 1: D√âTECTION AUTOMATIQUE DES BALISES\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        scout = RobustProductReviewScout()\n",
    "        \n",
    "        # Setup du driver scout\n",
    "        if not scout.setup_robust_driver(headless=True, timeout=30):\n",
    "            print(\"‚ùå Impossible d'initialiser le scout\")\n",
    "            return None\n",
    "        \n",
    "        # D√©tection des s√©lecteurs\n",
    "        site_url = f\"https://www.{site}.com\"\n",
    "        detected_selectors = scout.detect_site_selectors(site_url, category)\n",
    "        \n",
    "        if not detected_selectors or not detected_selectors.get('products'):\n",
    "            print(\"‚ùå √âchec de la d√©tection des s√©lecteurs\")\n",
    "            scout.close()\n",
    "            return None\n",
    "        \n",
    "        print(\"‚úÖ S√©lecteurs d√©tect√©s avec succ√®s!\")\n",
    "        print(f\"üì¶ Produits: {list(detected_selectors['products'].keys())}\")\n",
    "        print(f\"üìù Reviews: {list(detected_selectors['reviews'].keys())}\")\n",
    "        \n",
    "        # Fermer le scout\n",
    "        scout.close()\n",
    "        scout = None\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        \n",
    "        # ====================================================================\n",
    "        # PHASE 2: SCRAPING DES REVIEWS (SCRAPER)\n",
    "        # ====================================================================\n",
    "        print(\"üìä PHASE 2: SCRAPING DES REVIEWS\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        scraper = RobustProductReviewScraper()\n",
    "        scraper.selectors = detected_selectors  # Utiliser les s√©lecteurs d√©tect√©s\n",
    "        \n",
    "        # Setup du driver scraper\n",
    "        if not scraper.setup_robust_driver(headless=headless, timeout=60):\n",
    "            print(\"‚ùå Impossible d'initialiser le scraper\")\n",
    "            return None\n",
    "        \n",
    "        # Avertissement utilisateur\n",
    "        if not headless:\n",
    "            print(\"\\nüö® AVERTISSEMENT: Scraping sur site r√©el en cours!\")\n",
    "            print(\"‚è∞ Dur√©e estim√©e: {:.1f} minutes\".format(max_products * 3))\n",
    "            print(\"üìù Respectez les ToS et les limitations de d√©bit\")\n",
    "            \n",
    "            response = input(\"\\nüîÑ Continuer? (o/n): \").strip().lower()\n",
    "            if response not in ['o', 'oui', 'y', 'yes', '']:\n",
    "                print(\"‚èπÔ∏è Arr√™t demand√© par l'utilisateur\")\n",
    "                scraper.close()\n",
    "                return None\n",
    "        \n",
    "        # Lancement du scraping\n",
    "        print(f\"\\nüéØ D√©but du scraping pour '{category}' sur {site}...\")\n",
    "        \n",
    "        df_reviews = scraper.scrape_category_reviews(\n",
    "            category=category,\n",
    "            site=site,\n",
    "            max_products=max_products,\n",
    "            reviews_per_rating=reviews_per_rating\n",
    "        )\n",
    "        \n",
    "        # ====================================================================\n",
    "        # PHASE 3: R√âSULTATS ET SAUVEGARDE\n",
    "        # ====================================================================\n",
    "        if df_reviews.empty:\n",
    "            print(\"‚ùå Aucune review r√©cup√©r√©e\")\n",
    "            scraper.close()\n",
    "            return None\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"üìä ANALYSE DES R√âSULTATS\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        # Statistiques d√©taill√©es\n",
    "        stats = {\n",
    "            'total_reviews': len(df_reviews),\n",
    "            'unique_products': df_reviews['product_name'].nunique(),\n",
    "            'avg_review_length': df_reviews['review_length'].mean(),\n",
    "            'rating_distribution': df_reviews['user_rating'].value_counts().sort_index(),\n",
    "            'top_products': df_reviews['product_name'].value_counts().head(5)\n",
    "        }\n",
    "        \n",
    "        print(f\"‚úÖ Total reviews: {stats['total_reviews']}\")\n",
    "        print(f\"üì¶ Produits uniques: {stats['unique_products']}\")\n",
    "        print(f\"üìù Longueur moyenne: {stats['avg_review_length']:.0f} caract√®res\")\n",
    "        print(f\"‚≠ê Distribution des notes:\")\n",
    "        for rating, count in stats['rating_distribution'].items():\n",
    "            if pd.notna(rating):\n",
    "                print(f\"   {rating} √©toiles: {count} reviews\")\n",
    "        \n",
    "        print(f\"\\nüèÜ Top produits par nombre de reviews:\")\n",
    "        for product, count in stats['top_products'].items():\n",
    "            print(f\"   ‚Ä¢ {product[:50]}... ({count} reviews)\")\n",
    "        \n",
    "        # Sauvegarde\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"../data/raw/{site}_{category}_{timestamp}.csv\"\n",
    "        \n",
    "        saved_file = scraper.save_data(df_reviews, filename)\n",
    "        \n",
    "        # Aper√ßu des donn√©es\n",
    "        print(f\"\\nüìã APER√áU DES DONN√âES (5 premi√®res reviews):\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        sample_cols = ['product_name', 'user_rating', 'review_text', 'reviewer_name']\n",
    "        available_cols = [col for col in sample_cols if col in df_reviews.columns]\n",
    "        \n",
    "        for i, row in df_reviews[available_cols].head(5).iterrows():\n",
    "            print(f\"\\nReview {i+1}:\")\n",
    "            for col in available_cols:\n",
    "                value = str(row[col])\n",
    "                if col == 'review_text' and len(value) > 100:\n",
    "                    value = value[:100] + \"...\"\n",
    "                elif col == 'product_name' and len(value) > 50:\n",
    "                    value = value[:50] + \"...\"\n",
    "                print(f\"  {col}: {value}\")\n",
    "        \n",
    "        # Fermer le scraper\n",
    "        scraper.close()\n",
    "        scraper = None\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*100)\n",
    "        print(\"üéâ WORKFLOW TERMIN√â AVEC SUCC√àS!\")\n",
    "        print(\"=\"*100)\n",
    "        \n",
    "        return df_reviews\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n‚èπÔ∏è Arr√™t demand√© par l'utilisateur\")\n",
    "        return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Erreur workflow: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "        \n",
    "    finally:\n",
    "        # Cleanup des ressources\n",
    "        if scout:\n",
    "            try:\n",
    "                scout.close()\n",
    "            except:\n",
    "                pass\n",
    "        if scraper:\n",
    "            try:\n",
    "                scraper.close()\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "def quick_scout_test(site=\"amazon\", category=\"laptop\"):\n",
    "    \"\"\"\n",
    "    Test rapide du scout uniquement\n",
    "    \"\"\"\n",
    "    print(f\"üß™ TEST RAPIDE - Scout pour {category} sur {site}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    scout = RobustProductReviewScout()\n",
    "    \n",
    "    try:\n",
    "        if scout.setup_robust_driver(headless=True):\n",
    "            site_url = f\"https://www.{site}.com\"\n",
    "            selectors = scout.detect_site_selectors(site_url, category)\n",
    "            \n",
    "            if selectors:\n",
    "                print(\"‚úÖ Test scout r√©ussi!\")\n",
    "                print(f\"üì¶ S√©lecteurs produits: {list(selectors['products'].keys())}\")\n",
    "                print(f\"üìù S√©lecteurs reviews: {list(selectors['reviews'].keys())}\")\n",
    "                return selectors\n",
    "            else:\n",
    "                print(\"‚ùå Test scout √©chou√©\")\n",
    "                return None\n",
    "        else:\n",
    "            print(\"‚ùå Impossible de cr√©er le driver scout\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur test scout: {e}\")\n",
    "        return None\n",
    "        \n",
    "    finally:\n",
    "        scout.close()\n",
    "\n",
    "def robust_reviews_menu():\n",
    "    \"\"\"\n",
    "    Menu principal pour le workflow robuste\n",
    "    \"\"\"\n",
    "    print(\"=\"*100)\n",
    "    print(\"üéØ WORKFLOW ROBUSTE - REVIEWS DE PRODUITS\")\n",
    "    print(\"=\"*100)\n",
    "    print()\n",
    "    print(\"1Ô∏è‚É£ Workflow complet (scout + scraper)\")\n",
    "    print(\"2Ô∏è‚É£ Test scout uniquement\")\n",
    "    print(\"3Ô∏è‚É£ Configuration avanc√©e\")\n",
    "    print(\"4Ô∏è‚É£ Voir fichiers de donn√©es\")\n",
    "    print(\"5Ô∏è‚É£ Quitter\")\n",
    "    print()\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            choice = input(\"üëâ Votre choix (1-5): \").strip()\n",
    "            \n",
    "            if choice == '1':\n",
    "                print(\"\\nüìã CONFIGURATION DU WORKFLOW COMPLET\")\n",
    "                print(\"-\" * 50)\n",
    "                \n",
    "                category = input(\"üè∑Ô∏è Cat√©gorie (ex: laptop, smartphone): \").strip() or \"laptop\"\n",
    "                site = input(\"üåê Site (amazon/ebay): \").strip() or \"amazon\"\n",
    "                \n",
    "                try:\n",
    "                    max_products = int(input(\"üì¶ Nombre de produits (d√©faut: 5): \") or \"5\")\n",
    "                    reviews_per_rating = int(input(\"‚≠ê Reviews par note (d√©faut: 20): \") or \"20\")\n",
    "                except ValueError:\n",
    "                    max_products, reviews_per_rating = 5, 20\n",
    "                \n",
    "                headless_choice = input(\"üëÅÔ∏è Mode headless? (o/n, d√©faut: n): \").strip().lower()\n",
    "                headless = headless_choice in ['o', 'oui', 'y', 'yes']\n",
    "                \n",
    "                print(f\"\\nüöÄ Lancement du workflow...\")\n",
    "                result = robust_reviews_workflow(category, site, max_products, reviews_per_rating, headless)\n",
    "                \n",
    "                if result is not None:\n",
    "                    print(f\"\\n‚úÖ Workflow termin√© - {len(result)} reviews r√©cup√©r√©es\")\n",
    "                else:\n",
    "                    print(\"\\n‚ùå Workflow √©chou√©\")\n",
    "                \n",
    "                return result\n",
    "                \n",
    "            elif choice == '2':\n",
    "                print(\"\\nüß™ TEST SCOUT\")\n",
    "                print(\"-\" * 30)\n",
    "                \n",
    "                site = input(\"üåê Site (amazon/ebay): \").strip() or \"amazon\"\n",
    "                category = input(\"üè∑Ô∏è Cat√©gorie: \").strip() or \"laptop\"\n",
    "                \n",
    "                result = quick_scout_test(site, category)\n",
    "                if result:\n",
    "                    print(\"‚úÖ Scout fonctionne correctement!\")\n",
    "                else:\n",
    "                    print(\"‚ùå Probl√®me avec le scout\")\n",
    "                \n",
    "            elif choice == '3':\n",
    "                print(\"\\n‚öôÔ∏è CONFIGURATION AVANC√âE\")\n",
    "                print(\"-\" * 40)\n",
    "                print(\"üìñ Param√®tres disponibles dans robust_reviews_workflow():\")\n",
    "                print(\"   ‚Ä¢ category: cat√©gorie de produits\")\n",
    "                print(\"   ‚Ä¢ site: amazon ou ebay\")\n",
    "                print(\"   ‚Ä¢ max_products: nombre de produits max\")\n",
    "                print(\"   ‚Ä¢ reviews_per_rating: reviews par note (1-5)\")\n",
    "                print(\"   ‚Ä¢ headless: mode sans interface\")\n",
    "                print(\"\\nüí° Exemple:\")\n",
    "                print(\"   df = robust_reviews_workflow('gaming laptop', 'amazon', 8, 30, False)\")\n",
    "                \n",
    "            elif choice == '4':\n",
    "                print(\"\\nüìÅ FICHIERS DE DONN√âES\")\n",
    "                print(\"-\" * 30)\n",
    "                \n",
    "                import os\n",
    "                data_dir = \"../data/raw\"\n",
    "                \n",
    "                if os.path.exists(data_dir):\n",
    "                    files = [f for f in os.listdir(data_dir) if f.endswith('.csv')]\n",
    "                    if files:\n",
    "                        print(\"üìÑ Fichiers CSV trouv√©s:\")\n",
    "                        for f in sorted(files, reverse=True)[:10]:  # 10 plus r√©cents\n",
    "                            size = os.path.getsize(os.path.join(data_dir, f)) / 1024  # KB\n",
    "                            print(f\"   ‚Ä¢ {f} ({size:.1f} KB)\")\n",
    "                    else:\n",
    "                        print(\"‚ùå Aucun fichier CSV trouv√©\")\n",
    "                else:\n",
    "                    print(\"‚ùå Dossier data/raw non trouv√©\")\n",
    "                \n",
    "            elif choice == '5':\n",
    "                print(\"üëã √Ä bient√¥t!\")\n",
    "                break\n",
    "                \n",
    "            else:\n",
    "                print(\"‚ùå Choix invalide\")\n",
    "                \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nüëã √Ä bient√¥t!\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur: {e}\")\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Configuration d'exemples pr√™ts √† l'emploi\n",
    "ROBUST_EXAMPLES = {\n",
    "    'laptops_gaming': {\n",
    "        'category': 'gaming laptop',\n",
    "        'site': 'amazon',\n",
    "        'max_products': 8,\n",
    "        'reviews_per_rating': 25,\n",
    "        'headless': False\n",
    "    },\n",
    "    'smartphones': {\n",
    "        'category': 'smartphone',\n",
    "        'site': 'amazon', \n",
    "        'max_products': 6,\n",
    "        'reviews_per_rating': 30,\n",
    "        'headless': False\n",
    "    },\n",
    "    'headphones': {\n",
    "        'category': 'wireless headphones',\n",
    "        'site': 'amazon',\n",
    "        'max_products': 10,\n",
    "        'reviews_per_rating': 20,\n",
    "        'headless': False\n",
    "    }\n",
    "}\n",
    "\n",
    "def run_example(example_name):\n",
    "    \"\"\"Ex√©cute un exemple pr√©d√©fini\"\"\"\n",
    "    if example_name in ROBUST_EXAMPLES:\n",
    "        config = ROBUST_EXAMPLES[example_name]\n",
    "        print(f\"üöÄ Lancement exemple: {example_name}\")\n",
    "        return robust_reviews_workflow(**config)\n",
    "    else:\n",
    "        print(f\"‚ùå Exemple '{example_name}' non trouv√©\")\n",
    "        print(f\"üìã Disponibles: {list(ROBUST_EXAMPLES.keys())}\")\n",
    "        return None\n",
    "\n",
    "print(\"‚úÖ Workflow robuste pr√™t!\")\n",
    "print(\"üìñ Utilisez robust_reviews_menu() pour commencer\")\n",
    "print(\"üöÄ Ou run_example('laptops_gaming') pour un test rapide\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8295519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Lancement exemple: laptops_gaming\n",
      "====================================================================================================\n",
      "üöÄ WORKFLOW ROBUSTE - SCRAPING REVIEWS DE PRODUITS\n",
      "====================================================================================================\n",
      "üì¶ Cat√©gorie: gaming laptop\n",
      "üåê Site: amazon\n",
      "üìä Produits max: 8\n",
      "‚≠ê Reviews par note: 25\n",
      "üëÅÔ∏è Mode: Visible\n",
      "üìà Total estim√©: 1000 reviews max\n",
      "\n",
      "üîç PHASE 1: D√âTECTION AUTOMATIQUE DES BALISES\n",
      "----------------------------------------------------------------------\n",
      "üîß Tentative 1/3 - Setup driver scout...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-29 15:02:31,911 - INFO - patching driver executable C:\\Users\\Yann\\appdata\\roaming\\undetected_chromedriver\\undetected_chromedriver.exe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Driver scout initialis√© avec succ√®s!\n",
      "üîç D√©tection des s√©lecteurs pour amazon...\n",
      "üåê Navigation vers: https://www.amazon.com/s?k=gaming+laptop\n",
      "‚úÖ Conteneur: [data-component-type=\"s-search-result\"] (16 √©l√©ments)\n",
      "‚úÖ Titre: h2 span\n",
      "‚úÖ Conteneur: [data-component-type=\"s-search-result\"] (16 √©l√©ments)\n",
      "‚úÖ Titre: h2 span\n",
      "‚úÖ URL: .a-link-normal\n",
      "‚úÖ URL: .a-link-normal\n",
      "‚úÖ Rating: .a-icon-alt\n",
      "‚úÖ S√©lecteurs produits d√©tect√©s: 4\n",
      "üîó Test reviews sur: https://www.amazon.com/ACEMAGIC-16-1inch-Computer-Windows-Graphics/dp/B0FB99VF5F...\n",
      "‚úÖ Rating: .a-icon-alt\n",
      "‚úÖ S√©lecteurs produits d√©tect√©s: 4\n",
      "üîó Test reviews sur: https://www.amazon.com/ACEMAGIC-16-1inch-Computer-Windows-Graphics/dp/B0FB99VF5F...\n",
      "üîó Navigation vers page reviews: https://www.amazon.com/ACEMAGIC-16-1inch-Computer-Windows-Graphics/dp/B0FB99VF5F...\n",
      "üîó Navigation vers page reviews: https://www.amazon.com/ACEMAGIC-16-1inch-Computer-Windows-Graphics/dp/B0FB99VF5F...\n",
      "‚úÖ Conteneur reviews: [data-hook=\"review\"] (12 reviews)\n",
      "‚úÖ Texte review: .review-text\n",
      "‚úÖ Titre review: .review-title\n",
      "‚úÖ Rating review: [data-hook=\"review-star-rating\"] .a-icon-alt\n",
      "‚úÖ Auteur review: .a-profile-name\n",
      "‚úÖ Date review: [data-hook=\"review-date\"]\n",
      "‚úÖ S√©lecteurs reviews d√©tect√©s: 6\n",
      "‚úÖ S√©lecteurs sauvegard√©s: ../config/detected_selectors_amazon.json\n",
      "‚úÖ S√©lecteurs d√©tect√©s avec succ√®s!\n",
      "üì¶ Produits: ['container', 'title', 'url', 'rating']\n",
      "üìù Reviews: ['container', 'text', 'title', 'rating', 'author', 'date']\n",
      "‚úÖ Conteneur reviews: [data-hook=\"review\"] (12 reviews)\n",
      "‚úÖ Texte review: .review-text\n",
      "‚úÖ Titre review: .review-title\n",
      "‚úÖ Rating review: [data-hook=\"review-star-rating\"] .a-icon-alt\n",
      "‚úÖ Auteur review: .a-profile-name\n",
      "‚úÖ Date review: [data-hook=\"review-date\"]\n",
      "‚úÖ S√©lecteurs reviews d√©tect√©s: 6\n",
      "‚úÖ S√©lecteurs sauvegard√©s: ../config/detected_selectors_amazon.json\n",
      "‚úÖ S√©lecteurs d√©tect√©s avec succ√®s!\n",
      "üì¶ Produits: ['container', 'title', 'url', 'rating']\n",
      "üìù Reviews: ['container', 'text', 'title', 'rating', 'author', 'date']\n",
      "‚úÖ Driver scout ferm√©\n",
      "\n",
      "======================================================================\n",
      "üìä PHASE 2: SCRAPING DES REVIEWS\n",
      "----------------------------------------------------------------------\n",
      "üöÄ Tentative 1/3 - Setup driver scraper...\n",
      "‚úÖ Driver scout ferm√©\n",
      "\n",
      "======================================================================\n",
      "üìä PHASE 2: SCRAPING DES REVIEWS\n",
      "----------------------------------------------------------------------\n",
      "üöÄ Tentative 1/3 - Setup driver scraper...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-29 15:03:15,744 - INFO - patching driver executable C:\\Users\\Yann\\appdata\\roaming\\undetected_chromedriver\\undetected_chromedriver.exe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Driver scraper pr√™t!\n",
      "üé≠ User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko)...\n",
      "\n",
      "üö® AVERTISSEMENT: Scraping sur site r√©el en cours!\n",
      "‚è∞ Dur√©e estim√©e: 24.0 minutes\n",
      "üìù Respectez les ToS et les limitations de d√©bit\n",
      "\n",
      "üéØ D√©but du scraping pour 'gaming laptop' sur amazon...\n",
      "================================================================================\n",
      "üéØ D√âBUT DU SCRAPING ROBUSTE\n",
      "================================================================================\n",
      "üì¶ Cat√©gorie: gaming laptop\n",
      "üåê Site: amazon\n",
      "üìä Produits max: 8\n",
      "‚≠ê Reviews par note: 25\n",
      "\n",
      "üîç Recherche: https://www.amazon.com/s?k=gaming+laptop\n",
      "‚ùå Erreur r√©cup√©ration produits: Message: invalid session id: session deleted as the browser has closed the connection\n",
      "from disconnected: not connected to DevTools\n",
      "  (Session info: chrome=138.0.7204.50)\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x0xd04493+62419]\n",
      "\tGetHandleVerifier [0x0xd044d4+62484]\n",
      "\t(No symbol) [0x0xb42133]\n",
      "\t(No symbol) [0x0xb31b40]\n",
      "\t(No symbol) [0x0xb4f912]\n",
      "\t(No symbol) [0x0xbb5d6c]\n",
      "\t(No symbol) [0x0xbd0159]\n",
      "\t(No symbol) [0x0xbaf266]\n",
      "\t(No symbol) [0x0xb7e852]\n",
      "\t(No symbol) [0x0xb7f6f4]\n",
      "\tGetHandleVerifier [0x0xf74773+2619059]\n",
      "\tGetHandleVerifier [0x0xf6fb8a+2599626]\n",
      "\tGetHandleVerifier [0x0xd2b03a+221050]\n",
      "\tGetHandleVerifier [0x0xd1b2b8+156152]\n",
      "\tGetHandleVerifier [0x0xd21c6d+183213]\n",
      "\tGetHandleVerifier [0x0xd0c378+94904]\n",
      "\tGetHandleVerifier [0x0xd0c502+95298]\n",
      "\tGetHandleVerifier [0x0xcf765a+9626]\n",
      "\tBaseThreadInitThunk [0x0x75055d49+25]\n",
      "\tRtlInitializeExceptionChain [0x0x7756d09b+107]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x0x7756d021+561]\n",
      "\n",
      "‚ùå Aucun produit trouv√©\n",
      "‚ùå Aucune review r√©cup√©r√©e\n",
      "‚úÖ Driver scraper ferm√©\n",
      "\n",
      "üéØ D√©but du scraping pour 'gaming laptop' sur amazon...\n",
      "================================================================================\n",
      "üéØ D√âBUT DU SCRAPING ROBUSTE\n",
      "================================================================================\n",
      "üì¶ Cat√©gorie: gaming laptop\n",
      "üåê Site: amazon\n",
      "üìä Produits max: 8\n",
      "‚≠ê Reviews par note: 25\n",
      "\n",
      "üîç Recherche: https://www.amazon.com/s?k=gaming+laptop\n",
      "‚ùå Erreur r√©cup√©ration produits: Message: invalid session id: session deleted as the browser has closed the connection\n",
      "from disconnected: not connected to DevTools\n",
      "  (Session info: chrome=138.0.7204.50)\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x0xd04493+62419]\n",
      "\tGetHandleVerifier [0x0xd044d4+62484]\n",
      "\t(No symbol) [0x0xb42133]\n",
      "\t(No symbol) [0x0xb31b40]\n",
      "\t(No symbol) [0x0xb4f912]\n",
      "\t(No symbol) [0x0xbb5d6c]\n",
      "\t(No symbol) [0x0xbd0159]\n",
      "\t(No symbol) [0x0xbaf266]\n",
      "\t(No symbol) [0x0xb7e852]\n",
      "\t(No symbol) [0x0xb7f6f4]\n",
      "\tGetHandleVerifier [0x0xf74773+2619059]\n",
      "\tGetHandleVerifier [0x0xf6fb8a+2599626]\n",
      "\tGetHandleVerifier [0x0xd2b03a+221050]\n",
      "\tGetHandleVerifier [0x0xd1b2b8+156152]\n",
      "\tGetHandleVerifier [0x0xd21c6d+183213]\n",
      "\tGetHandleVerifier [0x0xd0c378+94904]\n",
      "\tGetHandleVerifier [0x0xd0c502+95298]\n",
      "\tGetHandleVerifier [0x0xcf765a+9626]\n",
      "\tBaseThreadInitThunk [0x0x75055d49+25]\n",
      "\tRtlInitializeExceptionChain [0x0x7756d09b+107]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x0x7756d021+561]\n",
      "\n",
      "‚ùå Aucun produit trouv√©\n",
      "‚ùå Aucune review r√©cup√©r√©e\n",
      "‚úÖ Driver scraper ferm√©\n"
     ]
    }
   ],
   "source": [
    "run_example('laptops_gaming')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f8436523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "üéØ WORKFLOW ROBUSTE - REVIEWS DE PRODUITS\n",
      "====================================================================================================\n",
      "\n",
      "1Ô∏è‚É£ Workflow complet (scout + scraper)\n",
      "2Ô∏è‚É£ Test scout uniquement\n",
      "3Ô∏è‚É£ Configuration avanc√©e\n",
      "4Ô∏è‚É£ Voir fichiers de donn√©es\n",
      "5Ô∏è‚É£ Quitter\n",
      "\n",
      "\n",
      "üìã CONFIGURATION DU WORKFLOW COMPLET\n",
      "--------------------------------------------------\n",
      "\n",
      "üìã CONFIGURATION DU WORKFLOW COMPLET\n",
      "--------------------------------------------------\n",
      "\n",
      "üöÄ Lancement du workflow...\n",
      "====================================================================================================\n",
      "üöÄ WORKFLOW ROBUSTE - SCRAPING REVIEWS DE PRODUITS\n",
      "====================================================================================================\n",
      "üì¶ Cat√©gorie: laptop\n",
      "üåê Site: amazon\n",
      "üìä Produits max: 2\n",
      "‚≠ê Reviews par note: 10\n",
      "üëÅÔ∏è Mode: Headless\n",
      "üìà Total estim√©: 100 reviews max\n",
      "\n",
      "üîç PHASE 1: D√âTECTION AUTOMATIQUE DES BALISES\n",
      "----------------------------------------------------------------------\n",
      "üîß Tentative 1/3 - Setup driver scout...\n",
      "\n",
      "üöÄ Lancement du workflow...\n",
      "====================================================================================================\n",
      "üöÄ WORKFLOW ROBUSTE - SCRAPING REVIEWS DE PRODUITS\n",
      "====================================================================================================\n",
      "üì¶ Cat√©gorie: laptop\n",
      "üåê Site: amazon\n",
      "üìä Produits max: 2\n",
      "‚≠ê Reviews par note: 10\n",
      "üëÅÔ∏è Mode: Headless\n",
      "üìà Total estim√©: 100 reviews max\n",
      "\n",
      "üîç PHASE 1: D√âTECTION AUTOMATIQUE DES BALISES\n",
      "----------------------------------------------------------------------\n",
      "üîß Tentative 1/3 - Setup driver scout...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-29 15:03:55,356 - INFO - patching driver executable C:\\Users\\Yann\\appdata\\roaming\\undetected_chromedriver\\undetected_chromedriver.exe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Driver scout initialis√© avec succ√®s!\n",
      "üîç D√©tection des s√©lecteurs pour amazon...\n",
      "üåê Navigation vers: https://www.amazon.com/s?k=laptop\n",
      "‚úÖ Conteneur: [data-component-type=\"s-search-result\"] (16 √©l√©ments)\n",
      "‚úÖ Titre: h2 span\n",
      "‚úÖ Conteneur: [data-component-type=\"s-search-result\"] (16 √©l√©ments)\n",
      "‚úÖ Titre: h2 span\n",
      "‚úÖ URL: .a-link-normal\n",
      "‚úÖ URL: .a-link-normal\n",
      "‚úÖ Rating: .a-icon-alt\n",
      "‚úÖ S√©lecteurs produits d√©tect√©s: 4\n",
      "üîó Test reviews sur: https://www.amazon.com/HP-Micro-edge-Microsoft-14-dq0040nr-Snowflake/dp/B0947BJ6...\n",
      "‚úÖ Rating: .a-icon-alt\n",
      "‚úÖ S√©lecteurs produits d√©tect√©s: 4\n",
      "üîó Test reviews sur: https://www.amazon.com/HP-Micro-edge-Microsoft-14-dq0040nr-Snowflake/dp/B0947BJ6...\n",
      "üîó Navigation vers page reviews: https://www.amazon.com/HP-Micro-edge-Microsoft-14-dq0040nr-Snowflake/dp/B0947BJ6...\n",
      "üîó Navigation vers page reviews: https://www.amazon.com/HP-Micro-edge-Microsoft-14-dq0040nr-Snowflake/dp/B0947BJ6...\n",
      "‚úÖ Conteneur reviews: [data-hook=\"review\"] (13 reviews)\n",
      "‚úÖ Texte review: [data-hook=\"review-body\"] span\n",
      "‚úÖ Titre review: .review-title\n",
      "‚úÖ Rating review: [data-hook=\"review-star-rating\"] .a-icon-alt\n",
      "‚úÖ Auteur review: .a-profile-name\n",
      "‚úÖ Date review: [data-hook=\"review-date\"]\n",
      "‚úÖ S√©lecteurs reviews d√©tect√©s: 6\n",
      "‚úÖ S√©lecteurs sauvegard√©s: ../config/detected_selectors_amazon.json\n",
      "‚úÖ S√©lecteurs d√©tect√©s avec succ√®s!\n",
      "üì¶ Produits: ['container', 'title', 'url', 'rating']\n",
      "üìù Reviews: ['container', 'text', 'title', 'rating', 'author', 'date']\n",
      "‚úÖ Conteneur reviews: [data-hook=\"review\"] (13 reviews)\n",
      "‚úÖ Texte review: [data-hook=\"review-body\"] span\n",
      "‚úÖ Titre review: .review-title\n",
      "‚úÖ Rating review: [data-hook=\"review-star-rating\"] .a-icon-alt\n",
      "‚úÖ Auteur review: .a-profile-name\n",
      "‚úÖ Date review: [data-hook=\"review-date\"]\n",
      "‚úÖ S√©lecteurs reviews d√©tect√©s: 6\n",
      "‚úÖ S√©lecteurs sauvegard√©s: ../config/detected_selectors_amazon.json\n",
      "‚úÖ S√©lecteurs d√©tect√©s avec succ√®s!\n",
      "üì¶ Produits: ['container', 'title', 'url', 'rating']\n",
      "üìù Reviews: ['container', 'text', 'title', 'rating', 'author', 'date']\n",
      "‚úÖ Driver scout ferm√©\n",
      "\n",
      "======================================================================\n",
      "üìä PHASE 2: SCRAPING DES REVIEWS\n",
      "----------------------------------------------------------------------\n",
      "üöÄ Tentative 1/3 - Setup driver scraper...\n",
      "‚úÖ Driver scout ferm√©\n",
      "\n",
      "======================================================================\n",
      "üìä PHASE 2: SCRAPING DES REVIEWS\n",
      "----------------------------------------------------------------------\n",
      "üöÄ Tentative 1/3 - Setup driver scraper...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-29 15:04:41,538 - INFO - patching driver executable C:\\Users\\Yann\\appdata\\roaming\\undetected_chromedriver\\undetected_chromedriver.exe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Driver scraper pr√™t!\n",
      "üé≠ User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0...\n",
      "\n",
      "üéØ D√©but du scraping pour 'laptop' sur amazon...\n",
      "================================================================================\n",
      "üéØ D√âBUT DU SCRAPING ROBUSTE\n",
      "================================================================================\n",
      "üì¶ Cat√©gorie: laptop\n",
      "üåê Site: amazon\n",
      "üìä Produits max: 2\n",
      "‚≠ê Reviews par note: 10\n",
      "\n",
      "üîç Recherche: https://www.amazon.com/s?k=laptop\n",
      "üì¶ 16 conteneurs trouv√©s\n",
      "‚úÖ Produit 1: HP 14 Laptop, Intel Celeron N4020, 4 GB ...\n",
      "‚úÖ Produit 2: SGIN Laptop 15.6 Inch Laptops Computer, ...\n",
      "‚úÖ 2 produits trouv√©s\n",
      "\n",
      "üì¶ PRODUIT 1/2\n",
      "üè∑Ô∏è HP 14 Laptop, Intel Celeron N4020, 4 GB RAM, 64 GB Storage, ...\n",
      "üì¶ 16 conteneurs trouv√©s\n",
      "‚úÖ Produit 1: HP 14 Laptop, Intel Celeron N4020, 4 GB ...\n",
      "‚úÖ Produit 2: SGIN Laptop 15.6 Inch Laptops Computer, ...\n",
      "‚úÖ 2 produits trouv√©s\n",
      "\n",
      "üì¶ PRODUIT 1/2\n",
      "üè∑Ô∏è HP 14 Laptop, Intel Celeron N4020, 4 GB RAM, 64 GB Storage, ...\n",
      "üìù Page 1: 12 reviews\n",
      "üìù Page 1: 12 reviews\n",
      "üìÑ Plus de pages disponibles\n",
      "‚úÖ 12 reviews r√©cup√©r√©es\n",
      "‚è≥ D√©lai: 6.0s...\n",
      "üìÑ Plus de pages disponibles\n",
      "‚úÖ 12 reviews r√©cup√©r√©es\n",
      "‚è≥ D√©lai: 6.0s...\n",
      "\n",
      "üì¶ PRODUIT 2/2\n",
      "üè∑Ô∏è SGIN Laptop 15.6 Inch Laptops Computer, Celeron N4000 Proces...\n",
      "\n",
      "üì¶ PRODUIT 2/2\n",
      "üè∑Ô∏è SGIN Laptop 15.6 Inch Laptops Computer, Celeron N4000 Proces...\n",
      "üìù Page 1: 5 reviews\n",
      "üìù Page 1: 5 reviews\n",
      "üìÑ Plus de pages disponibles\n",
      "‚úÖ 5 reviews r√©cup√©r√©es\n",
      "‚è≥ D√©lai: 5.2s...\n",
      "üìÑ Plus de pages disponibles\n",
      "‚úÖ 5 reviews r√©cup√©r√©es\n",
      "‚è≥ D√©lai: 5.2s...\n",
      "üßπ Nettoyage des donn√©es...\n",
      "üìù Doublons supprim√©s: 1\n",
      "‚úÖ 15 reviews nettoy√©es\n",
      "\n",
      "================================================================================\n",
      "üìä SCRAPING TERMIN√â - STATISTIQUES\n",
      "================================================================================\n",
      "‚è±Ô∏è Dur√©e: 4.0 minutes\n",
      "üì¶ Produits trait√©s: 2\n",
      "üìù Reviews r√©cup√©r√©es: 17\n",
      "‚ùå Erreurs: 0\n",
      "üìà Taux de succ√®s: 100.0%\n",
      "\n",
      "======================================================================\n",
      "üìä ANALYSE DES R√âSULTATS\n",
      "----------------------------------------------------------------------\n",
      "‚úÖ Total reviews: 15\n",
      "üì¶ Produits uniques: 2\n",
      "üìù Longueur moyenne: 492 caract√®res\n",
      "‚≠ê Distribution des notes:\n",
      "   1.0 √©toiles: 1 reviews\n",
      "   3.0 √©toiles: 1 reviews\n",
      "   4.0 √©toiles: 2 reviews\n",
      "   5.0 √©toiles: 9 reviews\n",
      "\n",
      "üèÜ Top produits par nombre de reviews:\n",
      "   ‚Ä¢ HP 14 Laptop, Intel Celeron N4020, 4 GB RAM, 64 GB... (10 reviews)\n",
      "   ‚Ä¢ SGIN Laptop 15.6 Inch Laptops Computer, Celeron N4... (5 reviews)\n",
      "üíæ Donn√©es sauvegard√©es: ../data/raw/amazon_laptop_20250629_150840.csv\n",
      "\n",
      "üìã APER√áU DES DONN√âES (5 premi√®res reviews):\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Review 1:\n",
      "  product_name: HP 14 Laptop, Intel Celeron N4020, 4 GB RAM, 64 GB...\n",
      "  user_rating: 5.0\n",
      "  review_text: Got myself this Laptop. The sound is great. It's not touch screen sadly, but the Speed is decent for...\n",
      "  reviewer_name: Maria Torres\n",
      "\n",
      "Review 2:\n",
      "  product_name: HP 14 Laptop, Intel Celeron N4020, 4 GB RAM, 64 GB...\n",
      "  user_rating: 5.0\n",
      "  review_text: This laptop is incredibly fast with amazing battery life, and it definitely doesn't overheat. The ke...\n",
      "  reviewer_name: Sigi-Ann Miller\n",
      "\n",
      "Review 3:\n",
      "  product_name: HP 14 Laptop, Intel Celeron N4020, 4 GB RAM, 64 GB...\n",
      "  user_rating: 4.0\n",
      "  review_text: This is listed as a budget laptop, but if one is knowledgeable about pc's you can make it into a les...\n",
      "  reviewer_name: sam17704\n",
      "\n",
      "Review 4:\n",
      "  product_name: HP 14 Laptop, Intel Celeron N4020, 4 GB RAM, 64 GB...\n",
      "  user_rating: 5.0\n",
      "  review_text: Great price. But now I know just how much I appreciate my tablet. Sorry. But I wouldn‚Äôt buy again\n",
      "  reviewer_name: skyhawk\n",
      "\n",
      "Review 5:\n",
      "  product_name: HP 14 Laptop, Intel Celeron N4020, 4 GB RAM, 64 GB...\n",
      "  user_rating: 5.0\n",
      "  review_text: this item been very helpful in what am doing with my own life. This product is helpful.\n",
      "  reviewer_name: sunny\n",
      "üßπ Nettoyage des donn√©es...\n",
      "üìù Doublons supprim√©s: 1\n",
      "‚úÖ 15 reviews nettoy√©es\n",
      "\n",
      "================================================================================\n",
      "üìä SCRAPING TERMIN√â - STATISTIQUES\n",
      "================================================================================\n",
      "‚è±Ô∏è Dur√©e: 4.0 minutes\n",
      "üì¶ Produits trait√©s: 2\n",
      "üìù Reviews r√©cup√©r√©es: 17\n",
      "‚ùå Erreurs: 0\n",
      "üìà Taux de succ√®s: 100.0%\n",
      "\n",
      "======================================================================\n",
      "üìä ANALYSE DES R√âSULTATS\n",
      "----------------------------------------------------------------------\n",
      "‚úÖ Total reviews: 15\n",
      "üì¶ Produits uniques: 2\n",
      "üìù Longueur moyenne: 492 caract√®res\n",
      "‚≠ê Distribution des notes:\n",
      "   1.0 √©toiles: 1 reviews\n",
      "   3.0 √©toiles: 1 reviews\n",
      "   4.0 √©toiles: 2 reviews\n",
      "   5.0 √©toiles: 9 reviews\n",
      "\n",
      "üèÜ Top produits par nombre de reviews:\n",
      "   ‚Ä¢ HP 14 Laptop, Intel Celeron N4020, 4 GB RAM, 64 GB... (10 reviews)\n",
      "   ‚Ä¢ SGIN Laptop 15.6 Inch Laptops Computer, Celeron N4... (5 reviews)\n",
      "üíæ Donn√©es sauvegard√©es: ../data/raw/amazon_laptop_20250629_150840.csv\n",
      "\n",
      "üìã APER√áU DES DONN√âES (5 premi√®res reviews):\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Review 1:\n",
      "  product_name: HP 14 Laptop, Intel Celeron N4020, 4 GB RAM, 64 GB...\n",
      "  user_rating: 5.0\n",
      "  review_text: Got myself this Laptop. The sound is great. It's not touch screen sadly, but the Speed is decent for...\n",
      "  reviewer_name: Maria Torres\n",
      "\n",
      "Review 2:\n",
      "  product_name: HP 14 Laptop, Intel Celeron N4020, 4 GB RAM, 64 GB...\n",
      "  user_rating: 5.0\n",
      "  review_text: This laptop is incredibly fast with amazing battery life, and it definitely doesn't overheat. The ke...\n",
      "  reviewer_name: Sigi-Ann Miller\n",
      "\n",
      "Review 3:\n",
      "  product_name: HP 14 Laptop, Intel Celeron N4020, 4 GB RAM, 64 GB...\n",
      "  user_rating: 4.0\n",
      "  review_text: This is listed as a budget laptop, but if one is knowledgeable about pc's you can make it into a les...\n",
      "  reviewer_name: sam17704\n",
      "\n",
      "Review 4:\n",
      "  product_name: HP 14 Laptop, Intel Celeron N4020, 4 GB RAM, 64 GB...\n",
      "  user_rating: 5.0\n",
      "  review_text: Great price. But now I know just how much I appreciate my tablet. Sorry. But I wouldn‚Äôt buy again\n",
      "  reviewer_name: skyhawk\n",
      "\n",
      "Review 5:\n",
      "  product_name: HP 14 Laptop, Intel Celeron N4020, 4 GB RAM, 64 GB...\n",
      "  user_rating: 5.0\n",
      "  review_text: this item been very helpful in what am doing with my own life. This product is helpful.\n",
      "  reviewer_name: sunny\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yann\\AppData\\Local\\Temp\\ipykernel_21532\\361164114.py:587: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['review_text'] = df['review_text'].str.strip()\n",
      "C:\\Users\\Yann\\AppData\\Local\\Temp\\ipykernel_21532\\361164114.py:588: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['review_title'] = df['review_title'].str.strip()\n",
      "C:\\Users\\Yann\\AppData\\Local\\Temp\\ipykernel_21532\\361164114.py:589: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['product_name'] = df['product_name'].str.strip()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Driver scraper ferm√©\n",
      "\n",
      "====================================================================================================\n",
      "üéâ WORKFLOW TERMIN√â AVEC SUCC√àS!\n",
      "====================================================================================================\n",
      "\n",
      "‚úÖ Workflow termin√© - 15 reviews r√©cup√©r√©es\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_name</th>\n",
       "      <th>product_category</th>\n",
       "      <th>product_url</th>\n",
       "      <th>product_price</th>\n",
       "      <th>scraped_at</th>\n",
       "      <th>review_text</th>\n",
       "      <th>review_title</th>\n",
       "      <th>user_rating</th>\n",
       "      <th>reviewer_name</th>\n",
       "      <th>review_date</th>\n",
       "      <th>review_length</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HP 14 Laptop, Intel Celeron N4020, 4 GB RAM, 6...</td>\n",
       "      <td>laptop</td>\n",
       "      <td>https://www.amazon.com/HP-Micro-edge-Microsoft...</td>\n",
       "      <td>N/A</td>\n",
       "      <td>2025-06-29T15:05:04.594833</td>\n",
       "      <td>Got myself this Laptop. The sound is great. It...</td>\n",
       "      <td>Honest Reviewww</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Maria Torres</td>\n",
       "      <td>Reviewed in the United States on June 20, 2025</td>\n",
       "      <td>224</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HP 14 Laptop, Intel Celeron N4020, 4 GB RAM, 6...</td>\n",
       "      <td>laptop</td>\n",
       "      <td>https://www.amazon.com/HP-Micro-edge-Microsoft...</td>\n",
       "      <td>N/A</td>\n",
       "      <td>2025-06-29T15:05:04.635622</td>\n",
       "      <td>This laptop is incredibly fast with amazing ba...</td>\n",
       "      <td>Good quality</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Sigi-Ann Miller</td>\n",
       "      <td>Reviewed in the United States on June 19, 2025</td>\n",
       "      <td>180</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HP 14 Laptop, Intel Celeron N4020, 4 GB RAM, 6...</td>\n",
       "      <td>laptop</td>\n",
       "      <td>https://www.amazon.com/HP-Micro-edge-Microsoft...</td>\n",
       "      <td>N/A</td>\n",
       "      <td>2025-06-29T15:05:04.666732</td>\n",
       "      <td>This is listed as a budget laptop, but if one ...</td>\n",
       "      <td>It's a steal if you know how to fine tune it.</td>\n",
       "      <td>4.0</td>\n",
       "      <td>sam17704</td>\n",
       "      <td>Reviewed in the United States on May 8, 2025</td>\n",
       "      <td>593</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HP 14 Laptop, Intel Celeron N4020, 4 GB RAM, 6...</td>\n",
       "      <td>laptop</td>\n",
       "      <td>https://www.amazon.com/HP-Micro-edge-Microsoft...</td>\n",
       "      <td>N/A</td>\n",
       "      <td>2025-06-29T15:05:04.707828</td>\n",
       "      <td>Great price. But now I know just how much I ap...</td>\n",
       "      <td>Slow</td>\n",
       "      <td>5.0</td>\n",
       "      <td>skyhawk</td>\n",
       "      <td>Reviewed in the United States on June 24, 2025</td>\n",
       "      <td>97</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HP 14 Laptop, Intel Celeron N4020, 4 GB RAM, 6...</td>\n",
       "      <td>laptop</td>\n",
       "      <td>https://www.amazon.com/HP-Micro-edge-Microsoft...</td>\n",
       "      <td>N/A</td>\n",
       "      <td>2025-06-29T15:05:04.741401</td>\n",
       "      <td>this item been very helpful in what am doing w...</td>\n",
       "      <td>great item</td>\n",
       "      <td>5.0</td>\n",
       "      <td>sunny</td>\n",
       "      <td>Reviewed in the United States on June 19, 2025</td>\n",
       "      <td>87</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HP 14 Laptop, Intel Celeron N4020, 4 GB RAM, 6...</td>\n",
       "      <td>laptop</td>\n",
       "      <td>https://www.amazon.com/HP-Micro-edge-Microsoft...</td>\n",
       "      <td>N/A</td>\n",
       "      <td>2025-06-29T15:05:04.773148</td>\n",
       "      <td>Perfect for the business I'm running</td>\n",
       "      <td>Is laptop very well needed</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Tim J Callantine</td>\n",
       "      <td>Reviewed in the United States on June 24, 2025</td>\n",
       "      <td>36</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>HP 14 Laptop, Intel Celeron N4020, 4 GB RAM, 6...</td>\n",
       "      <td>laptop</td>\n",
       "      <td>https://www.amazon.com/HP-Micro-edge-Microsoft...</td>\n",
       "      <td>N/A</td>\n",
       "      <td>2025-06-29T15:05:04.804627</td>\n",
       "      <td>This is a good laptop, not a great one. It is ...</td>\n",
       "      <td>GOOD, NOT GREAT</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Michael</td>\n",
       "      <td>Reviewed in the United States on May 26, 2025</td>\n",
       "      <td>162</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>HP 14 Laptop, Intel Celeron N4020, 4 GB RAM, 6...</td>\n",
       "      <td>laptop</td>\n",
       "      <td>https://www.amazon.com/HP-Micro-edge-Microsoft...</td>\n",
       "      <td>N/A</td>\n",
       "      <td>2025-06-29T15:05:04.835407</td>\n",
       "      <td>Works well.\\nWish I had searched for one with ...</td>\n",
       "      <td>Good basic laptop</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Eric Spindler</td>\n",
       "      <td>Reviewed in the United States on June 25, 2025</td>\n",
       "      <td>98</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>HP 14 Laptop, Intel Celeron N4020, 4 GB RAM, 6...</td>\n",
       "      <td>laptop</td>\n",
       "      <td>https://www.amazon.com/HP-Micro-edge-Microsoft...</td>\n",
       "      <td>N/A</td>\n",
       "      <td>2025-06-29T15:05:04.865424</td>\n",
       "      <td>Excelente producto</td>\n",
       "      <td>Laptop HP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sergio Vergara</td>\n",
       "      <td>Reviewed in Mexico on August 27, 2024</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>HP 14 Laptop, Intel Celeron N4020, 4 GB RAM, 6...</td>\n",
       "      <td>laptop</td>\n",
       "      <td>https://www.amazon.com/HP-Micro-edge-Microsoft...</td>\n",
       "      <td>N/A</td>\n",
       "      <td>2025-06-29T15:05:50.040514</td>\n",
       "      <td>Compre este equipo por la marca HP, la verdad ...</td>\n",
       "      <td>Laptop HP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Karla Alejandra Castillo</td>\n",
       "      <td>Reviewed in Mexico on November 30, 2023</td>\n",
       "      <td>649</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>SGIN Laptop 15.6 Inch Laptops Computer, Celero...</td>\n",
       "      <td>laptop</td>\n",
       "      <td>https://www.amazon.com/SGIN-Laptops-Computer-C...</td>\n",
       "      <td>N/A</td>\n",
       "      <td>2025-06-29T15:07:35.170075</td>\n",
       "      <td>Is 2025 a year of Linux laptop? Given how well...</td>\n",
       "      <td>Cheap laptop for Linux/Ubuntu setup</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Max G.</td>\n",
       "      <td>Reviewed in the United States on April 21, 2025</td>\n",
       "      <td>1718</td>\n",
       "      <td>306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>SGIN Laptop 15.6 Inch Laptops Computer, Celero...</td>\n",
       "      <td>laptop</td>\n",
       "      <td>https://www.amazon.com/SGIN-Laptops-Computer-C...</td>\n",
       "      <td>N/A</td>\n",
       "      <td>2025-06-29T15:07:35.202699</td>\n",
       "      <td>Honestly, I wasn‚Äôt expecting much when I order...</td>\n",
       "      <td>Lightweight and Fast</td>\n",
       "      <td>5.0</td>\n",
       "      <td>VIGY</td>\n",
       "      <td>Reviewed in the United States on May 1, 2025</td>\n",
       "      <td>1360</td>\n",
       "      <td>246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>SGIN Laptop 15.6 Inch Laptops Computer, Celero...</td>\n",
       "      <td>laptop</td>\n",
       "      <td>https://www.amazon.com/SGIN-Laptops-Computer-C...</td>\n",
       "      <td>N/A</td>\n",
       "      <td>2025-06-29T15:07:35.232934</td>\n",
       "      <td>This 15.6\" Laptop is the underdog that punches...</td>\n",
       "      <td>üíª Sleek, Silver, and Surprisingly Powerful! üöÄ</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Wehash Technology</td>\n",
       "      <td>Reviewed in the United States on April 8, 2025</td>\n",
       "      <td>1212</td>\n",
       "      <td>197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>SGIN Laptop 15.6 Inch Laptops Computer, Celero...</td>\n",
       "      <td>laptop</td>\n",
       "      <td>https://www.amazon.com/SGIN-Laptops-Computer-C...</td>\n",
       "      <td>N/A</td>\n",
       "      <td>2025-06-29T15:07:35.263044</td>\n",
       "      <td>At first, this laptop seemed like a solid choi...</td>\n",
       "      <td>Disappointing Screen Issues After Setup = Unus...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Brittney D. from the Britt Brat Breakdown</td>\n",
       "      <td>Reviewed in the United States on June 24, 2025</td>\n",
       "      <td>616</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>SGIN Laptop 15.6 Inch Laptops Computer, Celero...</td>\n",
       "      <td>laptop</td>\n",
       "      <td>https://www.amazon.com/SGIN-Laptops-Computer-C...</td>\n",
       "      <td>N/A</td>\n",
       "      <td>2025-06-29T15:07:35.293552</td>\n",
       "      <td>This laptop is a great choice. Multitasking, s...</td>\n",
       "      <td>Worth it.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Amazon Customer</td>\n",
       "      <td>Reviewed in the United States on June 3, 2025</td>\n",
       "      <td>330</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         product_name product_category  \\\n",
       "0   HP 14 Laptop, Intel Celeron N4020, 4 GB RAM, 6...           laptop   \n",
       "1   HP 14 Laptop, Intel Celeron N4020, 4 GB RAM, 6...           laptop   \n",
       "2   HP 14 Laptop, Intel Celeron N4020, 4 GB RAM, 6...           laptop   \n",
       "3   HP 14 Laptop, Intel Celeron N4020, 4 GB RAM, 6...           laptop   \n",
       "4   HP 14 Laptop, Intel Celeron N4020, 4 GB RAM, 6...           laptop   \n",
       "5   HP 14 Laptop, Intel Celeron N4020, 4 GB RAM, 6...           laptop   \n",
       "6   HP 14 Laptop, Intel Celeron N4020, 4 GB RAM, 6...           laptop   \n",
       "7   HP 14 Laptop, Intel Celeron N4020, 4 GB RAM, 6...           laptop   \n",
       "8   HP 14 Laptop, Intel Celeron N4020, 4 GB RAM, 6...           laptop   \n",
       "10  HP 14 Laptop, Intel Celeron N4020, 4 GB RAM, 6...           laptop   \n",
       "12  SGIN Laptop 15.6 Inch Laptops Computer, Celero...           laptop   \n",
       "13  SGIN Laptop 15.6 Inch Laptops Computer, Celero...           laptop   \n",
       "14  SGIN Laptop 15.6 Inch Laptops Computer, Celero...           laptop   \n",
       "15  SGIN Laptop 15.6 Inch Laptops Computer, Celero...           laptop   \n",
       "16  SGIN Laptop 15.6 Inch Laptops Computer, Celero...           laptop   \n",
       "\n",
       "                                          product_url product_price  \\\n",
       "0   https://www.amazon.com/HP-Micro-edge-Microsoft...           N/A   \n",
       "1   https://www.amazon.com/HP-Micro-edge-Microsoft...           N/A   \n",
       "2   https://www.amazon.com/HP-Micro-edge-Microsoft...           N/A   \n",
       "3   https://www.amazon.com/HP-Micro-edge-Microsoft...           N/A   \n",
       "4   https://www.amazon.com/HP-Micro-edge-Microsoft...           N/A   \n",
       "5   https://www.amazon.com/HP-Micro-edge-Microsoft...           N/A   \n",
       "6   https://www.amazon.com/HP-Micro-edge-Microsoft...           N/A   \n",
       "7   https://www.amazon.com/HP-Micro-edge-Microsoft...           N/A   \n",
       "8   https://www.amazon.com/HP-Micro-edge-Microsoft...           N/A   \n",
       "10  https://www.amazon.com/HP-Micro-edge-Microsoft...           N/A   \n",
       "12  https://www.amazon.com/SGIN-Laptops-Computer-C...           N/A   \n",
       "13  https://www.amazon.com/SGIN-Laptops-Computer-C...           N/A   \n",
       "14  https://www.amazon.com/SGIN-Laptops-Computer-C...           N/A   \n",
       "15  https://www.amazon.com/SGIN-Laptops-Computer-C...           N/A   \n",
       "16  https://www.amazon.com/SGIN-Laptops-Computer-C...           N/A   \n",
       "\n",
       "                    scraped_at  \\\n",
       "0   2025-06-29T15:05:04.594833   \n",
       "1   2025-06-29T15:05:04.635622   \n",
       "2   2025-06-29T15:05:04.666732   \n",
       "3   2025-06-29T15:05:04.707828   \n",
       "4   2025-06-29T15:05:04.741401   \n",
       "5   2025-06-29T15:05:04.773148   \n",
       "6   2025-06-29T15:05:04.804627   \n",
       "7   2025-06-29T15:05:04.835407   \n",
       "8   2025-06-29T15:05:04.865424   \n",
       "10  2025-06-29T15:05:50.040514   \n",
       "12  2025-06-29T15:07:35.170075   \n",
       "13  2025-06-29T15:07:35.202699   \n",
       "14  2025-06-29T15:07:35.232934   \n",
       "15  2025-06-29T15:07:35.263044   \n",
       "16  2025-06-29T15:07:35.293552   \n",
       "\n",
       "                                          review_text  \\\n",
       "0   Got myself this Laptop. The sound is great. It...   \n",
       "1   This laptop is incredibly fast with amazing ba...   \n",
       "2   This is listed as a budget laptop, but if one ...   \n",
       "3   Great price. But now I know just how much I ap...   \n",
       "4   this item been very helpful in what am doing w...   \n",
       "5                Perfect for the business I'm running   \n",
       "6   This is a good laptop, not a great one. It is ...   \n",
       "7   Works well.\\nWish I had searched for one with ...   \n",
       "8                                  Excelente producto   \n",
       "10  Compre este equipo por la marca HP, la verdad ...   \n",
       "12  Is 2025 a year of Linux laptop? Given how well...   \n",
       "13  Honestly, I wasn‚Äôt expecting much when I order...   \n",
       "14  This 15.6\" Laptop is the underdog that punches...   \n",
       "15  At first, this laptop seemed like a solid choi...   \n",
       "16  This laptop is a great choice. Multitasking, s...   \n",
       "\n",
       "                                         review_title  user_rating  \\\n",
       "0                                     Honest Reviewww          5.0   \n",
       "1                                        Good quality          5.0   \n",
       "2       It's a steal if you know how to fine tune it.          4.0   \n",
       "3                                                Slow          5.0   \n",
       "4                                          great item          5.0   \n",
       "5                          Is laptop very well needed          5.0   \n",
       "6                                     GOOD, NOT GREAT          3.0   \n",
       "7                                   Good basic laptop          4.0   \n",
       "8                                           Laptop HP          NaN   \n",
       "10                                          Laptop HP          NaN   \n",
       "12                Cheap laptop for Linux/Ubuntu setup          5.0   \n",
       "13                               Lightweight and Fast          5.0   \n",
       "14      üíª Sleek, Silver, and Surprisingly Powerful! üöÄ          5.0   \n",
       "15  Disappointing Screen Issues After Setup = Unus...          1.0   \n",
       "16                                          Worth it.          5.0   \n",
       "\n",
       "                                reviewer_name  \\\n",
       "0                                Maria Torres   \n",
       "1                             Sigi-Ann Miller   \n",
       "2                                    sam17704   \n",
       "3                                     skyhawk   \n",
       "4                                       sunny   \n",
       "5                            Tim J Callantine   \n",
       "6                                     Michael   \n",
       "7                               Eric Spindler   \n",
       "8                              Sergio Vergara   \n",
       "10                   Karla Alejandra Castillo   \n",
       "12                                     Max G.   \n",
       "13                                       VIGY   \n",
       "14                          Wehash Technology   \n",
       "15  Brittney D. from the Britt Brat Breakdown   \n",
       "16                            Amazon Customer   \n",
       "\n",
       "                                        review_date  review_length  word_count  \n",
       "0    Reviewed in the United States on June 20, 2025            224          40  \n",
       "1    Reviewed in the United States on June 19, 2025            180          29  \n",
       "2      Reviewed in the United States on May 8, 2025            593         126  \n",
       "3    Reviewed in the United States on June 24, 2025             97          19  \n",
       "4    Reviewed in the United States on June 19, 2025             87          17  \n",
       "5    Reviewed in the United States on June 24, 2025             36           6  \n",
       "6     Reviewed in the United States on May 26, 2025            162          34  \n",
       "7    Reviewed in the United States on June 25, 2025             98          19  \n",
       "8             Reviewed in Mexico on August 27, 2024             18           2  \n",
       "10          Reviewed in Mexico on November 30, 2023            649         126  \n",
       "12  Reviewed in the United States on April 21, 2025           1718         306  \n",
       "13     Reviewed in the United States on May 1, 2025           1360         246  \n",
       "14   Reviewed in the United States on April 8, 2025           1212         197  \n",
       "15   Reviewed in the United States on June 24, 2025            616         106  \n",
       "16    Reviewed in the United States on June 3, 2025            330          58  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "robust_reviews_menu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "503c90b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 12)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "df = pd.read_csv(\"../data/raw/amazon_laptop_20250629_150840.csv\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "631a9ba9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['product_name', 'product_category', 'product_url', 'product_price',\n",
       "       'scraped_at', 'review_text', 'review_title', 'user_rating',\n",
       "       'reviewer_name', 'review_date', 'review_length', 'word_count'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a6baaef6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_rating\n",
       "5.0    9\n",
       "4.0    2\n",
       "3.0    1\n",
       "1.0    1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['user_rating'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42d24a8",
   "metadata": {},
   "source": [
    "# More robust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c0342917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Scout ultra-s√©curis√© cr√©√© avec fallbacks multiples et logging!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SCOUT ULTRA-S√âCURIS√â AVEC FALLBACKS MULTIPLES\n",
    "# ============================================================================\n",
    "\n",
    "from datetime import datetime\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException, WebDriverException\n",
    "\n",
    "class UltraSecureProductReviewScout:\n",
    "    \"\"\"\n",
    "    Scout ultra-s√©curis√© avec fallbacks multiples pour d√©tecter les balises\n",
    "    - Fallbacks de driver multiples\n",
    "    - D√©tection adaptative des s√©lecteurs\n",
    "    - Gestion d'erreurs exhaustive\n",
    "    - Datetime pour tracking des commentaires\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.driver = None\n",
    "        self.wait = None\n",
    "        self.detected_selectors = {}\n",
    "        self.fallback_drivers = []\n",
    "        self.session_log = []\n",
    "        \n",
    "        # Base de s√©lecteurs √©tendue avec fallbacks\n",
    "        self.base_selectors = {\n",
    "            'amazon': {\n",
    "                'products': {\n",
    "                    'containers': [\n",
    "                        '[data-component-type=\"s-search-result\"]',\n",
    "                        '.s-result-item',\n",
    "                        '.s-widget-container .s-card-container',\n",
    "                        '[data-asin]:not([data-asin=\"\"])',\n",
    "                        '.s-card-container',\n",
    "                        '.sg-col-inner',\n",
    "                        '[cel_widget_id*=\"MAIN-SEARCH_RESULTS\"]'\n",
    "                    ],\n",
    "                    'titles': [\n",
    "                        'h2 span',\n",
    "                        'h2 a span',\n",
    "                        '.s-size-mini span',\n",
    "                        '.a-size-base-plus',\n",
    "                        '[data-cy=\"title-recipe-title\"]',\n",
    "                        '.a-size-medium',\n",
    "                        '.s-link-style a span',\n",
    "                        'h2.a-size-mini span'\n",
    "                    ],\n",
    "                    'urls': [\n",
    "                        'h2 a',\n",
    "                        '.a-link-normal',\n",
    "                        'a[href*=\"/dp/\"]',\n",
    "                        'a[href*=\"/gp/product/\"]',\n",
    "                        '.s-link-style a',\n",
    "                        'a[href*=\"amazon.com/\"]'\n",
    "                    ],\n",
    "                    'prices': [\n",
    "                        '.a-price .a-offscreen',\n",
    "                        '.a-price-whole',\n",
    "                        '.a-price-range .a-offscreen',\n",
    "                        '.a-price-symbol + .a-price-whole',\n",
    "                        '.a-price-fraction',\n",
    "                        '.s-price-range-display',\n",
    "                        '.a-price-display'\n",
    "                    ],\n",
    "                    'ratings': [\n",
    "                        '.a-icon-alt',\n",
    "                        '.a-star-mini .a-icon-alt',\n",
    "                        'span[aria-label*=\"stars\"]',\n",
    "                        '.a-icon-star',\n",
    "                        '.a-icon-star-small'\n",
    "                    ]\n",
    "                },\n",
    "                'reviews': {\n",
    "                    'containers': [\n",
    "                        '[data-hook=\"review\"]',\n",
    "                        '.review',\n",
    "                        '.cr-original-review-content',\n",
    "                        '.reviewText',\n",
    "                        '.a-section.review',\n",
    "                        '[data-hook=\"mobley-review-content\"]'\n",
    "                    ],\n",
    "                    'titles': [\n",
    "                        '[data-hook=\"review-title\"] span',\n",
    "                        '.review-title',\n",
    "                        '.cr-original-review-content .review-title',\n",
    "                        '.a-text-bold span',\n",
    "                        '[data-hook=\"review-title\"]'\n",
    "                    ],\n",
    "                    'texts': [\n",
    "                        '[data-hook=\"review-body\"] span',\n",
    "                        '.review-text',\n",
    "                        '.cr-original-review-content .review-text',\n",
    "                        '.reviewText',\n",
    "                        '[data-hook=\"review-collapsed-text\"]',\n",
    "                        '.a-expander-content span'\n",
    "                    ],\n",
    "                    'ratings': [\n",
    "                        '[data-hook=\"review-star-rating\"] .a-icon-alt',\n",
    "                        '.review-rating .a-icon-alt',\n",
    "                        '.cr-original-review-content .a-icon-alt',\n",
    "                        '.a-icon-star .a-icon-alt',\n",
    "                        'i.a-icon-star'\n",
    "                    ],\n",
    "                    'authors': [\n",
    "                        '.a-profile-name',\n",
    "                        '.review-author',\n",
    "                        '.cr-original-review-content .author',\n",
    "                        '[data-hook=\"genome-widget\"] .a-profile-name',\n",
    "                        '.a-profile-content .a-profile-name'\n",
    "                    ],\n",
    "                    'dates': [\n",
    "                        '[data-hook=\"review-date\"]',\n",
    "                        '.review-date',\n",
    "                        '.cr-original-review-content .review-date',\n",
    "                        '.a-color-secondary.review-date'\n",
    "                    ],\n",
    "                    'helpful_votes': [\n",
    "                        '[data-hook=\"helpful-vote-statement\"]',\n",
    "                        '.cr-vote-buttons',\n",
    "                        '.helpful-vote'\n",
    "                    ]\n",
    "                }\n",
    "            },\n",
    "            'ebay': {\n",
    "                'products': {\n",
    "                    'containers': [\n",
    "                        '.s-item',\n",
    "                        '.srp-results .s-item',\n",
    "                        '.b-listing__wrap',\n",
    "                        '.x-refine__main__list .s-item'\n",
    "                    ],\n",
    "                    'titles': [\n",
    "                        '.s-item__title',\n",
    "                        '.it-ttl',\n",
    "                        '.b-listing__title',\n",
    "                        '.s-item__title-text'\n",
    "                    ],\n",
    "                    'urls': [\n",
    "                        '.s-item__link',\n",
    "                        '.it-ttl a',\n",
    "                        '.b-listing__title a',\n",
    "                        '.s-item__title a'\n",
    "                    ],\n",
    "                    'prices': [\n",
    "                        '.s-item__price',\n",
    "                        '.notranslate',\n",
    "                        '.b-listing__price',\n",
    "                        '.s-item__price-detail'\n",
    "                    ]\n",
    "                },\n",
    "                'reviews': {\n",
    "                    'containers': [\n",
    "                        '.review-item',\n",
    "                        '.ebay-review',\n",
    "                        '.reviews .review',\n",
    "                        '.review-wrapper'\n",
    "                    ],\n",
    "                    'texts': [\n",
    "                        '.review-item-content',\n",
    "                        '.ebay-review-text',\n",
    "                        '.review-content',\n",
    "                        '.review-text'\n",
    "                    ],\n",
    "                    'ratings': [\n",
    "                        '.star-rating',\n",
    "                        '.rating-stars',\n",
    "                        '.ebay-star-rating'\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def log_action(self, action, status=\"SUCCESS\", details=\"\"):\n",
    "        \"\"\"Log des actions avec timestamp\"\"\"\n",
    "        entry = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'action': action,\n",
    "            'status': status,\n",
    "            'details': details\n",
    "        }\n",
    "        self.session_log.append(entry)\n",
    "        print(f\"[{entry['timestamp']}] {status}: {action} - {details}\")\n",
    "    \n",
    "    def setup_ultra_secure_driver(self, headless=True, timeout=30, max_fallback_attempts=5):\n",
    "        \"\"\"Configuration driver avec fallbacks multiples ultra-s√©curis√©s\"\"\"\n",
    "        \n",
    "        self.log_action(\"DRIVER_SETUP_START\", \"INFO\", f\"Headless: {headless}, Timeout: {timeout}s\")\n",
    "        \n",
    "        # M√©thodes de cr√©ation de driver par ordre de pr√©f√©rence\n",
    "        driver_methods = [\n",
    "            self._create_undetected_chrome_driver,\n",
    "            self._create_selenium_chrome_driver,\n",
    "            self._create_basic_chrome_driver,\n",
    "            self._create_firefox_fallback_driver,\n",
    "            self._create_edge_fallback_driver\n",
    "        ]\n",
    "        \n",
    "        for attempt in range(max_fallback_attempts):\n",
    "            for method_idx, method in enumerate(driver_methods):\n",
    "                try:\n",
    "                    self.log_action(f\"DRIVER_ATTEMPT\", \"INFO\", f\"Tentative {attempt+1}/{max_fallback_attempts}, M√©thode {method_idx+1}\")\n",
    "                    \n",
    "                    # Nettoyer les drivers existants\n",
    "                    self._cleanup_existing_drivers()\n",
    "                    \n",
    "                    # Essayer la m√©thode\n",
    "                    driver = method(headless, timeout)\n",
    "                    \n",
    "                    if driver and self._test_driver_functionality(driver):\n",
    "                        self.driver = driver\n",
    "                        self.wait = WebDriverWait(driver, timeout)\n",
    "                        self.log_action(\"DRIVER_SETUP_SUCCESS\", \"SUCCESS\", f\"M√©thode {method_idx+1} r√©ussie\")\n",
    "                        return True\n",
    "                    else:\n",
    "                        if driver:\n",
    "                            try:\n",
    "                                driver.quit()\n",
    "                            except:\n",
    "                                pass\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    self.log_action(f\"DRIVER_METHOD_{method_idx+1}_FAILED\", \"ERROR\", str(e)[:100])\n",
    "                    continue\n",
    "            \n",
    "            # D√©lai entre les tentatives\n",
    "            if attempt < max_fallback_attempts - 1:\n",
    "                delay = (attempt + 1) * 3\n",
    "                self.log_action(\"RETRY_DELAY\", \"INFO\", f\"Attente {delay}s avant nouvelle tentative\")\n",
    "                time.sleep(delay)\n",
    "        \n",
    "        self.log_action(\"DRIVER_SETUP_FAILED\", \"ERROR\", \"Toutes les m√©thodes ont √©chou√©\")\n",
    "        return False\n",
    "    \n",
    "    def _create_undetected_chrome_driver(self, headless, timeout):\n",
    "        \"\"\"M√©thode 1: Undetected Chrome (pr√©f√©r√©e)\"\"\"\n",
    "        try:\n",
    "            import undetected_chromedriver as uc\n",
    "            \n",
    "            options = uc.ChromeOptions()\n",
    "            \n",
    "            # Arguments ultra-s√©curis√©s\n",
    "            secure_args = [\n",
    "                '--no-sandbox',\n",
    "                '--disable-dev-shm-usage',\n",
    "                '--disable-gpu',\n",
    "                '--disable-web-security',\n",
    "                '--disable-features=VizDisplayCompositor',\n",
    "                '--disable-blink-features=AutomationControlled',\n",
    "                '--disable-extensions',\n",
    "                '--no-first-run',\n",
    "                '--no-default-browser-check',\n",
    "                '--disable-default-apps',\n",
    "                '--window-size=1920,1080',\n",
    "                '--start-maximized',\n",
    "                '--disable-infobars',\n",
    "                '--disable-notifications',\n",
    "                '--disable-popup-blocking'\n",
    "            ]\n",
    "            \n",
    "            for arg in secure_args:\n",
    "                options.add_argument(arg)\n",
    "            \n",
    "            if headless:\n",
    "                options.add_argument('--headless=new')\n",
    "            \n",
    "            # User agent ultra-r√©aliste\n",
    "            user_agents = [\n",
    "                'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "                'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',\n",
    "                'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n",
    "            ]\n",
    "            \n",
    "            try:\n",
    "                user_agent = random.choice(REALISTIC_USER_AGENTS)\n",
    "            except:\n",
    "                user_agent = random.choice(user_agents)\n",
    "                \n",
    "            options.add_argument(f'--user-agent={user_agent}')\n",
    "            \n",
    "            # Pr√©f√©rences s√©curis√©es\n",
    "            prefs = {\n",
    "                \"profile.default_content_setting_values.notifications\": 2,\n",
    "                \"profile.default_content_settings.popups\": 0,\n",
    "                \"profile.managed_default_content_settings.images\": 1,\n",
    "                \"profile.default_content_setting_values.geolocation\": 2,\n",
    "                \"profile.default_content_setting_values.media_stream_mic\": 2,\n",
    "                \"profile.default_content_setting_values.media_stream_camera\": 2\n",
    "            }\n",
    "            options.add_experimental_option(\"prefs\", prefs)\n",
    "            \n",
    "            # Cr√©ation du driver\n",
    "            driver = uc.Chrome(\n",
    "                options=options,\n",
    "                version_main=None,\n",
    "                headless=headless,\n",
    "                use_subprocess=False,\n",
    "                log_level=3\n",
    "            )\n",
    "            \n",
    "            # Configuration des timeouts\n",
    "            driver.set_page_load_timeout(timeout)\n",
    "            driver.implicitly_wait(10)\n",
    "            \n",
    "            return driver\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.log_action(\"UNDETECTED_CHROME_FAILED\", \"ERROR\", str(e))\n",
    "            return None\n",
    "    \n",
    "    def _create_selenium_chrome_driver(self, headless, timeout):\n",
    "        \"\"\"M√©thode 2: Selenium Chrome classique\"\"\"\n",
    "        try:\n",
    "            from selenium import webdriver\n",
    "            from selenium.webdriver.chrome.service import Service\n",
    "            from selenium.webdriver.chrome.options import Options\n",
    "            from webdriver_manager.chrome import ChromeDriverManager\n",
    "            \n",
    "            options = Options()\n",
    "            \n",
    "            # Arguments de base\n",
    "            args = [\n",
    "                '--no-sandbox',\n",
    "                '--disable-dev-shm-usage',\n",
    "                '--disable-gpu',\n",
    "                '--window-size=1920,1080'\n",
    "            ]\n",
    "            \n",
    "            if headless:\n",
    "                args.append('--headless')\n",
    "            \n",
    "            for arg in args:\n",
    "                options.add_argument(arg)\n",
    "            \n",
    "            # Service g√©r√© automatiquement\n",
    "            service = Service(ChromeDriverManager().install())\n",
    "            \n",
    "            driver = webdriver.Chrome(service=service, options=options)\n",
    "            driver.set_page_load_timeout(timeout)\n",
    "            \n",
    "            return driver\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.log_action(\"SELENIUM_CHROME_FAILED\", \"ERROR\", str(e))\n",
    "            return None\n",
    "    \n",
    "    def _create_basic_chrome_driver(self, headless, timeout):\n",
    "        \"\"\"M√©thode 3: Chrome basique sans webdriver-manager\"\"\"\n",
    "        try:\n",
    "            from selenium import webdriver\n",
    "            from selenium.webdriver.chrome.options import Options\n",
    "            \n",
    "            options = Options()\n",
    "            options.add_argument('--no-sandbox')\n",
    "            options.add_argument('--disable-dev-shm-usage')\n",
    "            \n",
    "            if headless:\n",
    "                options.add_argument('--headless')\n",
    "            \n",
    "            driver = webdriver.Chrome(options=options)\n",
    "            driver.set_page_load_timeout(timeout)\n",
    "            \n",
    "            return driver\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.log_action(\"BASIC_CHROME_FAILED\", \"ERROR\", str(e))\n",
    "            return None\n",
    "    \n",
    "    def _create_firefox_fallback_driver(self, headless, timeout):\n",
    "        \"\"\"M√©thode 4: Firefox fallback\"\"\"\n",
    "        try:\n",
    "            from selenium import webdriver\n",
    "            from selenium.webdriver.firefox.options import Options\n",
    "            \n",
    "            options = Options()\n",
    "            if headless:\n",
    "                options.add_argument('--headless')\n",
    "            \n",
    "            driver = webdriver.Firefox(options=options)\n",
    "            driver.set_page_load_timeout(timeout)\n",
    "            \n",
    "            return driver\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.log_action(\"FIREFOX_FAILED\", \"ERROR\", str(e))\n",
    "            return None\n",
    "    \n",
    "    def _create_edge_fallback_driver(self, headless, timeout):\n",
    "        \"\"\"M√©thode 5: Edge fallback\"\"\"\n",
    "        try:\n",
    "            from selenium import webdriver\n",
    "            from selenium.webdriver.edge.options import Options\n",
    "            \n",
    "            options = Options()\n",
    "            options.add_argument('--no-sandbox')\n",
    "            if headless:\n",
    "                options.add_argument('--headless')\n",
    "            \n",
    "            driver = webdriver.Edge(options=options)\n",
    "            driver.set_page_load_timeout(timeout)\n",
    "            \n",
    "            return driver\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.log_action(\"EDGE_FAILED\", \"ERROR\", str(e))\n",
    "            return None\n",
    "    \n",
    "    def _cleanup_existing_drivers(self):\n",
    "        \"\"\"Nettoie les drivers existants\"\"\"\n",
    "        try:\n",
    "            if self.driver:\n",
    "                self.driver.quit()\n",
    "                self.driver = None\n",
    "            \n",
    "            for driver in self.fallback_drivers:\n",
    "                try:\n",
    "                    driver.quit()\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            self.fallback_drivers.clear()\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.log_action(\"CLEANUP_WARNING\", \"WARNING\", str(e))\n",
    "    \n",
    "    def _test_driver_functionality(self, driver):\n",
    "        \"\"\"Test complet de fonctionnalit√© du driver\"\"\"\n",
    "        try:\n",
    "            # Test 1: Navigation basique\n",
    "            driver.get(\"data:text/html,<html><body><h1>Test</h1></body></html>\")\n",
    "            \n",
    "            # Test 2: Trouver un √©l√©ment\n",
    "            element = driver.find_element(By.TAG_NAME, \"h1\")\n",
    "            if element.text != \"Test\":\n",
    "                return False\n",
    "            \n",
    "            # Test 3: Ex√©cuter JavaScript\n",
    "            result = driver.execute_script(\"return 'test'\")\n",
    "            if result != \"test\":\n",
    "                return False\n",
    "            \n",
    "            # Test 4: Test de navigation r√©elle (optionnel et rapide)\n",
    "            try:\n",
    "                driver.set_page_load_timeout(10)\n",
    "                driver.get(\"https://httpbin.org/user-agent\")\n",
    "                time.sleep(2)\n",
    "            except:\n",
    "                pass  # Pas critique si √ßa √©choue\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.log_action(\"DRIVER_TEST_FAILED\", \"ERROR\", str(e))\n",
    "            return False\n",
    "    \n",
    "    def detect_site_selectors_ultra_secure(self, site_url, search_term=\"laptop\", max_retries=3):\n",
    "        \"\"\"\n",
    "        D√©tection ultra-s√©curis√©e avec fallbacks multiples pour les s√©lecteurs\n",
    "        \"\"\"\n",
    "        \n",
    "        if not self.driver:\n",
    "            self.log_action(\"DETECT_SELECTORS_NO_DRIVER\", \"ERROR\", \"Driver non initialis√©\")\n",
    "            return {}\n",
    "        \n",
    "        site_type = self._detect_site_type(site_url)\n",
    "        if site_type == 'unknown':\n",
    "            self.log_action(\"DETECT_SELECTORS_UNKNOWN_SITE\", \"ERROR\", f\"Site non support√©: {site_url}\")\n",
    "            return {}\n",
    "        \n",
    "        self.log_action(\"DETECT_SELECTORS_START\", \"INFO\", f\"Site: {site_type}, Terme: {search_term}\")\n",
    "        \n",
    "        detected = {\n",
    "            'site': site_type,\n",
    "            'products': {},\n",
    "            'reviews': {},\n",
    "            'detection_timestamp': datetime.now().isoformat(),\n",
    "            'search_term': search_term\n",
    "        }\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                self.log_action(f\"DETECT_ATTEMPT_{attempt+1}\", \"INFO\", f\"Tentative {attempt+1}/{max_retries}\")\n",
    "                \n",
    "                # Phase 1: D√©tection produits avec fallbacks\n",
    "                product_selectors = self._detect_product_selectors_with_fallbacks(site_url, search_term, site_type)\n",
    "                \n",
    "                if product_selectors:\n",
    "                    detected['products'] = product_selectors\n",
    "                    self.log_action(\"PRODUCT_SELECTORS_SUCCESS\", \"SUCCESS\", f\"{len(product_selectors)} s√©lecteurs trouv√©s\")\n",
    "                    \n",
    "                    # Phase 2: D√©tection reviews avec fallbacks\n",
    "                    review_selectors = self._detect_review_selectors_with_fallbacks(site_type, product_selectors)\n",
    "                    \n",
    "                    if review_selectors:\n",
    "                        detected['reviews'] = review_selectors\n",
    "                        self.log_action(\"REVIEW_SELECTORS_SUCCESS\", \"SUCCESS\", f\"{len(review_selectors)} s√©lecteurs trouv√©s\")\n",
    "                    else:\n",
    "                        # Fallback vers s√©lecteurs de base\n",
    "                        detected['reviews'] = self.base_selectors[site_type]['reviews']\n",
    "                        self.log_action(\"REVIEW_SELECTORS_FALLBACK\", \"WARNING\", \"Utilisation s√©lecteurs par d√©faut\")\n",
    "                    \n",
    "                    # Sauvegarde s√©curis√©e\n",
    "                    self._save_detected_selectors_secure(detected, site_type)\n",
    "                    return detected\n",
    "                else:\n",
    "                    self.log_action(f\"PRODUCT_SELECTORS_ATTEMPT_{attempt+1}_FAILED\", \"ERROR\", \"Aucun s√©lecteur produit trouv√©\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                self.log_action(f\"DETECT_ATTEMPT_{attempt+1}_ERROR\", \"ERROR\", str(e))\n",
    "                \n",
    "                if attempt < max_retries - 1:\n",
    "                    delay = (attempt + 1) * 5\n",
    "                    self.log_action(\"DETECT_RETRY_DELAY\", \"INFO\", f\"Attente {delay}s\")\n",
    "                    time.sleep(delay)\n",
    "        \n",
    "        # Fallback final : utiliser les s√©lecteurs de base\n",
    "        self.log_action(\"DETECT_FINAL_FALLBACK\", \"WARNING\", \"Utilisation s√©lecteurs de base complets\")\n",
    "        detected['products'] = self._get_fallback_product_selectors(site_type)\n",
    "        detected['reviews'] = self.base_selectors[site_type]['reviews']\n",
    "        \n",
    "        return detected\n",
    "    \n",
    "    def _detect_site_type(self, site_url):\n",
    "        \"\"\"D√©tection robuste du type de site\"\"\"\n",
    "        site_url_lower = site_url.lower()\n",
    "        \n",
    "        if 'amazon' in site_url_lower:\n",
    "            return 'amazon'\n",
    "        elif 'ebay' in site_url_lower:\n",
    "            return 'ebay'\n",
    "        else:\n",
    "            return 'unknown'\n",
    "    \n",
    "    def _detect_product_selectors_with_fallbacks(self, site_url, search_term, site_type, max_page_attempts=3):\n",
    "        \"\"\"D√©tection de s√©lecteurs produits avec fallbacks multiples\"\"\"\n",
    "        \n",
    "        # URLs de recherche alternatives\n",
    "        search_urls = self._generate_search_urls(site_type, search_term)\n",
    "        \n",
    "        for url_attempt, search_url in enumerate(search_urls[:max_page_attempts]):\n",
    "            try:\n",
    "                self.log_action(f\"NAVIGATE_SEARCH_URL_{url_attempt+1}\", \"INFO\", search_url[:80])\n",
    "                \n",
    "                # Navigation s√©curis√©e\n",
    "                if not self._safe_navigate(search_url):\n",
    "                    continue\n",
    "                \n",
    "                # Attendre le chargement avec fallbacks\n",
    "                if not self._wait_for_page_load():\n",
    "                    continue\n",
    "                \n",
    "                # Tester les s√©lecteurs avec diff√©rentes strat√©gies\n",
    "                selectors = self._test_product_selectors_comprehensive(site_type)\n",
    "                \n",
    "                if selectors and len(selectors) >= 2:  # Au moins conteneur + un autre\n",
    "                    self.log_action(\"PRODUCT_SELECTORS_FOUND\", \"SUCCESS\", f\"URL: {url_attempt+1}, S√©lecteurs: {len(selectors)}\")\n",
    "                    return selectors\n",
    "                \n",
    "            except Exception as e:\n",
    "                self.log_action(f\"SEARCH_URL_{url_attempt+1}_ERROR\", \"ERROR\", str(e))\n",
    "                continue\n",
    "        \n",
    "        return {}\n",
    "    \n",
    "    def _generate_search_urls(self, site_type, search_term):\n",
    "        \"\"\"G√©n√®re plusieurs URLs de recherche alternatives\"\"\"\n",
    "        \n",
    "        search_term_encoded = search_term.replace(' ', '+')\n",
    "        search_term_underscore = search_term.replace(' ', '_')\n",
    "        \n",
    "        if site_type == 'amazon':\n",
    "            return [\n",
    "                f\"https://www.amazon.com/s?k={search_term_encoded}\",\n",
    "                f\"https://www.amazon.com/s?k={search_term_encoded}&ref=sr_pg_1\",\n",
    "                f\"https://www.amazon.com/s?field-keywords={search_term_encoded}\",\n",
    "                f\"https://amazon.com/s?k={search_term_encoded}\"\n",
    "            ]\n",
    "        elif site_type == 'ebay':\n",
    "            return [\n",
    "                f\"https://www.ebay.com/sch/i.html?_nkw={search_term_encoded}\",\n",
    "                f\"https://www.ebay.com/sch/i.html?_nkw={search_term_underscore}\",\n",
    "                f\"https://ebay.com/sch/i.html?_nkw={search_term_encoded}\"\n",
    "            ]\n",
    "        \n",
    "        return []\n",
    "    \n",
    "    def _safe_navigate(self, url, max_retries=3):\n",
    "        \"\"\"Navigation s√©curis√©e avec retry\"\"\"\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                self.driver.get(url)\n",
    "                return True\n",
    "                \n",
    "            except TimeoutException:\n",
    "                self.log_action(f\"NAVIGATE_TIMEOUT_ATTEMPT_{attempt+1}\", \"WARNING\", f\"Timeout sur {url}\")\n",
    "                if attempt < max_retries - 1:\n",
    "                    time.sleep(3)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                self.log_action(f\"NAVIGATE_ERROR_ATTEMPT_{attempt+1}\", \"ERROR\", str(e))\n",
    "                if attempt < max_retries - 1:\n",
    "                    time.sleep(5)\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def _wait_for_page_load(self, timeout=15):\n",
    "        \"\"\"Attente du chargement de page avec fallbacks\"\"\"\n",
    "        \n",
    "        try:\n",
    "            # Strat√©gie 1: Attendre que le DOM soit pr√™t\n",
    "            self.wait.until(lambda driver: driver.execute_script(\"return document.readyState\") == \"complete\")\n",
    "            time.sleep(2)\n",
    "            \n",
    "            # Strat√©gie 2: Attendre des √©l√©ments sp√©cifiques\n",
    "            common_selectors = ['body', '[data-component-type]', '.s-result-item', '.s-item']\n",
    "            \n",
    "            for selector in common_selectors:\n",
    "                try:\n",
    "                    WebDriverWait(self.driver, 5).until(\n",
    "                        EC.presence_of_element_located((By.CSS_SELECTOR, selector))\n",
    "                    )\n",
    "                    break\n",
    "                except TimeoutException:\n",
    "                    continue\n",
    "            \n",
    "            # D√©lai suppl√©mentaire pour le JavaScript\n",
    "            time.sleep(3)\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.log_action(\"PAGE_LOAD_WARNING\", \"WARNING\", str(e))\n",
    "            time.sleep(5)  # Fallback basique\n",
    "            return True  # Continue m√™me en cas d'erreur\n",
    "    \n",
    "    def _test_product_selectors_comprehensive(self, site_type):\n",
    "        \"\"\"Test complet des s√©lecteurs produits avec scoring\"\"\"\n",
    "        \n",
    "        base_selectors = self.base_selectors[site_type]['products']\n",
    "        validated_selectors = {}\n",
    "        selector_scores = {}\n",
    "        \n",
    "        # Test conteneurs avec scoring\n",
    "        self.log_action(\"TEST_CONTAINERS_START\", \"INFO\", f\"Test de {len(base_selectors['containers'])} conteneurs\")\n",
    "        \n",
    "        for container_selector in base_selectors['containers']:\n",
    "            try:\n",
    "                elements = self.driver.find_elements(By.CSS_SELECTOR, container_selector)\n",
    "                score = len(elements)\n",
    "                \n",
    "                if score >= 3:  # Minimum 3 produits\n",
    "                    selector_scores[container_selector] = score\n",
    "                    self.log_action(\"CONTAINER_VALID\", \"SUCCESS\", f\"{container_selector}: {score} √©l√©ments\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                self.log_action(\"CONTAINER_ERROR\", \"WARNING\", f\"{container_selector}: {str(e)[:50]}\")\n",
    "                continue\n",
    "        \n",
    "        # Choisir le meilleur conteneur\n",
    "        if selector_scores:\n",
    "            best_container = max(selector_scores, key=selector_scores.get)\n",
    "            validated_selectors['container'] = best_container\n",
    "            self.log_action(\"BEST_CONTAINER_SELECTED\", \"SUCCESS\", f\"{best_container} (score: {selector_scores[best_container]})\")\n",
    "        else:\n",
    "            self.log_action(\"NO_CONTAINER_FOUND\", \"ERROR\", \"Aucun conteneur valide trouv√©\")\n",
    "            return {}\n",
    "        \n",
    "        # Test autres s√©lecteurs dans le contexte du meilleur conteneur\n",
    "        container_elements = self.driver.find_elements(By.CSS_SELECTOR, best_container)\n",
    "        \n",
    "        if container_elements:\n",
    "            first_container = container_elements[0]\n",
    "            \n",
    "            # Test chaque type de s√©lecteur\n",
    "            selector_types = ['titles', 'urls', 'prices', 'ratings']\n",
    "            \n",
    "            for selector_type in selector_types:\n",
    "                self.log_action(f\"TEST_{selector_type.upper()}_START\", \"INFO\", f\"Test de {len(base_selectors[selector_type])} {selector_type}\")\n",
    "                \n",
    "                for selector in base_selectors[selector_type]:\n",
    "                    try:\n",
    "                        element = first_container.find_element(By.CSS_SELECTOR, selector)\n",
    "                        \n",
    "                        # Validation sp√©cifique par type\n",
    "                        if self._validate_selector_content(element, selector_type):\n",
    "                            validated_selectors[selector_type[:-1]] = selector  # Enlever le 's'\n",
    "                            self.log_action(f\"{selector_type.upper()}_VALID\", \"SUCCESS\", f\"{selector}\")\n",
    "                            break\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        continue\n",
    "        \n",
    "        return validated_selectors\n",
    "    \n",
    "    def _validate_selector_content(self, element, selector_type):\n",
    "        \"\"\"Validation du contenu selon le type de s√©lecteur\"\"\"\n",
    "        \n",
    "        try:\n",
    "            if selector_type == 'titles':\n",
    "                text = element.text.strip()\n",
    "                return len(text) > 5 and not text.isdigit()\n",
    "            \n",
    "            elif selector_type == 'urls':\n",
    "                href = element.get_attribute('href')\n",
    "                return href and ('amazon.com' in href or 'ebay.com' in href or '/dp/' in href)\n",
    "            \n",
    "            elif selector_type == 'prices':\n",
    "                text = element.text.strip()\n",
    "                return text and ('$' in text or '‚Ç¨' in text or '¬£' in text or any(c.isdigit() for c in text))\n",
    "            \n",
    "            elif selector_type == 'ratings':\n",
    "                text = element.get_attribute('textContent') or element.text\n",
    "                return text and ('star' in text.lower() or '√©toile' in text.lower() or any(c.isdigit() for c in text))\n",
    "            \n",
    "        except Exception:\n",
    "            return False\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def _detect_review_selectors_with_fallbacks(self, site_type, product_selectors):\n",
    "        \"\"\"D√©tection s√©lecteurs reviews avec navigation s√©curis√©e\"\"\"\n",
    "        \n",
    "        if not product_selectors.get('url'):\n",
    "            self.log_action(\"REVIEW_DETECT_NO_URL\", \"ERROR\", \"Pas de s√©lecteur URL produit\")\n",
    "            return {}\n",
    "        \n",
    "        try:\n",
    "            # R√©cup√©rer plusieurs liens produits pour plus de robustesse\n",
    "            product_links = self.driver.find_elements(By.CSS_SELECTOR, product_selectors['url'])[:3]\n",
    "            \n",
    "            for link_idx, link in enumerate(product_links):\n",
    "                try:\n",
    "                    product_url = link.get_attribute('href')\n",
    "                    if not product_url:\n",
    "                        continue\n",
    "                    \n",
    "                    self.log_action(f\"TEST_PRODUCT_{link_idx+1}\", \"INFO\", product_url[:80])\n",
    "                    \n",
    "                    # Navigation vers le produit\n",
    "                    if not self._safe_navigate(product_url):\n",
    "                        continue\n",
    "                    \n",
    "                    if not self._wait_for_page_load():\n",
    "                        continue\n",
    "                    \n",
    "                    # Chercher et naviguer vers les reviews\n",
    "                    reviews_url = self._find_reviews_page_comprehensive(product_url)\n",
    "                    \n",
    "                    if reviews_url:\n",
    "                        self.log_action(\"REVIEWS_PAGE_FOUND\", \"SUCCESS\", reviews_url[:80])\n",
    "                        \n",
    "                        if not self._safe_navigate(reviews_url):\n",
    "                            continue\n",
    "                        \n",
    "                        if not self._wait_for_page_load():\n",
    "                            continue\n",
    "                    \n",
    "                    # Tester les s√©lecteurs de reviews\n",
    "                    review_selectors = self._test_review_selectors_comprehensive(site_type)\n",
    "                    \n",
    "                    if review_selectors and len(review_selectors) >= 2:\n",
    "                        self.log_action(\"REVIEW_SELECTORS_SUCCESS\", \"SUCCESS\", f\"Produit {link_idx+1}: {len(review_selectors)} s√©lecteurs\")\n",
    "                        return review_selectors\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    self.log_action(f\"REVIEW_PRODUCT_{link_idx+1}_ERROR\", \"ERROR\", str(e))\n",
    "                    continue\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.log_action(\"REVIEW_DETECT_GLOBAL_ERROR\", \"ERROR\", str(e))\n",
    "        \n",
    "        return {}\n",
    "    \n",
    "    def _find_reviews_page_comprehensive(self, product_url):\n",
    "        \"\"\"Recherche comprehensive de la page reviews\"\"\"\n",
    "        \n",
    "        # Strat√©gie 1: Chercher les liens existants\n",
    "        review_link_selectors = [\n",
    "            'a[href*=\"customer-reviews\"]',\n",
    "            'a[href*=\"reviews\"]',\n",
    "            'a[href*=\"review\"]',\n",
    "            '.cr-widget-ACR a',\n",
    "            '[data-hook=\"see-all-reviews-link-foot\"]',\n",
    "            'a[href*=\"product-reviews\"]'\n",
    "        ]\n",
    "        \n",
    "        for selector in review_link_selectors:\n",
    "            try:\n",
    "                links = self.driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                for link in links:\n",
    "                    href = link.get_attribute('href')\n",
    "                    if href and 'review' in href:\n",
    "                        self.log_action(\"REVIEW_LINK_FOUND\", \"SUCCESS\", f\"Via {selector}\")\n",
    "                        return href\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        # Strat√©gie 2: Construction d'URL pour Amazon\n",
    "        if 'amazon.com' in product_url:\n",
    "            asin_patterns = [\n",
    "                r'/dp/([A-Z0-9]{10})',\n",
    "                r'/gp/product/([A-Z0-9]{10})',\n",
    "                r'asin=([A-Z0-9]{10})'\n",
    "            ]\n",
    "            \n",
    "            for pattern in asin_patterns:\n",
    "                match = re.search(pattern, product_url)\n",
    "                if match:\n",
    "                    asin = match.group(1)\n",
    "                    constructed_url = f\"https://www.amazon.com/product-reviews/{asin}\"\n",
    "                    self.log_action(\"REVIEW_URL_CONSTRUCTED\", \"SUCCESS\", f\"ASIN: {asin}\")\n",
    "                    return constructed_url\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def _test_review_selectors_comprehensive(self, site_type):\n",
    "        \"\"\"Test complet des s√©lecteurs de reviews\"\"\"\n",
    "        \n",
    "        base_selectors = self.base_selectors[site_type]['reviews']\n",
    "        validated_selectors = {}\n",
    "        \n",
    "        # Test conteneurs de reviews\n",
    "        self.log_action(\"TEST_REVIEW_CONTAINERS\", \"INFO\", f\"Test de {len(base_selectors['containers'])} conteneurs\")\n",
    "        \n",
    "        best_container = None\n",
    "        max_reviews = 0\n",
    "        \n",
    "        for container_selector in base_selectors['containers']:\n",
    "            try:\n",
    "                review_elements = self.driver.find_elements(By.CSS_SELECTOR, container_selector)\n",
    "                if len(review_elements) > max_reviews:\n",
    "                    max_reviews = len(review_elements)\n",
    "                    best_container = container_selector\n",
    "                    \n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        if best_container and max_reviews >= 2:\n",
    "            validated_selectors['container'] = best_container\n",
    "            self.log_action(\"REVIEW_CONTAINER_SELECTED\", \"SUCCESS\", f\"{best_container}: {max_reviews} reviews\")\n",
    "            \n",
    "            # Test autres s√©lecteurs dans le contexte\n",
    "            review_containers = self.driver.find_elements(By.CSS_SELECTOR, best_container)\n",
    "            \n",
    "            if review_containers:\n",
    "                first_review = review_containers[0]\n",
    "                \n",
    "                # Test chaque type de s√©lecteur review\n",
    "                review_types = ['texts', 'titles', 'ratings', 'authors', 'dates']\n",
    "                \n",
    "                for review_type in review_types:\n",
    "                    if review_type in base_selectors:\n",
    "                        for selector in base_selectors[review_type]:\n",
    "                            try:\n",
    "                                element = first_review.find_element(By.CSS_SELECTOR, selector)\n",
    "                                \n",
    "                                if self._validate_review_content(element, review_type):\n",
    "                                    validated_selectors[review_type[:-1]] = selector  # Enlever le 's'\n",
    "                                    self.log_action(f\"REVIEW_{review_type.upper()}_VALID\", \"SUCCESS\", selector)\n",
    "                                    break\n",
    "                                    \n",
    "                            except:\n",
    "                                continue\n",
    "        \n",
    "        return validated_selectors\n",
    "    \n",
    "    def _validate_review_content(self, element, content_type):\n",
    "        \"\"\"Validation du contenu des reviews\"\"\"\n",
    "        \n",
    "        try:\n",
    "            if content_type == 'texts':\n",
    "                text = element.text.strip()\n",
    "                return len(text) > 20  # Review doit avoir au moins 20 caract√®res\n",
    "            \n",
    "            elif content_type == 'titles':\n",
    "                text = element.text.strip()\n",
    "                return len(text) > 5 and len(text) < 200\n",
    "            \n",
    "            elif content_type == 'ratings':\n",
    "                text = element.get_attribute('textContent') or element.text\n",
    "                return text and ('star' in text.lower() or any(c.isdigit() for c in text))\n",
    "            \n",
    "            elif content_type == 'authors':\n",
    "                text = element.text.strip()\n",
    "                return len(text) > 1 and len(text) < 100\n",
    "            \n",
    "            elif content_type == 'dates':\n",
    "                text = element.text.strip()\n",
    "                return len(text) > 5 and ('20' in text or 'on ' in text.lower() or 'le ' in text.lower())\n",
    "            \n",
    "        except:\n",
    "            return False\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def _get_fallback_product_selectors(self, site_type):\n",
    "        \"\"\"S√©lecteurs de fallback robustes\"\"\"\n",
    "        \n",
    "        if site_type == 'amazon':\n",
    "            return {\n",
    "                'container': '[data-component-type=\"s-search-result\"]',\n",
    "                'title': 'h2 span',\n",
    "                'url': 'h2 a',\n",
    "                'price': '.a-price .a-offscreen',\n",
    "                'rating': '.a-icon-alt'\n",
    "            }\n",
    "        elif site_type == 'ebay':\n",
    "            return {\n",
    "                'container': '.s-item',\n",
    "                'title': '.s-item__title',\n",
    "                'url': '.s-item__link',\n",
    "                'price': '.s-item__price'\n",
    "            }\n",
    "        \n",
    "        return {}\n",
    "    \n",
    "    def _save_detected_selectors_secure(self, selectors, site_type):\n",
    "        \"\"\"Sauvegarde s√©curis√©e avec backup\"\"\"\n",
    "        \n",
    "        try:\n",
    "            # Cr√©er le r√©pertoire de config\n",
    "            config_dir = \"../config\"\n",
    "            os.makedirs(config_dir, exist_ok=True)\n",
    "            \n",
    "            # Nom de fichier avec timestamp\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            filename = f\"{config_dir}/detected_selectors_{site_type}_{timestamp}.json\"\n",
    "            backup_filename = f\"{config_dir}/detected_selectors_{site_type}_backup.json\"\n",
    "            current_filename = f\"{config_dir}/detected_selectors_{site_type}.json\"\n",
    "            \n",
    "            # Sauvegarder avec timestamp\n",
    "            with open(filename, 'w', encoding='utf-8') as f:\n",
    "                json.dump(selectors, f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "            # Sauvegarder backup\n",
    "            with open(backup_filename, 'w', encoding='utf-8') as f:\n",
    "                json.dump(selectors, f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "            # Sauvegarder current (sans timestamp)\n",
    "            with open(current_filename, 'w', encoding='utf-8') as f:\n",
    "                json.dump(selectors, f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "            self.log_action(\"SELECTORS_SAVED\", \"SUCCESS\", f\"3 fichiers sauvegard√©s dans {config_dir}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.log_action(\"SAVE_ERROR\", \"ERROR\", str(e))\n",
    "            return False\n",
    "    \n",
    "    def get_session_log(self):\n",
    "        \"\"\"Retourne le log de session\"\"\"\n",
    "        return self.session_log\n",
    "    \n",
    "    def save_session_log(self, filename=None):\n",
    "        \"\"\"Sauvegarde le log de session\"\"\"\n",
    "        try:\n",
    "            if filename is None:\n",
    "                timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "                filename = f\"../logs/scout_session_{timestamp}.json\"\n",
    "            \n",
    "            os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "            \n",
    "            with open(filename, 'w', encoding='utf-8') as f:\n",
    "                json.dump(self.session_log, f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "            self.log_action(\"SESSION_LOG_SAVED\", \"SUCCESS\", filename)\n",
    "            return filename\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.log_action(\"SESSION_LOG_SAVE_ERROR\", \"ERROR\", str(e))\n",
    "            return None\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Fermeture s√©curis√©e avec log\"\"\"\n",
    "        self.log_action(\"SCOUT_CLOSING\", \"INFO\", \"Fermeture du scout\")\n",
    "        \n",
    "        try:\n",
    "            if self.driver:\n",
    "                self.driver.quit()\n",
    "                self.driver = None\n",
    "            \n",
    "            for driver in self.fallback_drivers:\n",
    "                try:\n",
    "                    driver.quit()\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            self.fallback_drivers.clear()\n",
    "            \n",
    "            # Sauvegarder le log automatiquement\n",
    "            self.save_session_log()\n",
    "            \n",
    "            self.log_action(\"SCOUT_CLOSED\", \"SUCCESS\", \"Scout ferm√© proprement\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.log_action(\"SCOUT_CLOSE_ERROR\", \"ERROR\", str(e))\n",
    "\n",
    "print(\"‚úÖ Scout ultra-s√©curis√© cr√©√© avec fallbacks multiples et logging!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68dbacc",
   "metadata": {},
   "source": [
    "# üîí Version Ultra-S√©curis√©e du Scout (2025-01-20)\n",
    "\n",
    "Cette version am√©liore encore le scout avec :\n",
    "- **Fallbacks multiples** pour la cr√©ation du driver (undetected_chromedriver, Selenium Chrome, Firefox, Edge)\n",
    "- **D√©tection adaptative** des balises avec scoring et fallback sur s√©lecteurs de base\n",
    "- **Gestion d'erreurs exhaustive** avec logs datetime\n",
    "- **Sauvegarde automatique** des s√©lecteurs d√©tect√©s avec timestamp et backup\n",
    "- **Navigation ultra-s√©curis√©e** avec anti-d√©tection avanc√©\n",
    "- **Options Chrome optimis√©es** pour √©viter la d√©tection\n",
    "- **Rotation d'user-agents** r√©alistes\n",
    "- **Gestion des proxies** (si configur√©s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "961cb153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Cr√©ation de la classe UltraSecureProductReviewScout...\n",
      "‚úÖ Classe UltraSecureProductReviewScout cr√©√©e avec succ√®s!\n"
     ]
    }
   ],
   "source": [
    "# üîí CLASSE ULTRA-S√âCURIS√âE POUR D√âTECTION DES BALISES (2025-01-20)\n",
    "print(\"üîÑ Cr√©ation de la classe UltraSecureProductReviewScout...\")\n",
    "\n",
    "class UltraSecureProductReviewScout:\n",
    "    \"\"\"\n",
    "    Scout ultra-s√©curis√© avec fallbacks multiples pour la d√©tection des balises\n",
    "    Am√©liorations 2025-01-20 :\n",
    "    - Fallbacks multiples pour driver (undetected_chromedriver, Selenium, Firefox, Edge)\n",
    "    - D√©tection adaptative avec scoring et fallback\n",
    "    - Options Chrome optimis√©es anti-d√©tection\n",
    "    - Gestion exhaustive des erreurs avec logs datetime\n",
    "    - Sauvegarde automatique des s√©lecteurs et logs\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, headless=True, save_screenshots=True, use_proxy=None):\n",
    "        self.headless = headless\n",
    "        self.save_screenshots = save_screenshots\n",
    "        self.use_proxy = use_proxy\n",
    "        self.driver = None\n",
    "        self.fallback_drivers = []\n",
    "        self.session_log = {\n",
    "            \"session_start\": datetime.now().isoformat(),\n",
    "            \"actions\": [],\n",
    "            \"detected_selectors\": {},\n",
    "            \"fallback_attempts\": [],\n",
    "            \"errors\": []\n",
    "        }\n",
    "        self.detected_selectors = {}\n",
    "        self.current_site = None\n",
    "        \n",
    "        # User agents r√©alistes rotatifs\n",
    "        self.user_agents = [\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36\",\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n",
    "            \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36\",\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:122.0) Gecko/20100101 Firefox/122.0\",\n",
    "            \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2.1 Safari/605.1.15\"\n",
    "        ]\n",
    "        self.current_user_agent = random.choice(self.user_agents)\n",
    "        \n",
    "    def log_action(self, action_type, status, details=\"\", extra_data=None):\n",
    "        \"\"\"Log avec datetime pour toutes les actions\"\"\"\n",
    "        log_entry = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"action\": action_type,\n",
    "            \"status\": status,\n",
    "            \"details\": details\n",
    "        }\n",
    "        if extra_data:\n",
    "            log_entry[\"extra_data\"] = extra_data\n",
    "            \n",
    "        self.session_log[\"actions\"].append(log_entry)\n",
    "        \n",
    "        # Affichage avec datetime pour les actions importantes\n",
    "        if status in [\"ERROR\", \"SUCCESS\", \"WARNING\"]:\n",
    "            timestamp = datetime.now().strftime(\"%H:%M:%S\")\n",
    "            print(f\"[{timestamp}] {action_type}: {status} - {details}\")\n",
    "    \n",
    "    def get_ultra_secure_chrome_options(self):\n",
    "        \"\"\"Options Chrome ultra-s√©curis√©es optimis√©es\"\"\"\n",
    "        options = Options()\n",
    "        \n",
    "        if self.headless:\n",
    "            options.add_argument(\"--headless=new\")\n",
    "        \n",
    "        # Arguments anti-d√©tection optimis√©s\n",
    "        anti_detection_args = [\n",
    "            \"--no-sandbox\",\n",
    "            \"--disable-dev-shm-usage\",\n",
    "            \"--disable-gpu\",\n",
    "            \"--disable-software-rasterizer\",\n",
    "            \"--disable-background-timer-throttling\",\n",
    "            \"--disable-backgrounding-occluded-windows\",\n",
    "            \"--disable-renderer-backgrounding\",\n",
    "            \"--disable-infobars\",\n",
    "            \"--disable-extensions\",\n",
    "            \"--disable-plugins\",\n",
    "            \"--disable-images\",  # Acc√©l√®re le chargement\n",
    "            \"--disable-javascript\",  # Peut √™tre retir√© si JS n√©cessaire\n",
    "            \"--no-first-run\",\n",
    "            \"--no-default-browser-check\",\n",
    "            \"--ignore-certificate-errors\",\n",
    "            \"--ignore-ssl-errors\",\n",
    "            \"--ignore-certificate-errors-spki-list\",\n",
    "            \"--disable-blink-features=AutomationControlled\",\n",
    "            \"--disable-features=VizDisplayCompositor\",\n",
    "            \"--window-size=1920,1080\",\n",
    "            \"--start-maximized\"\n",
    "        ]\n",
    "        \n",
    "        for arg in anti_detection_args:\n",
    "            options.add_argument(arg)\n",
    "        \n",
    "        # User agent r√©aliste\n",
    "        options.add_argument(f\"--user-agent={self.current_user_agent}\")\n",
    "        \n",
    "        # Pr√©f√©rences avanc√©es anti-d√©tection\n",
    "        prefs = {\n",
    "            \"profile.default_content_setting_values\": {\n",
    "                \"notifications\": 2,\n",
    "                \"media_stream\": 2,\n",
    "                \"geolocation\": 2\n",
    "            },\n",
    "            \"profile.managed_default_content_settings\": {\n",
    "                \"images\": 2  # Bloquer les images pour acc√©l√©rer\n",
    "            }\n",
    "        }\n",
    "        options.add_experimental_option(\"prefs\", prefs)\n",
    "        \n",
    "        # Proxy si configur√©\n",
    "        if self.use_proxy:\n",
    "            options.add_argument(f\"--proxy-server={self.use_proxy}\")\n",
    "        \n",
    "        return options\n",
    "    \n",
    "    def create_fallback_driver(self, driver_type=\"undetected_chrome\"):\n",
    "        \"\"\"Cr√©ation de driver avec fallbacks multiples\"\"\"\n",
    "        self.log_action(\"DRIVER_CREATION_START\", \"INFO\", f\"Tentative cr√©ation driver: {driver_type}\")\n",
    "        \n",
    "        try:\n",
    "            if driver_type == \"undetected_chrome\":\n",
    "                import undetected_chromedriver as uc\n",
    "                options = self.get_ultra_secure_chrome_options()\n",
    "                \n",
    "                driver = uc.Chrome(\n",
    "                    options=options,\n",
    "                    version_main=None,  # Auto-d√©tection de la version\n",
    "                    driver_executable_path=None,\n",
    "                    browser_executable_path=None,\n",
    "                    use_subprocess=True,\n",
    "                    debug=False\n",
    "                )\n",
    "                \n",
    "                # Scripts anti-d√©tection\n",
    "                driver.execute_cdp_cmd('Page.addScriptToEvaluateOnNewDocument', {\n",
    "                    \"source\": \"\"\"\n",
    "                        Object.defineProperty(navigator, 'webdriver', {\n",
    "                            get: () => undefined,\n",
    "                        });\n",
    "                        delete navigator.__proto__.webdriver;\n",
    "                    \"\"\"\n",
    "                })\n",
    "                \n",
    "                self.log_action(\"DRIVER_CREATION_SUCCESS\", \"SUCCESS\", f\"Driver {driver_type} cr√©√©\")\n",
    "                return driver\n",
    "                \n",
    "            elif driver_type == \"selenium_chrome\":\n",
    "                from selenium import webdriver\n",
    "                options = self.get_ultra_secure_chrome_options()\n",
    "                \n",
    "                driver = webdriver.Chrome(options=options)\n",
    "                \n",
    "                # Script anti-d√©tection pour Selenium\n",
    "                driver.execute_script(\"\"\"\n",
    "                    Object.defineProperty(navigator, 'webdriver', {\n",
    "                        get: () => undefined,\n",
    "                    });\n",
    "                    delete navigator.__proto__.webdriver;\n",
    "                \"\"\")\n",
    "                \n",
    "                self.log_action(\"DRIVER_CREATION_SUCCESS\", \"SUCCESS\", f\"Driver {driver_type} cr√©√©\")\n",
    "                return driver\n",
    "                \n",
    "            elif driver_type == \"firefox\":\n",
    "                from selenium import webdriver\n",
    "                from selenium.webdriver.firefox.options import Options as FirefoxOptions\n",
    "                \n",
    "                firefox_options = FirefoxOptions()\n",
    "                if self.headless:\n",
    "                    firefox_options.add_argument(\"--headless\")\n",
    "                \n",
    "                firefox_options.add_argument(\"--no-sandbox\")\n",
    "                firefox_options.add_argument(\"--disable-gpu\")\n",
    "                firefox_options.set_preference(\"general.useragent.override\", self.current_user_agent)\n",
    "                \n",
    "                driver = webdriver.Firefox(options=firefox_options)\n",
    "                self.log_action(\"DRIVER_CREATION_SUCCESS\", \"SUCCESS\", f\"Driver {driver_type} cr√©√©\")\n",
    "                return driver\n",
    "                \n",
    "            elif driver_type == \"edge\":\n",
    "                from selenium import webdriver\n",
    "                from selenium.webdriver.edge.options import Options as EdgeOptions\n",
    "                \n",
    "                edge_options = EdgeOptions()\n",
    "                if self.headless:\n",
    "                    edge_options.add_argument(\"--headless\")\n",
    "                \n",
    "                edge_options.add_argument(\"--no-sandbox\")\n",
    "                edge_options.add_argument(\"--disable-gpu\")\n",
    "                edge_options.add_argument(f\"--user-agent={self.current_user_agent}\")\n",
    "                \n",
    "                driver = webdriver.Edge(options=edge_options)\n",
    "                self.log_action(\"DRIVER_CREATION_SUCCESS\", \"SUCCESS\", f\"Driver {driver_type} cr√©√©\")\n",
    "                return driver\n",
    "                \n",
    "            else:\n",
    "                raise ValueError(f\"Type de driver non support√©: {driver_type}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            self.log_action(\"DRIVER_CREATION_ERROR\", \"ERROR\", f\"√âchec {driver_type}: {str(e)}\")\n",
    "            self.session_log[\"fallback_attempts\"].append({\n",
    "                \"driver_type\": driver_type,\n",
    "                \"error\": str(e),\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            })\n",
    "            return None\n",
    "    \n",
    "    def initialize_driver(self):\n",
    "        \"\"\"Initialisation du driver avec fallbacks multiples\"\"\"\n",
    "        driver_types = [\"undetected_chrome\", \"selenium_chrome\", \"firefox\", \"edge\"]\n",
    "        \n",
    "        self.log_action(\"DRIVER_INITIALIZATION_START\", \"INFO\", \"D√©marrage des fallbacks multiples\")\n",
    "        \n",
    "        for driver_type in driver_types:\n",
    "            try:\n",
    "                self.log_action(\"FALLBACK_ATTEMPT\", \"INFO\", f\"Tentative avec {driver_type}\")\n",
    "                \n",
    "                driver = self.create_fallback_driver(driver_type)\n",
    "                if driver:\n",
    "                    self.driver = driver\n",
    "                    self.log_action(\"DRIVER_INITIALIZED\", \"SUCCESS\", f\"Driver {driver_type} initialis√©\")\n",
    "                    \n",
    "                    # Test de fonctionnement\n",
    "                    driver.get(\"https://httpbin.org/user-agent\")\n",
    "                    time.sleep(2)\n",
    "                    \n",
    "                    self.log_action(\"DRIVER_TEST_SUCCESS\", \"SUCCESS\", f\"Driver {driver_type} fonctionnel\")\n",
    "                    return True\n",
    "                    \n",
    "            except Exception as e:\n",
    "                self.log_action(\"FALLBACK_ERROR\", \"ERROR\", f\"√âchec {driver_type}: {str(e)}\")\n",
    "                if driver:\n",
    "                    try:\n",
    "                        driver.quit()\n",
    "                    except:\n",
    "                        pass\n",
    "                continue\n",
    "        \n",
    "        self.log_action(\"DRIVER_INITIALIZATION_FAILED\", \"ERROR\", \"Tous les drivers ont √©chou√©\")\n",
    "        return False\n",
    "    \n",
    "    def detect_product_elements(self, sample_urls, max_products=5):\n",
    "        \"\"\"D√©tection adaptative des balises produits avec scoring et fallback\"\"\"\n",
    "        if not self.driver:\n",
    "            if not self.initialize_driver():\n",
    "                self.log_action(\"DETECTION_FAILED\", \"ERROR\", \"Aucun driver disponible\")\n",
    "                return {}\n",
    "        \n",
    "        site_selectors = {\n",
    "            \"product_link\": [],\n",
    "            \"product_title\": [],\n",
    "            \"rating\": [],\n",
    "            \"review_count\": [],\n",
    "            \"price\": []\n",
    "        }\n",
    "        \n",
    "        selector_scores = {}\n",
    "        \n",
    "        self.log_action(\"PRODUCT_DETECTION_START\", \"INFO\", f\"Analyse de {len(sample_urls)} URLs\")\n",
    "        \n",
    "        for i, url in enumerate(sample_urls[:max_products]):\n",
    "            try:\n",
    "                self.log_action(\"PAGE_ANALYSIS_START\", \"INFO\", f\"Analyse page {i+1}: {url[:50]}...\")\n",
    "                \n",
    "                self.driver.get(url)\n",
    "                time.sleep(random.uniform(3, 6))\n",
    "                \n",
    "                # D√©tection des liens produits\n",
    "                product_selectors = [\n",
    "                    'a[href*=\"/dp/\"]',  # Amazon\n",
    "                    'a[href*=\"/gp/product/\"]',  # Amazon\n",
    "                    'a[data-testid*=\"product\"]',  # G√©n√©rique\n",
    "                    'a[class*=\"product\"]',  # G√©n√©rique\n",
    "                    '.s-result-item a',  # Amazon search\n",
    "                    '[data-component-type=\"s-search-result\"] a'  # Amazon\n",
    "                ]\n",
    "                \n",
    "                for selector in product_selectors:\n",
    "                    try:\n",
    "                        elements = self.driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                        if elements:\n",
    "                            score = len(elements)\n",
    "                            if selector not in selector_scores:\n",
    "                                selector_scores[selector] = 0\n",
    "                            selector_scores[selector] += score\n",
    "                            self.log_action(\"SELECTOR_FOUND\", \"SUCCESS\", f\"{selector}: {score} √©l√©ments\")\n",
    "                    except:\n",
    "                        continue\n",
    "                \n",
    "                # D√©tection des titres\n",
    "                title_selectors = [\n",
    "                    'h1[class*=\"title\"]',\n",
    "                    'h2[class*=\"title\"]', \n",
    "                    '[data-testid*=\"title\"]',\n",
    "                    '.s-size-mini .s-color-base',  # Amazon\n",
    "                    'h3 a span'  # Amazon\n",
    "                ]\n",
    "                \n",
    "                for selector in title_selectors:\n",
    "                    try:\n",
    "                        elements = self.driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                        if elements:\n",
    "                            score = len([e for e in elements if e.text.strip()])\n",
    "                            if selector not in selector_scores:\n",
    "                                selector_scores[selector] = 0\n",
    "                            selector_scores[selector] += score\n",
    "                    except:\n",
    "                        continue\n",
    "                \n",
    "                # D√©tection des ratings\n",
    "                rating_selectors = [\n",
    "                    '[class*=\"rating\"]',\n",
    "                    '[data-testid*=\"rating\"]',\n",
    "                    '.a-icon-alt',  # Amazon\n",
    "                    '[aria-label*=\"star\"]',\n",
    "                    '[aria-label*=\"√©toile\"]'\n",
    "                ]\n",
    "                \n",
    "                for selector in rating_selectors:\n",
    "                    try:\n",
    "                        elements = self.driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                        if elements:\n",
    "                            score = len(elements)\n",
    "                            if selector not in selector_scores:\n",
    "                                selector_scores[selector] = 0\n",
    "                            selector_scores[selector] += score\n",
    "                    except:\n",
    "                        continue\n",
    "                \n",
    "                self.log_action(\"PAGE_ANALYSIS_SUCCESS\", \"SUCCESS\", f\"Page {i+1} analys√©e\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                self.log_action(\"PAGE_ANALYSIS_ERROR\", \"ERROR\", f\"Erreur page {i+1}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        # S√©lection des meilleurs s√©lecteurs\n",
    "        best_selectors = {}\n",
    "        for element_type in site_selectors.keys():\n",
    "            relevant_selectors = [(sel, score) for sel, score in selector_scores.items() \n",
    "                                if any(keyword in sel.lower() for keyword in [element_type, 'product', 'title', 'rating', 'price'])]\n",
    "            \n",
    "            if relevant_selectors:\n",
    "                best_selector = max(relevant_selectors, key=lambda x: x[1])[0]\n",
    "                best_selectors[element_type] = best_selector\n",
    "                self.log_action(\"BEST_SELECTOR_FOUND\", \"SUCCESS\", f\"{element_type}: {best_selector}\")\n",
    "        \n",
    "        # Fallback sur s√©lecteurs de base si d√©tection insuffisante\n",
    "        if len(best_selectors) < 2:\n",
    "            self.log_action(\"FALLBACK_TO_BASE_SELECTORS\", \"WARNING\", \"Utilisation des s√©lecteurs de base\")\n",
    "            \n",
    "            fallback_selectors = {\n",
    "                \"amazon\": {\n",
    "                    \"product_link\": 'a[href*=\"/dp/\"]',\n",
    "                    \"product_title\": 'h3 a span',\n",
    "                    \"rating\": '.a-icon-alt',\n",
    "                    \"review_count\": 'a[href*=\"#customerReviews\"]',\n",
    "                    \"price\": '.a-price-whole'\n",
    "                },\n",
    "                \"ebay\": {\n",
    "                    \"product_link\": 'a[href*=\"/itm/\"]',\n",
    "                    \"product_title\": '.s-item__title',\n",
    "                    \"rating\": '.ebay-star-rating',\n",
    "                    \"review_count\": '.s-item__reviews',\n",
    "                    \"price\": '.s-item__price'\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            site_name = \"amazon\" if \"amazon\" in sample_urls[0] else \"ebay\"\n",
    "            best_selectors.update(fallback_selectors.get(site_name, {}))\n",
    "        \n",
    "        self.detected_selectors[self.current_site] = best_selectors\n",
    "        self.log_action(\"DETECTION_COMPLETE\", \"SUCCESS\", f\"{len(best_selectors)} s√©lecteurs d√©tect√©s\")\n",
    "        \n",
    "        return best_selectors\n",
    "    \n",
    "    def save_detected_selectors(self, site_name):\n",
    "        \"\"\"Sauvegarde automatique des s√©lecteurs avec timestamp et backup\"\"\"\n",
    "        try:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            \n",
    "            # Dossier config\n",
    "            config_dir = \"../config\"\n",
    "            os.makedirs(config_dir, exist_ok=True)\n",
    "            \n",
    "            # Fichier principal\n",
    "            filename = f\"detected_selectors_{site_name}.json\"\n",
    "            filepath = os.path.join(config_dir, filename)\n",
    "            \n",
    "            # Backup de l'ancien fichier s'il existe\n",
    "            if os.path.exists(filepath):\n",
    "                backup_filename = f\"detected_selectors_{site_name}_backup_{timestamp}.json\"\n",
    "                backup_filepath = os.path.join(config_dir, backup_filename)\n",
    "                shutil.copy2(filepath, backup_filepath)\n",
    "                self.log_action(\"BACKUP_CREATED\", \"SUCCESS\", backup_filename)\n",
    "            \n",
    "            # Sauvegarde des nouveaux s√©lecteurs\n",
    "            selector_data = {\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"site\": site_name,\n",
    "                \"selectors\": self.detected_selectors.get(site_name, {}),\n",
    "                \"session_info\": {\n",
    "                    \"user_agent\": self.current_user_agent,\n",
    "                    \"detection_method\": \"ultra_secure_scout\",\n",
    "                    \"version\": \"2025-01-20\"\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            with open(filepath, 'w', encoding='utf-8') as f:\n",
    "                json.dump(selector_data, f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "            self.log_action(\"SELECTORS_SAVED\", \"SUCCESS\", filename)\n",
    "            return filename\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.log_action(\"SELECTORS_SAVE_ERROR\", \"ERROR\", str(e))\n",
    "            return None\n",
    "    \n",
    "    def save_session_log(self):\n",
    "        \"\"\"Sauvegarde automatique du log de session\"\"\"\n",
    "        try:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            \n",
    "            # Dossier logs\n",
    "            logs_dir = \"../logs\"\n",
    "            os.makedirs(logs_dir, exist_ok=True)\n",
    "            \n",
    "            filename = f\"ultra_scout_session_{timestamp}.json\"\n",
    "            filepath = os.path.join(logs_dir, filename)\n",
    "            \n",
    "            # Finalisation du log\n",
    "            self.session_log[\"session_end\"] = datetime.now().isoformat()\n",
    "            self.session_log[\"total_actions\"] = len(self.session_log[\"actions\"])\n",
    "            self.session_log[\"detected_selectors_count\"] = len(self.detected_selectors)\n",
    "            \n",
    "            with open(filepath, 'w', encoding='utf-8') as f:\n",
    "                json.dump(self.session_log, f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "            self.log_action(\"SESSION_LOG_SAVED\", \"SUCCESS\", filename)\n",
    "            return filename\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.log_action(\"SESSION_LOG_SAVE_ERROR\", \"ERROR\", str(e))\n",
    "            return None\n",
    "    \n",
    "    def scout_site(self, site_name, sample_urls, max_products=5):\n",
    "        \"\"\"Scout complet d'un site avec toutes les s√©curit√©s\"\"\"\n",
    "        self.current_site = site_name\n",
    "        self.log_action(\"SCOUT_START\", \"INFO\", f\"D√©but du scout pour {site_name}\")\n",
    "        \n",
    "        try:\n",
    "            # D√©tection des √©l√©ments\n",
    "            selectors = self.detect_product_elements(sample_urls, max_products)\n",
    "            \n",
    "            if selectors:\n",
    "                # Sauvegarde automatique\n",
    "                self.save_detected_selectors(site_name)\n",
    "                self.log_action(\"SCOUT_SUCCESS\", \"SUCCESS\", f\"Scout {site_name} termin√© avec succ√®s\")\n",
    "                return selectors\n",
    "            else:\n",
    "                self.log_action(\"SCOUT_FAILED\", \"ERROR\", f\"Aucun s√©lecteur d√©tect√© pour {site_name}\")\n",
    "                return {}\n",
    "                \n",
    "        except Exception as e:\n",
    "            self.log_action(\"SCOUT_ERROR\", \"ERROR\", f\"Erreur scout {site_name}: {str(e)}\")\n",
    "            return {}\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Fermeture s√©curis√©e avec log\"\"\"\n",
    "        self.log_action(\"SCOUT_CLOSING\", \"INFO\", \"Fermeture du scout ultra-s√©curis√©\")\n",
    "        \n",
    "        try:\n",
    "            if self.driver:\n",
    "                self.driver.quit()\n",
    "                self.driver = None\n",
    "            \n",
    "            for driver in self.fallback_drivers:\n",
    "                try:\n",
    "                    driver.quit()\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            self.fallback_drivers.clear()\n",
    "            \n",
    "            # Sauvegarder le log automatiquement\n",
    "            self.save_session_log()\n",
    "            \n",
    "            self.log_action(\"SCOUT_CLOSED\", \"SUCCESS\", \"Scout ultra-s√©curis√© ferm√© proprement\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.log_action(\"SCOUT_CLOSE_ERROR\", \"ERROR\", str(e))\n",
    "\n",
    "print(\"‚úÖ Classe UltraSecureProductReviewScout cr√©√©e avec succ√®s!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c5e117",
   "metadata": {},
   "source": [
    "# üß™ Exemples de Test du Workflow Ultra-S√©curis√©\n",
    "\n",
    "Cette section contient des exemples pr√™ts √† l'emploi pour tester le syst√®me complet :\n",
    "- **Test Amazon** : Laptops gaming, reviews d√©taill√©es\n",
    "- **Test eBay** : Appareils √©lectroniques, diff√©rentes cat√©gories  \n",
    "- **Test de robustesse** : Gestion des erreurs, fallbacks\n",
    "- **Validation des s√©lecteurs** : V√©rification de la d√©tection automatique\n",
    "- **Tests de performance** : Temps de r√©ponse, pagination\n",
    "\n",
    "‚ö†Ô∏è **Important** : Ces tests sont √† des fins √©ducatives uniquement. Respectez les conditions d'utilisation des sites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "becaaade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéÆ Exemple de test Amazon - Laptops Gaming\n",
      "==================================================\n",
      "‚úÖ Fonction de test Amazon cr√©√©e. D√©commentez la derni√®re ligne pour l'ex√©cuter.\n"
     ]
    }
   ],
   "source": [
    "# üß™ EXEMPLE 1: TEST AMAZON LAPTOP GAMING AVEC SCOUT ULTRA-S√âCURIS√â\n",
    "print(\"üéÆ Exemple de test Amazon - Laptops Gaming\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "def test_amazon_gaming_laptops_ultra_secure():\n",
    "    \"\"\"\n",
    "    Test complet du workflow ultra-s√©curis√© sur Amazon\n",
    "    Cat√©gorie: Gaming Laptops\n",
    "    \"\"\"\n",
    "    print(f\"[{datetime.now().strftime('%H:%M:%S')}] üöÄ D√©marrage du test Amazon Gaming Laptops\")\n",
    "    \n",
    "    # Configuration du test\n",
    "    test_config = {\n",
    "        \"site\": \"amazon\",\n",
    "        \"category\": \"gaming laptop\",\n",
    "        \"max_products\": 8,\n",
    "        \"reviews_per_rating\": 30,\n",
    "        \"headless\": False,  # Mode visible pour debugging\n",
    "        \"use_ultra_secure\": True\n",
    "    }\n",
    "    \n",
    "    print(\"üìã Configuration du test:\")\n",
    "    for key, value in test_config.items():\n",
    "        print(f\"   {key}: {value}\")\n",
    "    \n",
    "    try:\n",
    "        # Phase 1: Test du scout ultra-s√©curis√©\n",
    "        print(f\"\\n[{datetime.now().strftime('%H:%M:%S')}] üîç Phase 1: Test du Scout Ultra-S√©curis√©\")\n",
    "        \n",
    "        scout = UltraSecureProductReviewScout(headless=test_config[\"headless\"])\n",
    "        \n",
    "        # URLs d'exemple pour Amazon gaming laptops\n",
    "        sample_urls = [\n",
    "            \"https://www.amazon.com/s?k=gaming+laptop&ref=nb_sb_noss\",\n",
    "            \"https://www.amazon.com/s?k=gaming+laptop+rtx&ref=nb_sb_noss_2\",\n",
    "        ]\n",
    "        \n",
    "        detected_selectors = scout.scout_site(\"amazon\", sample_urls, max_products=5)\n",
    "        \n",
    "        if detected_selectors:\n",
    "            print(f\"‚úÖ S√©lecteurs d√©tect√©s avec succ√®s:\")\n",
    "            for elem_type, selector in detected_selectors.items():\n",
    "                print(f\"   {elem_type}: {selector}\")\n",
    "        else:\n",
    "            print(\"‚ùå √âchec de la d√©tection des s√©lecteurs\")\n",
    "            return None\n",
    "        \n",
    "        scout.close()\n",
    "        \n",
    "        # Phase 2: Test du workflow complet\n",
    "        print(f\"\\n[{datetime.now().strftime('%H:%M:%S')}] üîÑ Phase 2: Test du Workflow Complet\")\n",
    "        \n",
    "        # Utilisation du workflow principal avec les s√©lecteurs d√©tect√©s\n",
    "        result = robust_reviews_workflow(\n",
    "            category=test_config[\"category\"],\n",
    "            site=test_config[\"site\"],\n",
    "            max_products=test_config[\"max_products\"],\n",
    "            reviews_per_rating=test_config[\"reviews_per_rating\"],\n",
    "            headless=test_config[\"headless\"]\n",
    "        )\n",
    "        \n",
    "        if result is not None and len(result) > 0:\n",
    "            print(f\"‚úÖ Test termin√© avec succ√®s!\")\n",
    "            print(f\"üìä R√©sultats: {len(result)} reviews r√©cup√©r√©es\")\n",
    "            print(f\"üìà Colonnes: {list(result.columns)}\")\n",
    "            \n",
    "            # Analyse rapide des r√©sultats\n",
    "            if 'rating' in result.columns:\n",
    "                print(f\"‚≠ê Rating moyen: {result['rating'].mean():.2f}\")\n",
    "            if 'review_text' in result.columns:\n",
    "                print(f\"üìù Longueur moyenne des reviews: {result['review_text'].str.len().mean():.0f} caract√®res\")\n",
    "            \n",
    "            return result\n",
    "        else:\n",
    "            print(\"‚ùå Aucune donn√©e r√©cup√©r√©e\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur durant le test: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# ‚ö†Ô∏è D√âCOMMENTEZ POUR EX√âCUTER LE TEST\n",
    "# df_test_amazon = test_amazon_gaming_laptops_ultra_secure()\n",
    "\n",
    "print(\"‚úÖ Fonction de test Amazon cr√©√©e. D√©commentez la derni√®re ligne pour l'ex√©cuter.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d72be8f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîå Exemple de test eBay - √âlectronique\n",
      "==================================================\n",
      "‚úÖ Fonction de test eBay cr√©√©e. D√©commentez la derni√®re ligne pour l'ex√©cuter.\n"
     ]
    }
   ],
   "source": [
    "# üß™ EXEMPLE 2: TEST EBAY √âLECTRONIQUE AVEC ROBUSTESSE\n",
    "print(\"üîå Exemple de test eBay - √âlectronique\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "def test_ebay_electronics_robust():\n",
    "    \"\"\"\n",
    "    Test complet du workflow sur eBay\n",
    "    Cat√©gorie: Smartphones et √©lectronique\n",
    "    \"\"\"\n",
    "    print(f\"[{datetime.now().strftime('%H:%M:%S')}] üöÄ D√©marrage du test eBay √âlectronique\")\n",
    "    \n",
    "    # Configuration du test\n",
    "    test_config = {\n",
    "        \"site\": \"ebay\",\n",
    "        \"category\": \"smartphone iphone\",\n",
    "        \"max_products\": 6,\n",
    "        \"reviews_per_rating\": 20,\n",
    "        \"headless\": True,  # Mode headless pour eBay\n",
    "        \"use_ultra_secure\": True\n",
    "    }\n",
    "    \n",
    "    print(\"üìã Configuration du test:\")\n",
    "    for key, value in test_config.items():\n",
    "        print(f\"   {key}: {value}\")\n",
    "    \n",
    "    try:\n",
    "        # Test du workflow avec gestion d'erreurs renforc√©e\n",
    "        print(f\"\\n[{datetime.now().strftime('%H:%M:%S')}] üîÑ Lancement du workflow eBay\")\n",
    "        \n",
    "        result = robust_reviews_workflow(\n",
    "            category=test_config[\"category\"],\n",
    "            site=test_config[\"site\"],\n",
    "            max_products=test_config[\"max_products\"],\n",
    "            reviews_per_rating=test_config[\"reviews_per_rating\"],\n",
    "            headless=test_config[\"headless\"]\n",
    "        )\n",
    "        \n",
    "        if result is not None and len(result) > 0:\n",
    "            print(f\"‚úÖ Test eBay termin√© avec succ√®s!\")\n",
    "            print(f\"üìä R√©sultats: {len(result)} reviews r√©cup√©r√©es\")\n",
    "            \n",
    "            # Analyse des donn√©es eBay\n",
    "            print(f\"\\nüìà Analyse des r√©sultats eBay:\")\n",
    "            if 'rating' in result.columns:\n",
    "                print(f\"‚≠ê Rating moyen: {result['rating'].mean():.2f}\")\n",
    "                print(f\"üìä Distribution des ratings:\")\n",
    "                rating_dist = result['rating'].value_counts().sort_index()\n",
    "                for rating, count in rating_dist.items():\n",
    "                    print(f\"   {rating} √©toiles: {count} reviews\")\n",
    "            \n",
    "            if 'review_text' in result.columns:\n",
    "                avg_length = result['review_text'].str.len().mean()\n",
    "                print(f\"üìù Longueur moyenne des reviews: {avg_length:.0f} caract√®res\")\n",
    "            \n",
    "            return result\n",
    "        else:\n",
    "            print(\"‚ùå Aucune donn√©e r√©cup√©r√©e sur eBay\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur durant le test eBay: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# ‚ö†Ô∏è D√âCOMMENTEZ POUR EX√âCUTER LE TEST\n",
    "# df_test_ebay = test_ebay_electronics_robust()\n",
    "\n",
    "print(\"‚úÖ Fonction de test eBay cr√©√©e. D√©commentez la derni√®re ligne pour l'ex√©cuter.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e448e39b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üõ°Ô∏è Test de robustesse du syst√®me\n",
      "==================================================\n",
      "[15:17:16] üõ°Ô∏è D√©marrage du test de robustesse\n",
      "\n",
      "[15:17:16] üß™ Test 1: Amazon - Cat√©gorie obscure\n",
      "   üîç Test du scout...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-29 15:17:19,421 - INFO - patching driver executable C:\\Users\\Yann\\appdata\\roaming\\undetected_chromedriver\\undetected_chromedriver.exe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:17:20] DRIVER_CREATION_SUCCESS: SUCCESS - Driver undetected_chrome cr√©√©\n",
      "[15:17:20] DRIVER_INITIALIZED: SUCCESS - Driver undetected_chrome initialis√©\n",
      "[15:17:24] DRIVER_TEST_SUCCESS: SUCCESS - Driver undetected_chrome fonctionnel\n",
      "[15:17:30] SELECTOR_FOUND: SUCCESS - a[href*=\"/dp/\"]: 197 √©l√©ments\n",
      "[15:17:30] SELECTOR_FOUND: SUCCESS - .s-result-item a: 311 √©l√©ments\n",
      "[15:17:30] SELECTOR_FOUND: SUCCESS - [data-component-type=\"s-search-result\"] a: 283 √©l√©ments\n",
      "[15:17:30] PAGE_ANALYSIS_SUCCESS: SUCCESS - Page 1 analys√©e\n",
      "[15:17:30] FALLBACK_TO_BASE_SELECTORS: WARNING - Utilisation des s√©lecteurs de base\n",
      "[15:17:30] DETECTION_COMPLETE: SUCCESS - 5 s√©lecteurs d√©tect√©s\n",
      "[15:17:30] SELECTORS_SAVE_ERROR: ERROR - name 'shutil' is not defined\n",
      "[15:17:30] SCOUT_SUCCESS: SUCCESS - Scout amazon termin√© avec succ√®s\n",
      "   ‚úÖ Scout: S√©lecteurs d√©tect√©s\n",
      "[15:17:30] SESSION_LOG_SAVED: SUCCESS - ultra_scout_session_20250629_151730.json\n",
      "[15:17:30] SCOUT_CLOSED: SUCCESS - Scout ultra-s√©curis√© ferm√© proprement\n",
      "\n",
      "[15:17:32] üß™ Test 2: eBay - Recherche vide\n",
      "   üîç Test du scout...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-29 15:17:35,186 - INFO - patching driver executable C:\\Users\\Yann\\appdata\\roaming\\undetected_chromedriver\\undetected_chromedriver.exe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:17:35] DRIVER_CREATION_SUCCESS: SUCCESS - Driver undetected_chrome cr√©√©\n",
      "[15:17:35] DRIVER_INITIALIZED: SUCCESS - Driver undetected_chrome initialis√©\n",
      "[15:17:39] DRIVER_TEST_SUCCESS: SUCCESS - Driver undetected_chrome fonctionnel\n",
      "[15:17:47] PAGE_ANALYSIS_SUCCESS: SUCCESS - Page 1 analys√©e\n",
      "[15:17:47] BEST_SELECTOR_FOUND: SUCCESS - product_link: h1[class*=\"title\"]\n",
      "[15:17:47] BEST_SELECTOR_FOUND: SUCCESS - product_title: h1[class*=\"title\"]\n",
      "[15:17:47] BEST_SELECTOR_FOUND: SUCCESS - rating: h1[class*=\"title\"]\n",
      "[15:17:47] BEST_SELECTOR_FOUND: SUCCESS - review_count: h1[class*=\"title\"]\n",
      "[15:17:47] BEST_SELECTOR_FOUND: SUCCESS - price: h1[class*=\"title\"]\n",
      "[15:17:47] DETECTION_COMPLETE: SUCCESS - 5 s√©lecteurs d√©tect√©s\n",
      "[15:17:47] SELECTORS_SAVED: SUCCESS - detected_selectors_ebay.json\n",
      "[15:17:47] SCOUT_SUCCESS: SUCCESS - Scout ebay termin√© avec succ√®s\n",
      "   ‚úÖ Scout: S√©lecteurs d√©tect√©s\n",
      "[15:17:47] SESSION_LOG_SAVED: SUCCESS - ultra_scout_session_20250629_151747.json\n",
      "[15:17:47] SCOUT_CLOSED: SUCCESS - Scout ultra-s√©curis√© ferm√© proprement\n",
      "\n",
      "[15:17:49] üìä R√âSUM√â DES TESTS DE ROBUSTESSE:\n",
      "============================================================\n",
      "‚úÖ Amazon - Cat√©gorie obscure: scout_success\n",
      "‚úÖ eBay - Recherche vide: scout_success\n",
      "\n",
      "[15:17:49] üîß Test des fallbacks de drivers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-29 15:17:51,252 - INFO - patching driver executable C:\\Users\\Yann\\appdata\\roaming\\undetected_chromedriver\\undetected_chromedriver.exe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:17:52] DRIVER_CREATION_SUCCESS: SUCCESS - Driver undetected_chrome cr√©√©\n",
      "[15:17:52] DRIVER_INITIALIZED: SUCCESS - Driver undetected_chrome initialis√©\n",
      "[15:17:56] DRIVER_TEST_SUCCESS: SUCCESS - Driver undetected_chrome fonctionnel\n",
      "‚úÖ Au moins un driver fonctionne\n",
      "   Driver utilis√©: Chrome\n",
      "[15:17:56] SESSION_LOG_SAVED: SUCCESS - ultra_scout_session_20250629_151756.json\n",
      "[15:17:56] SCOUT_CLOSED: SUCCESS - Scout ultra-s√©curis√© ferm√© proprement\n",
      "‚úÖ Fonction de test de robustesse cr√©√©e. D√©commentez la derni√®re ligne pour l'ex√©cuter.\n"
     ]
    }
   ],
   "source": [
    "# üß™ EXEMPLE 3: TEST DE ROBUSTESSE ET GESTION D'ERREURS\n",
    "print(\"üõ°Ô∏è Test de robustesse du syst√®me\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "def test_system_robustness():\n",
    "    \"\"\"\n",
    "    Test de robustesse du syst√®me complet\n",
    "    Validation des fallbacks et de la gestion d'erreurs\n",
    "    \"\"\"\n",
    "    print(f\"[{datetime.now().strftime('%H:%M:%S')}] üõ°Ô∏è D√©marrage du test de robustesse\")\n",
    "    \n",
    "    # Tests avec diff√©rents sc√©narios difficiles\n",
    "    test_scenarios = [\n",
    "        {\n",
    "            \"name\": \"Amazon - Cat√©gorie obscure\",\n",
    "            \"site\": \"amazon\",\n",
    "            \"category\": \"xyz rare product\",\n",
    "            \"max_products\": 3,\n",
    "            \"reviews_per_rating\": 10,\n",
    "            \"expected_result\": \"fallback_selectors\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"eBay - Recherche vide\",\n",
    "            \"site\": \"ebay\",\n",
    "            \"category\": \"nonexistent product xyz\",\n",
    "            \"max_products\": 2,\n",
    "            \"reviews_per_rating\": 5,\n",
    "            \"expected_result\": \"no_products\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for i, scenario in enumerate(test_scenarios):\n",
    "        print(f\"\\n[{datetime.now().strftime('%H:%M:%S')}] üß™ Test {i+1}: {scenario['name']}\")\n",
    "        \n",
    "        try:\n",
    "            # Test du scout ultra-s√©curis√©\n",
    "            print(\"   üîç Test du scout...\")\n",
    "            scout = UltraSecureProductReviewScout(headless=True)\n",
    "            \n",
    "            # URLs de test (peuvent √©chouer volontairement)\n",
    "            sample_urls = [\n",
    "                f\"https://www.{scenario['site']}.com/s?k={scenario['category'].replace(' ', '+')}\"\n",
    "            ]\n",
    "            \n",
    "            detected_selectors = scout.scout_site(scenario['site'], sample_urls, max_products=2)\n",
    "            \n",
    "            if detected_selectors:\n",
    "                print(\"   ‚úÖ Scout: S√©lecteurs d√©tect√©s\")\n",
    "                results[scenario['name']] = \"scout_success\"\n",
    "            else:\n",
    "                print(\"   ‚ö†Ô∏è Scout: Fallback utilis√©\")\n",
    "                results[scenario['name']] = \"scout_fallback\"\n",
    "            \n",
    "            scout.close()\n",
    "            \n",
    "            # Petit d√©lai entre les tests\n",
    "            time.sleep(2)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Scout: Erreur - {str(e)[:100]}\")\n",
    "            results[scenario['name']] = f\"scout_error: {str(e)[:50]}\"\n",
    "    \n",
    "    # R√©sum√© des tests de robustesse\n",
    "    print(f\"\\n[{datetime.now().strftime('%H:%M:%S')}] üìä R√âSUM√â DES TESTS DE ROBUSTESSE:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for scenario_name, result in results.items():\n",
    "        status = \"‚úÖ\" if \"success\" in result or \"fallback\" in result else \"‚ùå\"\n",
    "        print(f\"{status} {scenario_name}: {result}\")\n",
    "    \n",
    "    # Test des drivers en fallback\n",
    "    print(f\"\\n[{datetime.now().strftime('%H:%M:%S')}] üîß Test des fallbacks de drivers:\")\n",
    "    \n",
    "    scout = UltraSecureProductReviewScout(headless=True)\n",
    "    driver_success = scout.initialize_driver()\n",
    "    \n",
    "    if driver_success:\n",
    "        print(\"‚úÖ Au moins un driver fonctionne\")\n",
    "        print(f\"   Driver utilis√©: {type(scout.driver).__name__}\")\n",
    "    else:\n",
    "        print(\"‚ùå Aucun driver disponible\")\n",
    "    \n",
    "    scout.close()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# ‚ö†Ô∏è D√âCOMMENTEZ POUR EX√âCUTER LE TEST DE ROBUSTESSE\n",
    "test_results = test_system_robustness()\n",
    "\n",
    "print(\"‚úÖ Fonction de test de robustesse cr√©√©e. D√©commentez la derni√®re ligne pour l'ex√©cuter.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a04779",
   "metadata": {},
   "source": [
    "# run on extra robust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "661717ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Menu Interactif du Syst√®me Ultra-S√©curis√©\n",
      "============================================================\n",
      "\n",
      "[15:18:11] üéÆ MENU PRINCIPAL - SYST√àME ULTRA-S√âCURIS√â\n",
      "============================================================\n",
      "1Ô∏è‚É£ Workflow complet (scout + scraper)\n",
      "2Ô∏è‚É£ Test scout ultra-s√©curis√© uniquement\n",
      "3Ô∏è‚É£ Exemples de test pr√™ts √† l'emploi\n",
      "4Ô∏è‚É£ Test de robustesse du syst√®me\n",
      "5Ô∏è‚É£ Configuration avanc√©e\n",
      "6Ô∏è‚É£ Voir les logs et sauvegardes\n",
      "7Ô∏è‚É£ Documentation et aide\n",
      "0Ô∏è‚É£ Quitter\n",
      "------------------------------------------------------------\n",
      "\n",
      "üìã CONFIGURATION DU WORKFLOW COMPLET\n",
      "----------------------------------------\n",
      "\n",
      "üöÄ Lancement du workflow...\n",
      "   Site: amazon\n",
      "   Cat√©gorie: laptop\n",
      "   Max produits: 3\n",
      "   Reviews par rating: 15\n",
      "   Mode headless: True\n",
      "====================================================================================================\n",
      "üöÄ WORKFLOW ROBUSTE - SCRAPING REVIEWS DE PRODUITS\n",
      "====================================================================================================\n",
      "üì¶ Cat√©gorie: laptop\n",
      "üåê Site: amazon\n",
      "üìä Produits max: 3\n",
      "‚≠ê Reviews par note: 15\n",
      "üëÅÔ∏è Mode: Headless\n",
      "üìà Total estim√©: 225 reviews max\n",
      "\n",
      "üîç PHASE 1: D√âTECTION AUTOMATIQUE DES BALISES\n",
      "----------------------------------------------------------------------\n",
      "üîß Tentative 1/3 - Setup driver scout...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-29 15:18:36,070 - INFO - patching driver executable C:\\Users\\Yann\\appdata\\roaming\\undetected_chromedriver\\undetected_chromedriver.exe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Driver scout initialis√© avec succ√®s!\n",
      "üîç D√©tection des s√©lecteurs pour amazon...\n",
      "üåê Navigation vers: https://www.amazon.com/s?k=laptop\n",
      "‚úÖ Conteneur: [data-component-type=\"s-search-result\"] (21 √©l√©ments)\n",
      "‚úÖ Titre: h2 span\n",
      "‚úÖ URL: .a-link-normal\n",
      "‚úÖ Rating: .a-icon-alt\n",
      "‚úÖ S√©lecteurs produits d√©tect√©s: 4\n",
      "üîó Test reviews sur: https://www.amazon.com/HP-Micro-edge-Microsoft-14-dq0040nr-Snowflake/dp/B0947BJ6...\n",
      "üîó Navigation vers page reviews: https://www.amazon.com/HP-Micro-edge-Microsoft-14-dq0040nr-Snowflake/dp/B0947BJ6...\n",
      "‚úÖ Conteneur reviews: [data-hook=\"review\"] (13 reviews)\n",
      "‚úÖ Texte review: [data-hook=\"review-body\"] span\n",
      "‚úÖ Titre review: .review-title\n",
      "‚úÖ Rating review: [data-hook=\"review-star-rating\"] .a-icon-alt\n",
      "‚úÖ Auteur review: .a-profile-name\n",
      "‚úÖ Date review: [data-hook=\"review-date\"]\n",
      "‚úÖ S√©lecteurs reviews d√©tect√©s: 6\n",
      "‚úÖ S√©lecteurs sauvegard√©s: ../config/detected_selectors_amazon.json\n",
      "‚úÖ S√©lecteurs d√©tect√©s avec succ√®s!\n",
      "üì¶ Produits: ['container', 'title', 'url', 'rating']\n",
      "üìù Reviews: ['container', 'text', 'title', 'rating', 'author', 'date']\n",
      "‚úÖ Driver scout ferm√©\n",
      "\n",
      "======================================================================\n",
      "üìä PHASE 2: SCRAPING DES REVIEWS\n",
      "----------------------------------------------------------------------\n",
      "üöÄ Tentative 1/3 - Setup driver scraper...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-29 15:19:20,825 - INFO - patching driver executable C:\\Users\\Yann\\appdata\\roaming\\undetected_chromedriver\\undetected_chromedriver.exe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Driver scraper pr√™t!\n",
      "üé≠ User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, lik...\n",
      "\n",
      "üéØ D√©but du scraping pour 'laptop' sur amazon...\n",
      "================================================================================\n",
      "üéØ D√âBUT DU SCRAPING ROBUSTE\n",
      "================================================================================\n",
      "üì¶ Cat√©gorie: laptop\n",
      "üåê Site: amazon\n",
      "üìä Produits max: 3\n",
      "‚≠ê Reviews par note: 15\n",
      "\n",
      "üîç Recherche: https://www.amazon.com/s?k=laptop\n",
      "üì¶ 0 conteneurs trouv√©s\n",
      "‚ùå Aucun produit trouv√©\n",
      "‚ùå Aucune review r√©cup√©r√©e\n",
      "‚úÖ Driver scraper ferm√©\n",
      "‚ùå Aucune donn√©e r√©cup√©r√©e\n",
      "\n",
      "[15:19:43] üéÆ MENU PRINCIPAL - SYST√àME ULTRA-S√âCURIS√â\n",
      "============================================================\n",
      "1Ô∏è‚É£ Workflow complet (scout + scraper)\n",
      "2Ô∏è‚É£ Test scout ultra-s√©curis√© uniquement\n",
      "3Ô∏è‚É£ Exemples de test pr√™ts √† l'emploi\n",
      "4Ô∏è‚É£ Test de robustesse du syst√®me\n",
      "5Ô∏è‚É£ Configuration avanc√©e\n",
      "6Ô∏è‚É£ Voir les logs et sauvegardes\n",
      "7Ô∏è‚É£ Documentation et aide\n",
      "0Ô∏è‚É£ Quitter\n",
      "------------------------------------------------------------\n",
      "üì¶ 0 conteneurs trouv√©s\n",
      "‚ùå Aucun produit trouv√©\n",
      "‚ùå Aucune review r√©cup√©r√©e\n",
      "‚úÖ Driver scraper ferm√©\n",
      "‚ùå Aucune donn√©e r√©cup√©r√©e\n",
      "\n",
      "[15:19:43] üéÆ MENU PRINCIPAL - SYST√àME ULTRA-S√âCURIS√â\n",
      "============================================================\n",
      "1Ô∏è‚É£ Workflow complet (scout + scraper)\n",
      "2Ô∏è‚É£ Test scout ultra-s√©curis√© uniquement\n",
      "3Ô∏è‚É£ Exemples de test pr√™ts √† l'emploi\n",
      "4Ô∏è‚É£ Test de robustesse du syst√®me\n",
      "5Ô∏è‚É£ Configuration avanc√©e\n",
      "6Ô∏è‚É£ Voir les logs et sauvegardes\n",
      "7Ô∏è‚É£ Documentation et aide\n",
      "0Ô∏è‚É£ Quitter\n",
      "------------------------------------------------------------\n",
      "‚ùå Choix invalide. Veuillez choisir entre 0 et 7.\n",
      "\n",
      "[15:19:51] üéÆ MENU PRINCIPAL - SYST√àME ULTRA-S√âCURIS√â\n",
      "============================================================\n",
      "1Ô∏è‚É£ Workflow complet (scout + scraper)\n",
      "2Ô∏è‚É£ Test scout ultra-s√©curis√© uniquement\n",
      "3Ô∏è‚É£ Exemples de test pr√™ts √† l'emploi\n",
      "4Ô∏è‚É£ Test de robustesse du syst√®me\n",
      "5Ô∏è‚É£ Configuration avanc√©e\n",
      "6Ô∏è‚É£ Voir les logs et sauvegardes\n",
      "7Ô∏è‚É£ Documentation et aide\n",
      "0Ô∏è‚É£ Quitter\n",
      "------------------------------------------------------------\n",
      "‚ùå Choix invalide. Veuillez choisir entre 0 et 7.\n",
      "\n",
      "[15:19:51] üéÆ MENU PRINCIPAL - SYST√àME ULTRA-S√âCURIS√â\n",
      "============================================================\n",
      "1Ô∏è‚É£ Workflow complet (scout + scraper)\n",
      "2Ô∏è‚É£ Test scout ultra-s√©curis√© uniquement\n",
      "3Ô∏è‚É£ Exemples de test pr√™ts √† l'emploi\n",
      "4Ô∏è‚É£ Test de robustesse du syst√®me\n",
      "5Ô∏è‚É£ Configuration avanc√©e\n",
      "6Ô∏è‚É£ Voir les logs et sauvegardes\n",
      "7Ô∏è‚É£ Documentation et aide\n",
      "0Ô∏è‚É£ Quitter\n",
      "------------------------------------------------------------\n",
      "‚ùå Choix invalide. Veuillez choisir entre 0 et 7.\n",
      "\n",
      "[15:19:53] üéÆ MENU PRINCIPAL - SYST√àME ULTRA-S√âCURIS√â\n",
      "============================================================\n",
      "1Ô∏è‚É£ Workflow complet (scout + scraper)\n",
      "2Ô∏è‚É£ Test scout ultra-s√©curis√© uniquement\n",
      "3Ô∏è‚É£ Exemples de test pr√™ts √† l'emploi\n",
      "4Ô∏è‚É£ Test de robustesse du syst√®me\n",
      "5Ô∏è‚É£ Configuration avanc√©e\n",
      "6Ô∏è‚É£ Voir les logs et sauvegardes\n",
      "7Ô∏è‚É£ Documentation et aide\n",
      "0Ô∏è‚É£ Quitter\n",
      "------------------------------------------------------------\n",
      "‚ùå Choix invalide. Veuillez choisir entre 0 et 7.\n",
      "\n",
      "[15:19:53] üéÆ MENU PRINCIPAL - SYST√àME ULTRA-S√âCURIS√â\n",
      "============================================================\n",
      "1Ô∏è‚É£ Workflow complet (scout + scraper)\n",
      "2Ô∏è‚É£ Test scout ultra-s√©curis√© uniquement\n",
      "3Ô∏è‚É£ Exemples de test pr√™ts √† l'emploi\n",
      "4Ô∏è‚É£ Test de robustesse du syst√®me\n",
      "5Ô∏è‚É£ Configuration avanc√©e\n",
      "6Ô∏è‚É£ Voir les logs et sauvegardes\n",
      "7Ô∏è‚É£ Documentation et aide\n",
      "0Ô∏è‚É£ Quitter\n",
      "------------------------------------------------------------\n",
      "‚ùå Choix invalide. Veuillez choisir entre 0 et 7.\n",
      "\n",
      "[15:19:56] üéÆ MENU PRINCIPAL - SYST√àME ULTRA-S√âCURIS√â\n",
      "============================================================\n",
      "1Ô∏è‚É£ Workflow complet (scout + scraper)\n",
      "2Ô∏è‚É£ Test scout ultra-s√©curis√© uniquement\n",
      "3Ô∏è‚É£ Exemples de test pr√™ts √† l'emploi\n",
      "4Ô∏è‚É£ Test de robustesse du syst√®me\n",
      "5Ô∏è‚É£ Configuration avanc√©e\n",
      "6Ô∏è‚É£ Voir les logs et sauvegardes\n",
      "7Ô∏è‚É£ Documentation et aide\n",
      "0Ô∏è‚É£ Quitter\n",
      "------------------------------------------------------------\n",
      "üëã Au revoir!\n",
      "‚úÖ Menu interactif am√©lior√© cr√©√©. D√©commentez la derni√®re ligne pour le lancer.\n",
      "‚ùå Choix invalide. Veuillez choisir entre 0 et 7.\n",
      "\n",
      "[15:19:56] üéÆ MENU PRINCIPAL - SYST√àME ULTRA-S√âCURIS√â\n",
      "============================================================\n",
      "1Ô∏è‚É£ Workflow complet (scout + scraper)\n",
      "2Ô∏è‚É£ Test scout ultra-s√©curis√© uniquement\n",
      "3Ô∏è‚É£ Exemples de test pr√™ts √† l'emploi\n",
      "4Ô∏è‚É£ Test de robustesse du syst√®me\n",
      "5Ô∏è‚É£ Configuration avanc√©e\n",
      "6Ô∏è‚É£ Voir les logs et sauvegardes\n",
      "7Ô∏è‚É£ Documentation et aide\n",
      "0Ô∏è‚É£ Quitter\n",
      "------------------------------------------------------------\n",
      "üëã Au revoir!\n",
      "‚úÖ Menu interactif am√©lior√© cr√©√©. D√©commentez la derni√®re ligne pour le lancer.\n"
     ]
    }
   ],
   "source": [
    "# üéØ MENU INTERACTIF AM√âLIOR√â - VERSION ULTRA-S√âCURIS√âE\n",
    "print(\"üéØ Menu Interactif du Syst√®me Ultra-S√©curis√©\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def enhanced_interactive_menu():\n",
    "    \"\"\"\n",
    "    Menu interactif am√©lior√© avec tous les outils disponibles\n",
    "    Version 2025-01-20 avec scout ultra-s√©curis√©\n",
    "    \"\"\"\n",
    "    \n",
    "    while True:\n",
    "        print(f\"\\n[{datetime.now().strftime('%H:%M:%S')}] üéÆ MENU PRINCIPAL - SYST√àME ULTRA-S√âCURIS√â\")\n",
    "        print(\"=\" * 60)\n",
    "        print(\"1Ô∏è‚É£ Workflow complet (scout + scraper)\")\n",
    "        print(\"2Ô∏è‚É£ Test scout ultra-s√©curis√© uniquement\")\n",
    "        print(\"3Ô∏è‚É£ Exemples de test pr√™ts √† l'emploi\")\n",
    "        print(\"4Ô∏è‚É£ Test de robustesse du syst√®me\")\n",
    "        print(\"5Ô∏è‚É£ Configuration avanc√©e\")\n",
    "        print(\"6Ô∏è‚É£ Voir les logs et sauvegardes\")\n",
    "        print(\"7Ô∏è‚É£ Documentation et aide\")\n",
    "        print(\"0Ô∏è‚É£ Quitter\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        try:\n",
    "            choice = input(\"üîπ Votre choix (0-7): \").strip()\n",
    "            \n",
    "            if choice == \"0\":\n",
    "                print(\"üëã Au revoir!\")\n",
    "                break\n",
    "                \n",
    "            elif choice == \"1\":\n",
    "                print(\"\\nüìã CONFIGURATION DU WORKFLOW COMPLET\")\n",
    "                print(\"-\" * 40)\n",
    "                \n",
    "                # Configuration interactive\n",
    "                site = input(\"üåê Site (amazon/ebay) [amazon]: \").strip() or \"amazon\"\n",
    "                category = input(\"üì¶ Cat√©gorie/recherche [laptop]: \").strip() or \"laptop\"\n",
    "                max_products = int(input(\"üî¢ Max produits [5]: \").strip() or \"5\")\n",
    "                reviews_per_rating = int(input(\"‚≠ê Reviews par rating [20]: \").strip() or \"20\")\n",
    "                headless = input(\"üëÅÔ∏è Mode headless? (y/n) [y]: \").strip().lower() != \"n\"\n",
    "                \n",
    "                print(f\"\\nüöÄ Lancement du workflow...\")\n",
    "                print(f\"   Site: {site}\")\n",
    "                print(f\"   Cat√©gorie: {category}\")\n",
    "                print(f\"   Max produits: {max_products}\")\n",
    "                print(f\"   Reviews par rating: {reviews_per_rating}\")\n",
    "                print(f\"   Mode headless: {headless}\")\n",
    "                \n",
    "                result = robust_reviews_workflow(category, site, max_products, reviews_per_rating, headless)\n",
    "                \n",
    "                if result is not None and len(result) > 0:\n",
    "                    print(f\"‚úÖ Workflow termin√©! {len(result)} reviews r√©cup√©r√©es\")\n",
    "                    \n",
    "                    # Option de sauvegarde\n",
    "                    save_choice = input(\"\\nüíæ Sauvegarder dans une variable? (y/n): \").strip().lower()\n",
    "                    if save_choice == \"y\":\n",
    "                        var_name = input(\"üìù Nom de la variable [df_results]: \").strip() or \"df_results\"\n",
    "                        globals()[var_name] = result\n",
    "                        print(f\"‚úÖ Donn√©es sauvegard√©es dans '{var_name}'\")\n",
    "                else:\n",
    "                    print(\"‚ùå Aucune donn√©e r√©cup√©r√©e\")\n",
    "                    \n",
    "            elif choice == \"2\":\n",
    "                print(\"\\nüîç TEST SCOUT ULTRA-S√âCURIS√â\")\n",
    "                print(\"-\" * 40)\n",
    "                \n",
    "                site = input(\"üåê Site √† analyser (amazon/ebay) [amazon]: \").strip() or \"amazon\"\n",
    "                search_term = input(\"üîç Terme de recherche [laptop]: \").strip() or \"laptop\"\n",
    "                headless = input(\"üëÅÔ∏è Mode headless? (y/n) [y]: \").strip().lower() != \"n\"\n",
    "                \n",
    "                scout = UltraSecureProductReviewScout(headless=headless)\n",
    "                \n",
    "                sample_urls = [\n",
    "                    f\"https://www.{site}.com/s?k={search_term.replace(' ', '+')}\"\n",
    "                ]\n",
    "                \n",
    "                print(f\"üîç Analyse de {site} pour '{search_term}'...\")\n",
    "                selectors = scout.scout_site(site, sample_urls)\n",
    "                \n",
    "                if selectors:\n",
    "                    print(\"‚úÖ S√©lecteurs d√©tect√©s:\")\n",
    "                    for elem_type, selector in selectors.items():\n",
    "                        print(f\"   {elem_type}: {selector}\")\n",
    "                else:\n",
    "                    print(\"‚ùå Aucun s√©lecteur d√©tect√©\")\n",
    "                \n",
    "                scout.close()\n",
    "                \n",
    "            elif choice == \"3\":\n",
    "                print(\"\\nüß™ EXEMPLES DE TEST PR√äTS √Ä L'EMPLOI\")\n",
    "                print(\"-\" * 40)\n",
    "                print(\"1. Test Amazon Gaming Laptops\")\n",
    "                print(\"2. Test eBay √âlectronique\")\n",
    "                print(\"3. Retour au menu principal\")\n",
    "                \n",
    "                test_choice = input(\"üîπ Votre choix (1-3): \").strip()\n",
    "                \n",
    "                if test_choice == \"1\":\n",
    "                    print(\"üéÆ Lancement du test Amazon Gaming Laptops...\")\n",
    "                    result = test_amazon_gaming_laptops_ultra_secure()\n",
    "                    if result is not None:\n",
    "                        globals()['df_test_amazon'] = result\n",
    "                        print(\"‚úÖ R√©sultats sauvegard√©s dans 'df_test_amazon'\")\n",
    "                        \n",
    "                elif test_choice == \"2\":\n",
    "                    print(\"üîå Lancement du test eBay √âlectronique...\")\n",
    "                    result = test_ebay_electronics_robust()\n",
    "                    if result is not None:\n",
    "                        globals()['df_test_ebay'] = result\n",
    "                        print(\"‚úÖ R√©sultats sauvegard√©s dans 'df_test_ebay'\")\n",
    "                        \n",
    "            elif choice == \"4\":\n",
    "                print(\"\\nüõ°Ô∏è TEST DE ROBUSTESSE DU SYST√àME\")\n",
    "                print(\"-\" * 40)\n",
    "                print(\"Ce test valide les fallbacks et la gestion d'erreurs...\")\n",
    "                \n",
    "                confirm = input(\"üîπ Continuer? (y/n): \").strip().lower()\n",
    "                if confirm == \"y\":\n",
    "                    test_results = test_system_robustness()\n",
    "                    globals()['robustness_results'] = test_results\n",
    "                    print(\"‚úÖ R√©sultats sauvegard√©s dans 'robustness_results'\")\n",
    "                    \n",
    "            elif choice == \"5\":\n",
    "                print(\"\\n‚öôÔ∏è CONFIGURATION AVANC√âE\")\n",
    "                print(\"-\" * 40)\n",
    "                print(\"üìã Configurations disponibles:\")\n",
    "                print(f\"   Proxies configur√©s: {len(PROXY_LIST)}\")\n",
    "                print(f\"   User agents disponibles: {len(REALISTIC_USER_AGENTS)}\")\n",
    "                print(\"   Exemples pr√©d√©finis: Amazon, eBay, Trustpilot\")\n",
    "                print(\"\\nüîß Pour modifier les configurations, √©ditez les variables:\")\n",
    "                print(\"   - PROXY_LIST (pour les proxies)\")\n",
    "                print(\"   - REALISTIC_USER_AGENTS (pour les user agents)\")\n",
    "                print(\"   - ROBUST_EXAMPLES (pour les exemples)\")\n",
    "                \n",
    "            elif choice == \"6\":\n",
    "                print(\"\\nüìÅ LOGS ET SAUVEGARDES\")\n",
    "                print(\"-\" * 40)\n",
    "                \n",
    "                # V√©rifier les fichiers de sauvegarde\n",
    "                config_dir = \"../config\"\n",
    "                logs_dir = \"../logs\"\n",
    "                data_dir = \"../data/raw\"\n",
    "                \n",
    "                for directory, desc in [(config_dir, \"S√©lecteurs d√©tect√©s\"), \n",
    "                                      (logs_dir, \"Logs de session\"), \n",
    "                                      (data_dir, \"Donn√©es scrap√©es\")]:\n",
    "                    if os.path.exists(directory):\n",
    "                        files = [f for f in os.listdir(directory) if f.endswith(('.json', '.csv'))]\n",
    "                        print(f\"\\nüìÇ {desc} ({directory}):\")\n",
    "                        if files:\n",
    "                            for file in sorted(files)[-5:]:  # 5 derniers fichiers\n",
    "                                print(f\"   - {file}\")\n",
    "                            if len(files) > 5:\n",
    "                                print(f\"   ... et {len(files)-5} autres fichiers\")\n",
    "                        else:\n",
    "                            print(\"   (aucun fichier)\")\n",
    "                    else:\n",
    "                        print(f\"\\nüìÇ {desc}: Dossier non cr√©√©\")\n",
    "                        \n",
    "            elif choice == \"7\":\n",
    "                print(\"\\nüìñ DOCUMENTATION ET AIDE\")\n",
    "                print(\"-\" * 40)\n",
    "                print(\"üîó Fonctions principales disponibles:\")\n",
    "                print(\"   ‚Ä¢ robust_reviews_workflow() - Workflow complet\")\n",
    "                print(\"   ‚Ä¢ UltraSecureProductReviewScout - Scout ultra-s√©curis√©\")\n",
    "                print(\"   ‚Ä¢ RobustProductReviewScraper - Scraper robuste\")\n",
    "                print(\"\\nüìã Variables globales importantes:\")\n",
    "                print(\"   ‚Ä¢ ROBUST_EXAMPLES - Exemples pr√©d√©finis\")\n",
    "                print(\"   ‚Ä¢ PROXY_LIST - Liste des proxies\")\n",
    "                print(\"   ‚Ä¢ REALISTIC_USER_AGENTS - User agents\")\n",
    "                print(\"\\nüß™ Fonctions de test:\")\n",
    "                print(\"   ‚Ä¢ test_amazon_gaming_laptops_ultra_secure()\")\n",
    "                print(\"   ‚Ä¢ test_ebay_electronics_robust()\")\n",
    "                print(\"   ‚Ä¢ test_system_robustness()\")\n",
    "                print(\"\\nüí° Conseils:\")\n",
    "                print(\"   - Utilisez le mode non-headless pour debugging\")\n",
    "                print(\"   - Les s√©lecteurs sont sauvegard√©s automatiquement\")\n",
    "                print(\"   - Les logs sont horodat√©s pour le suivi\")\n",
    "                \n",
    "            else:\n",
    "                print(\"‚ùå Choix invalide. Veuillez choisir entre 0 et 7.\")\n",
    "                \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n\\nüõë Interruption utilisateur. Au revoir!\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur: {str(e)}\")\n",
    "            print(\"üîÑ Retour au menu principal...\")\n",
    "\n",
    "# ‚ö†Ô∏è D√âCOMMENTEZ POUR LANCER LE MENU INTERACTIF\n",
    "enhanced_interactive_menu()\n",
    "\n",
    "print(\"‚úÖ Menu interactif am√©lior√© cr√©√©. D√©commentez la derni√®re ligne pour le lancer.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61eece5b",
   "metadata": {},
   "source": [
    "# üéØ R√©sum√© Final - Syst√®me Ultra-S√©curis√© Pr√™t (2025-01-20)\n",
    "\n",
    "## ‚úÖ **SYST√àME COMPLET IMPL√âMENT√â**\n",
    "\n",
    "### üîí **Scout Ultra-S√©curis√©**\n",
    "- **UltraSecureProductReviewScout** : D√©tection automatique avec fallbacks multiples\n",
    "- **Drivers** : undetected_chromedriver ‚Üí Selenium Chrome ‚Üí Firefox ‚Üí Edge\n",
    "- **Anti-d√©tection** : Options Chrome optimis√©es, rotation user-agents, scripts anti-d√©tection\n",
    "- **Logs** : Horodatage complet, sauvegarde automatique des sessions\n",
    "- **S√©lecteurs** : D√©tection adaptative avec scoring + fallback sur s√©lecteurs de base\n",
    "\n",
    "### ü§ñ **Scraper Robuste**\n",
    "- **RobustProductReviewScraper** : Navigation s√©curis√©e, pagination, extraction compl√®te\n",
    "- **Donn√©es** : Texte, note, date, auteur, titre produit, prix, etc.\n",
    "- **Gestion d'erreurs** : Retry automatique, fallbacks, validation des donn√©es\n",
    "- **Sauvegarde** : CSV + JSON automatique avec timestamp\n",
    "\n",
    "### üéÆ **Workflow Principal**\n",
    "- **robust_reviews_workflow()** : Pipeline complet scout ‚Üí scraper ‚Üí analyse\n",
    "- **Menu interactif** : Interface utilisateur avec toutes les options\n",
    "- **Exemples pr√™ts** : Amazon gaming laptops, eBay √©lectronique\n",
    "- **Tests robustesse** : Validation fallbacks et gestion d'erreurs\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ **COMMENT UTILISER LE SYST√àME**\n",
    "\n",
    "### **Option 1 : Menu Interactif** (Recommand√©)\n",
    "```python\n",
    "# D√©commentez dans la cellule pr√©c√©dente :\n",
    "enhanced_interactive_menu()\n",
    "```\n",
    "\n",
    "### **Option 2 : Workflow Direct**\n",
    "```python\n",
    "# Exemple rapide Amazon\n",
    "df = robust_reviews_workflow('gaming laptop', 'amazon', 5, 30, False)\n",
    "```\n",
    "\n",
    "### **Option 3 : Tests Sp√©cifiques**\n",
    "```python\n",
    "# Test Amazon gaming laptops\n",
    "df_amazon = test_amazon_gaming_laptops_ultra_secure()\n",
    "\n",
    "# Test eBay √©lectronique  \n",
    "df_ebay = test_ebay_electronics_robust()\n",
    "\n",
    "# Test de robustesse\n",
    "results = test_system_robustness()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üìÅ **FICHIERS G√âN√âR√âS AUTOMATIQUEMENT**\n",
    "\n",
    "- **`../config/detected_selectors_{site}.json`** : S√©lecteurs d√©tect√©s\n",
    "- **`../config/detected_selectors_{site}_backup_{timestamp}.json`** : Backups\n",
    "- **`../logs/ultra_scout_session_{timestamp}.json`** : Logs de session\n",
    "- **`../data/raw/{site}_{category}_{timestamp}.csv`** : Donn√©es scrap√©es\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö†Ô∏è **IMPORTANT - UTILISATION RESPONSABLE**\n",
    "\n",
    "- **√âducatif uniquement** : Ce syst√®me est √† des fins d'apprentissage\n",
    "- **Respect des ToS** : V√©rifiez les conditions d'utilisation des sites\n",
    "- **Rate limiting** : D√©lais automatiques pour √©viter la surcharge\n",
    "- **Proxies** : Configurez PROXY_LIST si n√©cessaire pour la production\n",
    "\n",
    "---\n",
    "\n",
    "## üîÑ **PROCHAINES √âTAPES POSSIBLES**\n",
    "\n",
    "1. **Tester sur cas r√©els** : Lancer les exemples Amazon/eBay\n",
    "2. **Valider robustesse** : Ex√©cuter les tests de fallback\n",
    "3. **Personnaliser** : Adapter les s√©lecteurs pour d'autres sites\n",
    "4. **Am√©liorer** : Ajouter Playwright si besoin, proxies rotation\n",
    "5. **Automatiser** : Scheduler des collectes r√©guli√®res\n",
    "\n",
    "Le syst√®me est maintenant **ultra-robuste** et **pr√™t √† l'emploi** ! üéâ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scraper_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
