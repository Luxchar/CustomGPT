import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, WeightedRandomSampler
import gc
import time
from tqdm.auto import tqdm

from transformers import (
    AutoTokenizer, AutoModelForSequenceClassification,
    TrainingArguments, Trainer, EarlyStoppingCallback,
    get_cosine_schedule_with_warmup
)
from datasets import Dataset, DatasetDict
from sklearn.metrics import f1_score, hamming_loss, classification_report
from sklearn.utils.class_weight import compute_class_weight
from collections import Counter
import warnings
warnings.filterwarnings('ignore')

# Configuration GPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"üöÄ Device: {device}")

# ========================================
# FOCAL LOSS ET LABEL SMOOTHING
# ========================================

class FocalLoss(nn.Module):
    """Focal Loss pour g√©rer le d√©s√©quilibre de classes"""
    def __init__(self, alpha=1.0, gamma=2.0, reduction='mean'):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction
    
    def forward(self, inputs, targets):
        bce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')
        pt = torch.exp(-bce_loss)
        focal_weight = self.alpha * (1 - pt) ** self.gamma
        focal_loss = focal_weight * bce_loss
        
        if self.reduction == 'mean':
            return focal_loss.mean()
        elif self.reduction == 'sum':
            return focal_loss.sum()
        else:
            return focal_loss

class LabelSmoothingLoss(nn.Module):
    """Label Smoothing pour la r√©gularisation"""
    def __init__(self, smoothing=0.1):
        super().__init__()
        self.smoothing = smoothing
    
    def forward(self, logits, targets):
        targets_smooth = targets * (1 - self.smoothing) + self.smoothing / 2
        return F.binary_cross_entropy_with_logits(logits, targets_smooth)

# ========================================
# MOD√àLE ROBERTA ULTIMATE OPTIMIS√â M√âMOIRE
# ========================================

class MemoryOptimizedRoBERTaForMultiLabel(nn.Module):
    """Mod√®le RoBERTa optimis√© pour √©conomiser la m√©moire"""
    def __init__(self, num_labels, dropout_rate=0.2, use_pooling=True):
        super().__init__()
        
        # RoBERTa-Base (stable et performant)
        self.roberta = AutoModelForSequenceClassification.from_pretrained(
            "roberta-base",
            num_labels=num_labels,
            problem_type="multi_label_classification",
            output_attentions=False,  # üî• D√âSACTIV√â pour √©conomiser la m√©moire
            output_hidden_states=False  # üî• D√âSACTIV√â pour √©conomiser la m√©moire
        )
        
        # Configuration
        hidden_size = self.roberta.config.hidden_size  # 768 pour roberta-base
        self.num_labels = num_labels
        self.use_pooling = use_pooling
        
        # Couches de classification simplifi√©es pour √©conomiser la m√©moire
        if use_pooling:
            self.attention_pooling = nn.MultiheadAttention(
                embed_dim=hidden_size,
                num_heads=8,  # üî• R√âDUIT DE 12 √Ä 8
                dropout=dropout_rate,
                batch_first=True
            )
        
        # Classifier simplifi√©
        self.dropout = nn.Dropout(dropout_rate)
        self.classifier = nn.Linear(hidden_size, num_labels)
        
        # Loss functions
        self.focal_loss = FocalLoss(alpha=1.0, gamma=2.0)
        self.label_smoothing = LabelSmoothingLoss(smoothing=0.1)
        
        # üî• ACTIV√â gradient checkpointing au niveau du mod√®le RoBERTa
        if hasattr(self.roberta, 'gradient_checkpointing_enable'):
            self.roberta.gradient_checkpointing_enable()
    
    def gradient_checkpointing_enable(self, gradient_checkpointing_kwargs=None):
        """Active le gradient checkpointing pour √©conomiser la m√©moire"""
        if hasattr(self.roberta, 'gradient_checkpointing_enable'):
            if gradient_checkpointing_kwargs is not None:
                self.roberta.gradient_checkpointing_enable(gradient_checkpointing_kwargs)
            else:
                self.roberta.gradient_checkpointing_enable()
    
    def gradient_checkpointing_disable(self):
        """D√©sactive le gradient checkpointing"""
        if hasattr(self.roberta, 'gradient_checkpointing_disable'):
            self.roberta.gradient_checkpointing_disable()
    
    def forward(self, input_ids, attention_mask=None, labels=None):
        # Sortie RoBERTa
        outputs = self.roberta.roberta(
            input_ids=input_ids,
            attention_mask=attention_mask,
        )
        
        sequence_output = outputs.last_hidden_state
        
        # Pooling optionnel (simplifi√©)
        if self.use_pooling and attention_mask is not None:
            # Pooling par attention simplifi√©
            pooled_output = sequence_output[:, 0, :]  # Prendre seulement [CLS]
        else:
            pooled_output = sequence_output[:, 0, :]  # [CLS] token
        
        # Classification
        pooled_output = self.dropout(pooled_output)
        logits = self.classifier(pooled_output)
        
        # Calcul de la loss
        loss = None
        if labels is not None:
            # Loss combin√©e (simplifi√©e)
            focal_loss = self.focal_loss(logits, labels.float())
            smooth_loss = self.label_smoothing(logits, labels.float())
            loss = 0.7 * focal_loss + 0.3 * smooth_loss
        
        # Retour simplifi√©
        from transformers.modeling_outputs import SequenceClassifierOutput
        return SequenceClassifierOutput(
            loss=loss,
            logits=logits,
            hidden_states=None,  # üî• NULL pour √©conomiser la m√©moire
            attentions=None      # üî• NULL pour √©conomiser la m√©moire
        )

# ========================================
# PR√âPARATION DES DONN√âES
# ========================================

def prepare_memory_optimized_data(df_train, df_val, df_test):
    """Pr√©paration des donn√©es optimis√©e pour la m√©moire"""
    
    print("üîß PR√âPARATION DES DONN√âES (OPTIMIS√âE M√âMOIRE)")
    print("=" * 50)
    
    # 1. Nettoyer les DataFrames
    exclude_cols = ['id', 'text', 'example_very_unclear', 'num_labels', 'text_length', '__index_level_0__']
    emotion_columns = [col for col in df_train.columns if col not in exclude_cols]
    
    print(f"üìä Colonnes d'√©motions: {len(emotion_columns)}")
    
    # 2. Nettoyer et r√©duire les DataFrames
    df_train_clean = df_train.reset_index(drop=True)
    df_val_clean = df_val.reset_index(drop=True)
    df_test_clean = df_test.reset_index(drop=True)
    
    # 3. Tokenizer
    tokenizer = AutoTokenizer.from_pretrained("roberta-base")
    
    def tokenize_memory_optimized(examples):
        """Tokenisation optimis√©e pour la m√©moire"""
        # üî• LONGUEUR R√âDUITE pour √©conomiser la m√©moire
        tokenized = tokenizer(
            examples["text"],
            truncation=True,
            padding="max_length",
            max_length=128,  # üî• R√âDUIT DE 256 √Ä 128
            add_special_tokens=True,
            return_attention_mask=True,
            return_token_type_ids=False
        )
        
        # Labels
        labels = []
        for i in range(len(examples["text"])):
            label_vector = [float(examples[col][i]) for col in emotion_columns]
            labels.append(label_vector)
        
        tokenized["labels"] = labels
        return tokenized
    
    # 4. Cr√©er datasets
    datasets = {}
    for split_name, df in [("train", df_train_clean), ("validation", df_val_clean), ("test", df_test_clean)]:
        df_for_dataset = df[['text'] + emotion_columns].copy()
        df_for_dataset = df_for_dataset[df_for_dataset['text'].notna() & (df_for_dataset['text'].str.len() > 0)]
        
        dataset = Dataset.from_pandas(df_for_dataset)
        datasets[split_name] = dataset
    
    dataset_dict = DatasetDict(datasets)
    
    # 5. Tokenisation avec nettoyage m√©moire
    print("üîß Tokenisation en cours...")
    
    # Tokeniser par petits batches pour √©conomiser la m√©moire
    tokenized_datasets = dataset_dict.map(
        tokenize_memory_optimized,
        batched=True,
        batch_size=500,  # üî• R√âDUIT DE 1000 √Ä 500
        remove_columns=dataset_dict["train"].column_names,
        desc="Tokenisation optimis√©e"
    )
    
    # 6. Nettoyage m√©moire
    del dataset_dict, datasets
    gc.collect()
    
    tokenized_datasets.set_format("torch")
    
    print("‚úÖ Pr√©paration termin√©e!")
    return tokenizer, tokenized_datasets, emotion_columns

# ========================================
# CALCUL DES POIDS DE CLASSES
# ========================================

def compute_class_weights_optimized(df_train, emotion_columns):
    """Calcul optimis√© des poids de classes"""
    
    print("‚öñÔ∏è CALCUL DES POIDS DE CLASSES")
    
    y_train = df_train[emotion_columns].values
    pos_weights = []
    class_info = {}
    
    for i, emotion in enumerate(emotion_columns):
        emotion_labels = y_train[:, i]
        pos_count = np.sum(emotion_labels)
        neg_count = len(emotion_labels) - pos_count
        
        # Calcul du poids avec lissage
        pos_weight = (neg_count + 1) / (pos_count + 1)
        pos_weights.append(pos_weight)
        
        class_info[emotion] = {
            'pos_weight': pos_weight,
            'frequency': pos_count / len(emotion_labels),
            'samples': int(pos_count)
        }
    
    # Affichage informatif
    sorted_emotions = sorted(class_info.items(), key=lambda x: x[1]['frequency'], reverse=True)
    
    print("üìä Top 5 √©motions les plus fr√©quentes:")
    for emotion, info in sorted_emotions[:5]:
        print(f"   {emotion}: {info['frequency']:.3f} ({info['samples']} √©chantillons)")
    
    return torch.tensor(pos_weights, dtype=torch.float32), class_info

# ========================================
# M√âTRIQUES
# ========================================

def compute_metrics_optimized(eval_pred):
    """M√©triques optimis√©es"""
    predictions, labels = eval_pred
    
    # Conversion en probabilit√©s
    probs = torch.sigmoid(torch.tensor(predictions)).numpy()
    
    # Seuils optimis√©s
    optimal_thresholds = np.full(probs.shape[1], 0.5)
    
    # Pr√©dictions binaires
    y_pred = (probs > optimal_thresholds).astype(int)
    y_true = labels.astype(int)
    
    # M√©triques principales
    f1_micro = f1_score(y_true, y_pred, average="micro", zero_division=0)
    f1_macro = f1_score(y_true, y_pred, average="macro", zero_division=0)
    f1_weighted = f1_score(y_true, y_pred, average="weighted", zero_division=0)
    hamming = hamming_loss(y_true, y_pred)
    
    return {
        "f1_micro": f1_micro,
        "f1_macro": f1_macro,
        "f1_weighted": f1_weighted,
        "hamming_loss": hamming,
    }

# ========================================
# TRAINER OPTIMIS√â AVEC TQDM
# ========================================

class MemoryOptimizedTrainer(Trainer):
    """Trainer optimis√© pour la m√©moire avec monitoring tqdm"""
    
    def __init__(self, pos_weights=None, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.pos_weights = pos_weights.to(self.args.device) if pos_weights is not None else None
        self.progress_bar = None
        self.total_steps = None
        
    def compute_loss(self, model, inputs, return_outputs=False):
        labels = inputs.get("labels")
        outputs = model(**inputs)
        
        if labels is not None:
            loss = outputs.loss
            
            # Ajout optionnel d'une loss avec class weights
            if self.pos_weights is not None:
                weighted_loss = F.binary_cross_entropy_with_logits(
                    outputs.logits, 
                    labels.float(), 
                    pos_weight=self.pos_weights
                )
                loss = 0.8 * loss + 0.2 * weighted_loss
            
            outputs.loss = loss
        
        return (outputs.loss, outputs) if return_outputs else outputs.loss
    
    def _inner_training_loop(self, batch_size=None, args=None, resume_from_checkpoint=None, trial=None, ignore_keys_for_eval=None):
        """Training loop avec barre de progression tqdm"""
        
        # Calculer le nombre total d'√©tapes
        if self.total_steps is None:
            num_train_samples = len(self.train_dataset)
            batch_size = args.per_device_train_batch_size * args.gradient_accumulation_steps
            num_steps_per_epoch = num_train_samples // batch_size
            self.total_steps = num_steps_per_epoch * args.num_train_epochs
        
        # Initialiser la barre de progression
        self.progress_bar = tqdm(
            total=self.total_steps,
            desc="üöÄ Entra√Ænement RoBERTa",
            unit="step",
            position=0,
            leave=True
        )
        
        # Appeler le training loop parent
        return super()._inner_training_loop(
            batch_size=batch_size,
            args=args,
            resume_from_checkpoint=resume_from_checkpoint,
            trial=trial,
            ignore_keys_for_eval=ignore_keys_for_eval
        )
    
    def training_step(self, model, inputs):
        """Step d'entra√Ænement avec mise √† jour de la barre de progression"""
        
        # Nettoyage m√©moire p√©riodique
        if self.state.global_step % 100 == 0:
            torch.cuda.empty_cache()
            gc.collect()
        
        # Training step normal
        loss = super().training_step(model, inputs)
        
        # Mise √† jour de la barre de progression
        if self.progress_bar is not None:
            self.progress_bar.update(1)
            
            # Mise √† jour des informations
            if hasattr(self.state, 'log_history') and self.state.log_history:
                last_log = self.state.log_history[-1]
                if 'train_loss' in last_log:
                    self.progress_bar.set_postfix({
                        'loss': f"{last_log['train_loss']:.4f}",
                        'step': self.state.global_step,
                        'epoch': f"{self.state.epoch:.1f}"
                    })
        
        return loss
    
    def evaluate(self, eval_dataset=None, ignore_keys=None, metric_key_prefix="eval"):
        """√âvaluation avec nettoyage m√©moire"""
        
        # Nettoyage m√©moire avant √©valuation
        torch.cuda.empty_cache()
        gc.collect()
        
        # √âvaluation normale
        results = super().evaluate(eval_dataset, ignore_keys, metric_key_prefix)
        
        # Mise √† jour de la barre de progression avec m√©triques
        if self.progress_bar is not None:
            f1_macro = results.get(f"{metric_key_prefix}_f1_macro", 0)
            self.progress_bar.set_postfix({
                'f1_macro': f"{f1_macro:.4f}",
                'step': self.state.global_step
            })
        
        return results

# ========================================
# CONFIGURATION D'ENTRA√éNEMENT
# ========================================

def create_extreme_memory_optimized_training_args():
    """Configuration d'entra√Ænement avec optimisation m√©moire extr√™me"""
    
    return TrainingArguments(
        # R√©pertoires
        output_dir="./results_ultimate_roberta",
        logging_dir="./logs_ultimate",
        
        # Strat√©gie d'√©valuation
        evaluation_strategy="steps",
        eval_steps=2000,  # üî• MOINS FR√âQUENT
        save_strategy="steps",
        save_steps=2000,
        logging_steps=500,  # üî• MOINS FR√âQUENT
        
        # Hyperparam√®tres ULTRA-OPTIMIS√âS pour m√©moire
        num_train_epochs=6,
        per_device_train_batch_size=1,  # üî• MINIMUM ABSOLU
        per_device_eval_batch_size=2,   # üî• MINIMUM ABSOLU
        gradient_accumulation_steps=32, # üî• TR√àS √âLEV√â pour compenser
        
        # Optimisation
        learning_rate=8e-6,
        weight_decay=0.01,
        warmup_ratio=0.1,
        lr_scheduler_type="cosine",
        
        # Performance ULTRA-OPTIMIS√âE M√âMOIRE
        fp16=True,
        dataloader_num_workers=0,
        dataloader_pin_memory=False,
        gradient_checkpointing=True,
        
        # S√©lection du meilleur mod√®le
        load_best_model_at_end=True,
        metric_for_best_model="f1_macro",
        greater_is_better=True,
        
        # Stability
        seed=42,
        data_seed=42,
        
        # Monitoring
        report_to="none",
        run_name="roberta_ultimate_memory_extreme",
        
        # Sauvegarde ULTRA-OPTIMIS√âE
        save_total_limit=1,  # üî• MINIMUM ABSOLU
        
        # Optimisations avanc√©es
        remove_unused_columns=False,
        push_to_hub=False,
        
        # Optimisations m√©moire suppl√©mentaires
        max_grad_norm=1.0,
        optim="adamw_torch",
        
        # üî• NOUVELLES OPTIMISATIONS M√âMOIRE
        dataloader_drop_last=True,
        ignore_data_skip=True,
        bf16=False,  # √âviter bf16 si probl√®mes
        tf32=False,  # D√©sactiver tf32 si probl√®mes
    )

# ========================================
# FONCTION PRINCIPALE D'ENTRA√éNEMENT
# ========================================

def train_roberta_ultimate_memory_extreme(df_train, df_val, df_test):
    """
    Pipeline d'entra√Ænement RoBERTa Ultimate avec optimisation m√©moire extr√™me
    et monitoring tqdm complet
    """
    
    print("üî• ENTRA√éNEMENT ROBERTA ULTIMATE - OPTIMISATION M√âMOIRE EXTR√äME")
    print("=" * 80)
    
    # 1. Nettoyage m√©moire initial
    print("üßπ Nettoyage m√©moire initial...")
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
    gc.collect()
    
    # 2. Affichage des informations m√©moire
    if torch.cuda.is_available():
        total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"üíæ M√©moire GPU totale: {total_memory:.2f} GB")
        print(f"üíæ M√©moire libre: {(total_memory - torch.cuda.memory_reserved() / 1024**3):.2f} GB")
    
    # 3. Pr√©paration des donn√©es
    print("\nüîß PR√âPARATION DES DONN√âES...")
    start_time = time.time()
    
    tokenizer, tokenized_datasets, emotion_columns = prepare_memory_optimized_data(
        df_train, df_val, df_test
    )
    
    print(f"‚è±Ô∏è Temps pr√©paration: {time.time() - start_time:.2f}s")
    
    # 4. Calcul des poids
    pos_weights, class_info = compute_class_weights_optimized(df_train, emotion_columns)
    
    # 5. Initialisation du mod√®le
    print("\nüèóÔ∏è Initialisation du mod√®le optimis√©...")
    model = MemoryOptimizedRoBERTaForMultiLabel(
        num_labels=len(emotion_columns),
        dropout_rate=0.2,
        use_pooling=True
    ).to(device)
    
    print(f"üìä Mod√®le initialis√© avec {len(emotion_columns)} labels")
    
    # 6. Configuration d'entra√Ænement
    training_args = create_extreme_memory_optimized_training_args()
    
    print("\nüîß Configuration optimisation m√©moire EXTR√äME:")
    print(f"   - Batch size: {training_args.per_device_train_batch_size}")
    print(f"   - Gradient accumulation: {training_args.gradient_accumulation_steps}")
    print(f"   - Batch effectif: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}")
    print(f"   - Longueur max s√©quences: 128")
    print(f"   - Gradient checkpointing: {training_args.gradient_checkpointing}")
    
    # 7. Callbacks
    early_stopping = EarlyStoppingCallback(
        early_stopping_patience=5,
        early_stopping_threshold=0.0001
    )
    
    # 8. Trainer optimis√©
    trainer = MemoryOptimizedTrainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_datasets["train"],
        eval_dataset=tokenized_datasets["validation"],
        tokenizer=tokenizer,
        compute_metrics=compute_metrics_optimized,
        callbacks=[early_stopping],
        pos_weights=pos_weights
    )
    
    # 9. Estimation du temps d'entra√Ænement
    num_train_samples = len(tokenized_datasets["train"])
    effective_batch_size = training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps
    steps_per_epoch = num_train_samples // effective_batch_size
    total_steps = steps_per_epoch * training_args.num_train_epochs
    
    print(f"\n‚è±Ô∏è ESTIMATION TEMPS D'ENTRA√éNEMENT:")
    print(f"   - √âchantillons d'entra√Ænement: {num_train_samples:,}")
    print(f"   - Steps par √©poque: {steps_per_epoch:,}")
    print(f"   - Steps totaux: {total_steps:,}")
    print(f"   - Temps estim√©: {total_steps * 2 / 3600:.1f}h (‚âà2s/step)")
    
    # 10. Nettoyage final avant entra√Ænement
    torch.cuda.empty_cache()
    gc.collect()
    
    # 11. LANCEMENT DE L'ENTRA√éNEMENT
    print("\nüöÄ LANCEMENT DE L'ENTRA√éNEMENT...")
    print("=" * 80)
    
    try:
        # Entra√Ænement avec monitoring tqdm
        train_start = time.time()
        train_result = trainer.train()
        train_time = time.time() - train_start
        
        print(f"\n‚úÖ Entra√Ænement termin√© en {train_time/3600:.2f}h")
        
        # 12. √âvaluation finale
        print("\nüìä √âVALUATION FINALE...")
        
        # Validation
        val_results = trainer.evaluate(tokenized_datasets["validation"])
        print(f"\nüéØ R√âSULTATS VALIDATION:")
        for key, value in val_results.items():
            if 'eval_' in key:
                print(f"   {key.replace('eval_', '').upper()}: {value:.4f}")
        
        # Test
        test_results = trainer.evaluate(tokenized_datasets["test"])
        print(f"\nüèÜ R√âSULTATS TEST FINAUX:")
        for key, value in test_results.items():
            if 'eval_' in key:
                print(f"   {key.replace('eval_', '').upper()}: {value:.4f}")
        
        # 13. Sauvegarde
        print("\nüíæ Sauvegarde du mod√®le...")
        trainer.save_model("./best_roberta_emotion_model")
        tokenizer.save_pretrained("./best_roberta_emotion_model")
        
        # 14. Statistiques finales
        print("\nüìä STATISTIQUES FINALES:")
        print(f"   Temps total: {train_time/3600:.2f}h")
        print(f"   F1 Macro: {test_results.get('eval_f1_macro', 0):.4f}")
        if torch.cuda.is_available():
            print(f"   M√©moire GPU utilis√©e: {torch.cuda.max_memory_allocated() / 1024**3:.2f} GB")
        
        print("\n‚úÖ MOD√àLE SAUVEGARD√â DANS: ./best_roberta_emotion_model")
        
        # Fermer la barre de progression
        if trainer.progress_bar is not None:
            trainer.progress_bar.close()
        
        return trainer, tokenizer, test_results, emotion_columns, class_info
        
    except torch.cuda.OutOfMemoryError as e:
        print(f"\n‚ùå ERREUR M√âMOIRE GPU: {e}")
        print("\nüí° SOLUTIONS D'URGENCE:")
        print("   1. Red√©marrer le kernel et relancer")
        print("   2. R√©duire max_length √† 64")
        print("   3. Utiliser DistilBERT au lieu de RoBERTa")
        print("   4. Entra√Æner sur CPU (tr√®s lent)")
        
        # Nettoyage d'urgence
        torch.cuda.empty_cache()
        gc.collect()
        raise
        
    except Exception as e:
        print(f"\n‚ùå ERREUR INATTENDUE: {e}")
        if trainer.progress_bar is not None:
            trainer.progress_bar.close()
        raise

# ========================================
# FONCTION DE NETTOYAGE M√âMOIRE
# ========================================

def cleanup_memory():
    """Nettoyage complet de la m√©moire"""
    print("üßπ NETTOYAGE M√âMOIRE COMPLET...")
    
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
        torch.cuda.ipc_collect()
    
    gc.collect()
    
    if torch.cuda.is_available():
        print(f"üíæ M√©moire GPU libre: {(torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_reserved()) / 1024**3:.2f} GB")
    
    print("‚úÖ Nettoyage termin√©!")

# ========================================
# EXEMPLE D'UTILISATION
# ========================================

if __name__ == "__main__":
    # Exemple d'utilisation (√† adapter selon vos donn√©es)
    print("üöÄ EXEMPLE D'UTILISATION:")
    print("cleanup_memory()")
    print("trainer, tokenizer, results, emotions, info = train_roberta_ultimate_memory_extreme(df_train, df_val, df_test)")
    print("print(f'F1 Macro: {results[\"eval_f1_macro\"]:.4f}')") 